\documentclass[12pt,a4paper]{article}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{microtype} 
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}


\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newcommand{\mycomment}[1]{}

\geometry{top=1in, bottom=1in, left=1in, right=1in}


\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Dos Santos}
\rfoot{Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}



\begin{document}

\section{Modern Algebra II}


Before beginning work on anything, start by reviewing the definitions that were important in Math 171.\\

\noindent\underline{Exercise:} If $N\subset H$, then there is a homomorphism $\bar f : G/N \to G'$ such that $\Bar{f}(aN) = f(a)$ for all $a$. In particular, if $N=H$, then $\Bar{f}$ is injective.

\subsection{Solving Polynomials}

\subsubsection{Cubic Equation}

\underline{Exercise:} Solve a cubic equation by Cardano's method.

\subsubsection{Ferrari Quartic Function}
We begin by applying a change of variables which is left as an exercise.
\[
a_4t^4 + a_3t^3 + a_2t^2 + a_1t + x_0 = 0
\]
\[
x^4 = -px^2 - qx - r
\]
\[
(x^2 + a)^2 = x^4 + 2ax^2 + a^2 = (2a - p)x^2 + qx - (a^2 - r)
\]
\[
q^2 - 4ac = 0
\]
\[
\implies q^2 - a(2a-p)(a^2-r) = 0
\]
Which is now a cubic polynomial in $a$. So we solve for $a$ by \underline{Cardano's} method.

\subsubsection{Quintic Equation}

\subsubsection{Solvable}

\textbf{Definition:} A group $G$ is \textit{solvable} if there exists a finite sequence of subgroups $G = H_0 \geq H_1 \geq \cdots \geq H_n = \{e\}$ such that each $H_{i+1}$ is a normal subgroup of $H_i$ (denoted $H_{i+1} \trianglelefteq H_i$) and the quotient group $H_i / H_{i+1}$ is Abelian for all $0 \leq i < n$.\\

\noindent\underline{Corollary:} Any ablien group is solvable.\\

\noindent\underline{Exercise:} Show that for $n\geq 5$ $S_n$ is not solvable.\\


\underline{\textbf{Lemma:}} $f:G\to G'$ hom of groups. $H = \ker f$, $N$ normal subhroup of $G$. Assume $N\subset H$. Then $f$ induces a hom. $\bar f: g/N \to G'$ defined by $\bar f(xN) = f(x), x\in G$
\begin{proof}
    By assumption, $xN = yN$ therefore $x \in yN$ and $y\in xN$ thus $x = y \cdot b \in N$. Since $N\subset H$, $f(b) = e$ so $f(x) = f(y)f(b) = f(y)$.
\end{proof}

\underline{Exercise:} Assume $G$ is solvable. $H$ is a subgroup of $G$. Prove $H$ is solvable.


\newpage

\section{Linear Algebra II}

\subsection{Vector Spaces}

\begin{definition}
    Let $V$ be a vector space, let $U,W$ be subspaces of $V$ define their 'sum', $U+W$ as the following set:
    \[
    U+W = \{u + w : u\in U, w\in W\}
    \]
\end{definition}

We can loosely say without proof $U + W = \mathbb{R}^2$

\begin{center}
    
\begin{tikzpicture}[scale=1.5, >=Stealth]
    % Draw axes
    \draw[<->] (-2,0) -- (2,0) node[right] {$x$};
    \draw[<->] (0,-2) -- (0,2) node[above] {$y$};

    % Draw subspace U
    \draw[thick, blue, ->] (0,0) -- (1.5,1) node[anchor=south west] {$U$};
    % Draw subspace W
    \draw[thick, red, ->] (0,0) -- (-1,1.5) node[anchor=south east] {$W$};

    % Draw U + W (the entire plane here is implied to be U + W)
    \node at (1.5,-1.5) [circle,fill,inner sep=1pt]{};
    \node at (1.5,-1.5) [right] {$U + W = \mathbb{R}^2$ (implied)};

    % Optional: Highlighting the plane to indicate U+W, uncomment the line below
    % \fill[green!20,opacity=0.3] (-2,-2) rectangle (2,2); % This fills the entire visible area to represent U+W

\end{tikzpicture}

\end{center}


\begin{definition}
    We say that a sum $U+W$ is \underline{direct} if for all $v \in U+W$ there are unique $u,v$ such that $v = u + v$. In this case we can write:
    \[
    U\oplus V
    \]
\end{definition}

\underline{Example 1:} $U + V$ Direct\\

Since $(a,0) + (0,b) = (a,b)$ is the only way to write $(a,b)$.\\

\underline{Example 2:} $U + Y + V$ not direct.\\

Since $(a,0) + (0,b) + (0,0) = (a,b)$ and $(a+1,0) + (0,b-1) + (-1,1) = (a,b)$\\


\begin{definition}
    $U+V$ is direct $\iff$ $U\cap W = \{0\}$
\end{definition}

\subsection{Transformations}

\begin{definition}
    Suppose $T:V\to W$ is a linear map, then the kernel of $T$ is the subspace of $V$ defined by 
    \[
    \ker(T) = \{v\in V : T(v) = 0\}
    \]
\end{definition}
\begin{definition}
    The image of $T$ is $T$ applied to entire domain.
\end{definition}

\underline{Exercise:} Consider $T:C^1(\mathbb{R}) \to C^0(\mathbb{R})$. What is the kernel and the image.\\

Every continuous function is integrable so use FTC 1. I.E. The map is surjective.\\

The kernel is all constant functions as $\dv{}{x} C = 0$.












\begin{algorithm}[H]
    \caption{Find Maximum Value}
    \label{alg:find_max}
    \SetAlgoLined
    \KwData{An array $array$ of length $n$}
    \KwResult{The maximum value in the array}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{An array $array$}
    \Output{The maximum value $max$}
    
    $max \leftarrow array[0]$\;
    \For{$i \leftarrow 1$ \KwTo $n$}{
        \If{$array[i] > max$}{
            $max \leftarrow array[i]$\;
        }
    }
    \Return{$max$}\;
\end{algorithm}

Algorithm~\ref{alg:find_max} outlines the procedure for finding the maximum value in an array.

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}
\usepackage{amsmath, amscd, amsthm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{mdframed}
\usepackage{verbatim}
\usepackage{tikz}

\usetikzlibrary{cd}
\usetikzlibrary{arrows}

\usepackage{listings}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}%\usecolortheme{beetle}
%\usecolortheme{crane}
\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[]{An Introduction to Discrete Shrödinger Operators} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Tomas, Darius, Alejandro } % Your name
\institute[UCR] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
University of California, Riverside \\ % Your institution for the title page
\medskip
\textit{} % Your email address
}
\date{June 2024} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}






\begin{frame}{Introduction}
    
        \textbf{Structure of the Presentation}
        \begin{itemize}
            \item Overview of Probability Measures
            \item Special Linear 2x2 Matrices and Spectral Properties
            \item Ergodic Theorems and Hyperbolic Dynamics
            \item Application to Discrete Schrödinger Operators
        \end{itemize}

        
    
\end{frame}


\begin{frame}{Probability Measure\footnote{Alex}}
    \begin{itemize}
        \item \textbf{Definition:} A probability measure is a function that assigns a number between 0 and 1 to each event in a sample space, representing the likelihood of the event.
        \item \textbf{Properties:}
        \begin{itemize}
            \item \textbf{Non-negativity:} $P(E) \geq 0$ for all events $E$.
            \item \textbf{Normalization:} $P(\Omega) = 1$, where $\Omega$ is the sample space.
            \item \textbf{Additivity:} For any countable sequence of mutually exclusive events $E_1, E_2, \ldots$, $P\left(\bigcup_{i=1}^{\infty} E_i\right) = \sum_{i=1}^{\infty} P(E_i)$.
        \end{itemize}
        \item \textbf{Examples:} Coin toss, rolling a die, continuous distributions like the normal distribution.
    \end{itemize}
\end{frame}

\begin{frame}{Probability spaces\footnote{Alex}}
    \begin{itemize}
        \item \textbf{Definition:} Given our sample space $\Omega$, we call a collection of subsets $\mathcal{A} \subseteq \mathcal{P}(\Omega)$ a $\sigma$-algebra if it is closed under complements and countable unions and intersections
        \item \textbf{Definition:} We call the triple $(\Omega, \mathcal{A}, P)$ a probability space
        \item \textbf{Example:} Consider the rolling of a 6-sided die. Then we could take $\Omega=\{1,2,3,4,5,6\}$ and $\mathcal{A}$ be the set of all subsets of $\Omega$. Then taking the subset $E-\{1,3\}$, then $P(E)$ is the probability that you if roll the dice you will either a $1$ or a $3$
    \end{itemize}
\end{frame}




\begin{frame}{$\ell^2(\mathbb{N})$\footnote{Alex}}
    \begin{itemize}
        \item \textbf{Definition:} We define
    \[\ell^2(\mathbb N) := \{\psi: \mathbb{N} \rightarrow \mathbb{C}: ||\psi||_2 = \sum_{j=1}^\infty |\psi(j)|^2 < \infty\}\]
    Where we equip the space with the inner product
    \[\langle\phi, \psi\rangle = \sum_{j=1}^\infty\overline{\phi(j)} \psi(j)\]
    \end{itemize}
    
\end{frame}


\begin{frame}{Background on Discrete Schrödinger Operators\footnote{Alex}}
    \begin{itemize}
        \item A discrete Schrödinger operator is a linear operator used to model quantum systems on discrete lattice points.
        \item Typically written as
        \[
        (H_{\omega}\psi)(n) = \psi(n+1) + \psi(n-1) + V_{\omega}(n)\psi(n),
        \]
        where $\psi(n)$ represents the wave function at site $n$ and $V(n)$ is the potential at site $n$.
        \item $V_{\omega}(n) = f(T^n \omega)$, where $f:\Omega \to \mathbb{R}$ is bounded and measurable, and $\omega \in \Omega$, $\Omega$ a probability space.
        \item \textbf{Applications:}
        \begin{itemize}
            \item Quantum mechanics: Modeling quantum particles.
            \item Solid-state physics: Studying electronic properties of materials.
            \item Mathematical physics: Understanding spectral properties of operators.
        \end{itemize}
    \end{itemize}
\end{frame}



% \begin{frame}{Hilbert Space}
%     \begin{itemize}
%         \item \textbf{Definition:} A Hilbert space is a complete inner product space.
%         \item \textbf{Inner Product:} A function $\langle \cdot, \cdot \rangle : H \times H \to \mathbb{C}$ (or $\mathbb{R}$) satisfying:
%         \begin{itemize}
%             \item $\langle x, x \rangle \geq 0$ and $\langle x, x \rangle = 0 \iff x = 0$
%             \item $\langle x, y \rangle = \overline{\langle y, x \rangle}$
%             \item $\langle ax + by, z \rangle = a \langle x, z \rangle + b \langle y, z \rangle$
%         \end{itemize}
%         \item \textbf{Completeness:} Every Cauchy sequence in $H$ converges to an element in $H$.
%         \item \textbf{Examples:} $\mathbb{R}^n$ with dot product, $L^2$ space of square-integrable functions.
%     \end{itemize}
%     Every Hilbert space is also a Banach space because the completeness of the inner product space implies the completeness of the normed space derived from the inner product.
% \end{frame}


\begin{frame}{Special Linear Matrices\footnote{Alex}}

    To study our operators its important to understand special linear matrices defined by:
    $$
    SL_2(\mathbb F) = SL(2, \mathbb F) = \{ A \in \mathbb M_{2\times 2}(\mathbb F) \; : \; \det A = 1\}
    $$
    SL2 matrices are useful as we can understand their behavior by just looking at their trace. We know that the eigenvalues of special linear matrices will always be of the form 
    \[
    \lambda = \frac{\Tr(A)}{2} \pm \frac{\sqrt{Tr(A)^2 - 4}}{2}
    \]
\end{frame}

\begin{frame}{Special Linear Matrices\footnote{Alex}}

    This leads us to the definition.\\
    \vspace{0.4cm}
    
    \textbf{Definition}

    
    Given a matrix $A\in SL_2(\mathbb R)$ we say that
    \begin{enumerate}
        \item $A$ is \textbf{elliptic} if $Tr(A) \in (-2,2)$.
        \item $A$ is \textbf{hyperbolic} if $Tr(A) \in \mathbb{R} \setminus (-2,2)$
        \item $A$ is \textbf{parabolic} if $Tr(A) \in \{-2,2\}$
    \end{enumerate}
    \vspace{0.4cm}

    In addition, in case 1 and 2 we can even determine the eigenvalues. In case 3, we know that $A$ will be similar to a $2\times2$ Jordan block with $\pm 1$ in the upper right entry
    
\end{frame}

\begin{frame}{Eigenvalues of $A \in SL_2(\mathbb{R})$\footnote{Alex}}
    \textbf{Definition} Given a matrix $A \in SL_2(\mathbb{R})$, we have that 
    \begin{itemize}
        \item If A is elliptic ($Tr(A) \in (-2,2)$) then eigenvalues are of the form $e^{i\theta}$
        \item If A is hyperbolic ($Tr(A) \in \mathbb{R}\setminus(-2,2)$), then eigenvalues of the form $\lambda, \frac{1}{\lambda}$, where $|\lambda| > 1$
        \item If A is parabolic ($Tr(A) \in \{-2,2\}$) then A is of the form \[\begin{pmatrix}
            1 &\pm1\\
            0 & 1
        \end{pmatrix}\]
    \end{itemize}
\end{frame}

\begin{frame}{$\mathbb{RP}^1$\footnote{Darius}}
\textbf{Definiton:} We define $\mathbb{RP}^1$, or real projective space, as the quotient space of $\mathbb{R}^2\setminus\{0\}$ under the equivalence relation $\sim$ , where $x\sim y$ if there is some $c \in \mathbb{R}$ such that $x = cy$\\
More simplistically, you can just think of $\mathbb{RP}^1$ as a unit circle
\end{frame}

\begin{frame}{Projective Special Linear\footnote{Darius}}
    Define
    \[
        PSL(2, \mathbb{R}) :=  \{ \tilde{A} : \mathbb{RP}^1 \to \mathbb{RP}^1 \; | \; \tilde{A} \text{ is induced via } A \in SL(2, \mathbb{R}) \}
    \]
    What does $\tilde{A}$ look like? Imagine a $U$ such that $AU = \tilde{A} U$
    
    \[
    \begin{tikzpicture}[>=stealth, node distance=3cm, auto]
        \node (A) at (0, 1.5) {$\mathbb{R}^2$};
        \node (B) at (3, 1.5) {$\mathbb{R}^2$};
        \node (C) at (0, 0) {$\mathbb{RP}^1$};
        \node (D) at (3, 0) {$\mathbb{RP}^1$};

        \draw[->] (A) to node {$A$} (B);
        \draw[->, dashed] (A) to node [swap] {$U$} (C);
        \draw[->, dashed] (B) to node {$U$} (D);
        \draw[->] (C) to node [swap] {$\tilde{A}$} (D);
    \end{tikzpicture}
    \]
    We are removing the stretching and reflection so that only rotation is left which simplifies our computations once we are in $\mathbb R \mathbb P^1$.
\end{frame}



\begin{frame}{SL(2) Cocycles\footnote{Darius}}
    We can define $SL(2)$ cocycles which are
\begin{align*}
    (T,A): \Omega \times \mathbb R^2 &\rightarrow \Omega \times \mathbb R^2\\
    (\omega,\Vec{v}) &\mapsto (T\omega, A(\omega) \Vec{v})
\end{align*}
If $T$ is invertible, then define the map by iterating (if $T$ is not invertible we only consider positive $n$)
\begin{align*}
    A^n: \Omega &\rightarrow SL_2(\mathbb R)\\
    \omega &\mapsto \begin{cases}
        A(T^{n-1}\omega) \cdots A(T\omega) A(\omega) & \text{ if } n\geq 1\\
        I & \text{ if } n = 0\\
        A(T^n\omega)^{-1}\times \cdots \times A(T^{-1}\omega)^{-1} & \text{ if } n\leq 1\\
    \end{cases}
\end{align*}
\end{frame}


\begin{frame}{Projectivizing $SL_2$-Cocycles\footnote{Tomas}}
    \begin{itemize}
        \item \textbf{Projection:}
        \begin{itemize}
            \item By properties of $SL_2(\mathbb{R})$, we can projectivize the cocycles to:
            \[
            (T,A): \Omega \times \mathbb{RP}^1 \rightarrow \Omega \times \mathbb{RP}^1
            \]
            \item This projection is crucial for understanding the dynamics of these maps by focusing on the angle in $\mathbb{RP}^1$.
        \end{itemize}
        \item \textbf{Commutative Diagram:}
        \begin{itemize}
            \item We seek a map $U$ such that the following diagram commutes:
        \end{itemize}
    \end{itemize}
    
    \[
    \begin{tikzpicture}[>=stealth, node distance=3cm, auto]
        \node (A1) at (0, 1.5) {$\Omega \times \mathbb{R}^2$};
        \node (A2) at (3, 1.5) {$\Omega \times \mathbb{R}^2$};
        \node (P1) at (0, 0) {$\Omega \times \mathbb{RP}^1$};
        \node (P2) at (3, 0) {$\Omega \times \mathbb{RP}^1$};
        
        \draw[->] (A1) to node {$(T, A)$} (A2);
        \draw[->] (A1) to node [swap] {$U$} (P1);
        \draw[->] (A2) to node {$U$} (P2);
        \draw[->] (P1) to node [swap] {$(T, \tilde{A})$} (P2);
    \end{tikzpicture}
    \]
    
    \begin{itemize}
        \item This approach can be similarly applied to $\mathbb{CP}^1$.
    \end{itemize}
    
\end{frame}



\begin{frame}{Connection to Schrödinger Operators\footnote{Darius}}
    The space $(X,B,\mu,T)$ must be ergodic, meaning that any invariant set under $T$, must have measure 0 or 1.
    \begin{itemize}
        \item \textbf{Ergodic Probability Space:}
        \begin{itemize}
            \item Consider $(\Omega, \mathcal{B}, \mu, T)$ where $T$ is ergodic.
            \item Let $f: \Omega \rightarrow \mathbb{R}$ be bounded and measurable.
        \end{itemize}
        \item \textbf{Schrödinger Operators:}
        \begin{align*}
            [H_\omega \psi](n) &= \psi(n+1) + \psi(n-1) + V_\omega(n) \psi(n), \\
            V_\omega(n) &= f(T^n \omega),
        \end{align*}
        where $\psi \in \ell^2(\mathbb{Z})$.
        \item \textbf{Potential:}
        \begin{itemize}
            \item The potential $V_\omega(n) = f(T^n \omega)$ is called a dynamically defined potential.
        \end{itemize}
        \item \textbf{Eigenvalue Equation:}
        \begin{itemize}
            \item We seek solutions to $H_\omega \psi = E\psi$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Transfer Matrices\footnote{Darius}}
    We begin by defining transfer matrices, these make it easier to relate our eigenvalue solutions to what we already know.
    \begin{itemize}
        \item \textbf{Definition:}
        \[
        A_E(\omega) := \begin{pmatrix}
            E - f(T\omega) & -1 \\
            1 & 0
        \end{pmatrix} = \begin{pmatrix}
            E - V_\omega(1) & -1 \\
            1 & 0
        \end{pmatrix}
        \]
        \item \textbf{Relation to Eigenvalue Equation:}
        \[
        H_\omega \psi = E\psi \iff \begin{pmatrix}
            \psi(n+1) \\
            \psi(n)
        \end{pmatrix} = 
        A^n_E
        \begin{pmatrix}
            \psi(1) \\
            \psi(0)
        \end{pmatrix}
        \]
    \end{itemize}
    
\end{frame}

% \begin{frame}{Example}
%     We show $H\psi = E\psi \iff \begin{pmatrix}
%         \psi(n+1) \\
%         \psi(n)
%     \end{pmatrix} = A_E^n\begin{pmatrix}
%         \psi(1)\\
%         \psi(0)
%     \end{pmatrix}$
%     \begin{align*}
%         &\implies \\
%         &\text{Base case: (trivial)} \\
%         &\text{Inductive: Assume its true for $1,...,n$, we want to show it holds for $n+1$} \\
%         & A_E^{n+1} \begin{pmatrix}
%             \psi(1)\\
%             \psi(0)
%         \end{pmatrix} = A_E \begin{pmatrix}
%             \psi(n+1) \\
%             \psi(n)
%         \end{pmatrix} = \begin{pmatrix}
%             \psi(n+1)(E-f(T\omega)) - \psi(n)\\
%             \psi(n+1)
%         \end{pmatrix}\\
%         &\begin{pmatrix}
            
%         \end{pmatrix}
%     \end{align*}
% \end{frame}



\begin{frame}{Equivalent Conditions for Uniform Hyperbolicity\footnote{Darius}}
    \begin{theorem}
        Given an $SL_2(\mathbb{R})$-cocycle $(T,A)$, the following are equivalent:
        \begin{enumerate}
            \item \textbf{Exponential Growth Condition:}
            \[
            \exists c>0, \lambda>1, \text{ such that } ||A^n(\omega)|| \geq c \lambda^{|n|} \text{ for all } n \in \mathbb{Z}, \omega \in \Omega
            \]
            \item \textbf{Invariant Exponential Splitting:}
            \begin{itemize}
                \item Maps $\Lambda_s, \Lambda_u: \Omega \rightarrow \mathbb{RP}^1$.
                \item Constants $c>0, \lambda >1$.
                \item Invariance:
                \[
                A(\omega)\Lambda_s(\omega) = \Lambda_s(T\omega), \quad A(\omega)\Lambda_u(\omega) = \Lambda_u(T\omega)
                \]
                \item Exponential:
                \[
                ||A^n(\omega)\vec{v}_s|| \leq c \lambda^{-n}, \quad ||A^{-n}(\omega)\vec{v}_u|| \leq c \lambda^{-n}
                \]
            \end{itemize}
        \end{enumerate}
    \end{theorem}
\end{frame}

% \begin{frame}{Equivalent Conditions for Uniform Hyperbolicity (Cont.)}
%     \begin{theorem}
%         \begin{enumerate}
%             \setcounter{enumi}{2}
%             \item \textbf{No Bounded Orbit:}
%             \[
%             \exists \omega \in \Omega, \vec{v} \in \mathbb{S}^1, \text{ such that } ||A^n(\omega)\vec{v}|| \leq 1 \text{ for all } n \in \mathbb{Z}
%             \]
%             \item \textbf{Projective Conjugacy:}
%             \begin{itemize}
%                 \item Constants $\alpha>0, \beta>1$.
%                 \item Continuous function $B: \Omega \rightarrow PSL(2, \mathbb{R})$, $r: \Omega \rightarrow \mathbb{R}_+$.
%                 \item Conjugacy relation:
%                 \[
%                 B(T\omega)^{-1}A(\omega)B(\omega) = \begin{pmatrix}
%                     r(\omega) & 0 \\
%                     0 & r(\omega)^{-1}
%                 \end{pmatrix}
%                 \]
%                 \item Growth condition:
%                 \[
%                 r_n(\omega) := \prod_{j=0}^{n-1} r(T^j \omega) \geq \alpha \beta^n
%                 \]
%             \end{itemize}
%         \end{enumerate}
%     \end{theorem}
% \end{frame}

\begin{frame}{Example}
    We will show that if $\Omega$ is a singleton, $T$ is the identity map, and $A:\Omega \to SL_2(\mathbb{R})$, then 
    \[(T,A) \in \mathcal{UH} \iff |Tr(A)| > 2\]


    \begin{mdframed}
    $(\impliedby)$ If $|Tr(A)| > 2$, then $A$ is hyperbolic and therefore it has eigenvalues $\lambda, \frac{1}{\lambda}$ where $|\lambda| > 1$. Let $v$ be normed eigenvector associated to $\lambda$, then $||A^n v|| = |\lambda^n| > \frac{1}{2}|\lambda|^1$. This shows $A$ satisfies the exponential growth condition and thus $(T,A) \in \mathcal{UH}$.
    \end{mdframed}
    \begin{mdframed}
    $(\implies)$ If $|Tr(A)|\leq 2$, then eigenvalues are $|\cdot| \leq 1$. Thus, failing exponential growth condition so $(T,A)\notin \mathcal{UH}$.
    \end{mdframed}
\end{frame}


\begin{frame}{Kingman's Theorem}
    \begin{theorem}
    Given $(X,\mathcal{B}, \mu, T)$ ergodic. If $\{f_n\}: X \rightarrow \RR$ are $L^1$ and satisfy 
    \[f_{n+m}(\omega) \leq f_n(\omega) + f_m(T^n\omega)\]
    then there is a constant function $f$ so that
    \[\lim_{n\rightarrow \infty} \frac{1}{n} f_n(\omega) = f\geq -\infty\]
    for a.e. $\omega \in X$. Moreover, if we have a uniform bound $||f_n||_\infty \leq Cn$ with constant $C$ then
    \[f = \inf_{n\geq 1} \frac{1}{n}\int f_n d\mu \]
\end{theorem}

The space $(X,B,\mu,T)$ must be ergodic, meaning that any invariant set under $T$, must have measure 0 or 1.

\end{frame}


\begin{frame}{Non-Uniformly Hyperbolic: Lyapunov Exponent\footnote{Darius}}
    \begin{definition}
        The \textbf{Lyapunov Exponent} is defined as:
        \[
        L(A) := \lim_{n \rightarrow \infty} \frac{1}{n} \int_\Omega \log ||A^n(\omega)|| \, d\mu
        \]
    \end{definition}
    \begin{itemize}
        \item The limit exists by Kingman's Ergodic Theorem.
        \item The Lyapunov exponent takes values in $[0, \infty)$.
    \end{itemize}
\end{frame}


\begin{frame}{Existence Lyapunov Exponent}
    \begin{proof}
To show the limit exists by Kingman's Ergodic Theorem, we check two conditions:

\begin{itemize}
    \item \textbf{Subadditivity:} Define \( f_n(\omega) = \log ||A^n(\omega)|| \). We need:
    \[
    f_{n+m}(\omega) \leq f_n(\omega) + f_m(T^n \omega)
    \]
    This holds because:
    \[
    A^{n+m}(\omega) = A^n(\omega) A^m(T^n \omega)\]\[ \implies \log ||A^{n+m}(\omega)|| \leq \log ||A^n(\omega)|| + \log ||A^m(T^n \omega)||
    \]

    \item \textbf{Integrability:} We have:
    \[
    \int_\Omega f_1 \, d\mu = \int_\Omega \log ||A(\omega)|| \, d\mu > -\infty
    \]
    
\end{itemize}
\end{proof}
\end{frame}

\begin{frame}{Equivalent Definitions of Lyapunov Exponent}
    
        We have:
        \begin{align*}
            L(A) &= \lim_{n \rightarrow \infty} \frac{1}{n} \int_\Omega \log ||A^n(\omega)|| \, d\mu \\
            &= \inf_{n \rightarrow \infty} \frac{1}{n} \int_\Omega \log ||A^n(\omega)|| \, d\mu \\
            &= \lim_{n \rightarrow \infty} \frac{1}{n} \log ||A^n(\omega)|| \text{ for a.e. } \omega \in \Omega
        \end{align*}
    
    
\end{frame}


% \begin{frame}{Johnson's Theorem}
%     \begin{theorem}[Johnson's Theorem]
%         \[
%         \sigma(H_\omega) = \{E \in \mathbb{R} : A_E \notin \mathcal{UH}\}
%         \]
%     \end{theorem}
%     The proof involves showing that if $E\in \mathbb R$ such that $A_E\in \mathcal U\mathcal H$, then $E\in \rho(H_\omega)$ then the opposite inclusion then follows directly.
    
% \end{frame}

\begin{frame}{Non-Uniformly Hyperbolic}
    Finally, we denote what it means for our operator to be non-uniformly hyperbolic, we denote this as $(T, A) \in \mathcal{NUH}$.

    \vspace{0.4cm}

    \begin{definition}
    \centering
        $(T,A) \in \mathcal{NUH}$ if $(T,A) \notin \mathcal{UH}$ and $L(A) > 0$.
    \end{definition}
    
    \vspace{0.4cm}
    
    We note that $(T,A) \in \mathcal{UH} \implies L(A) > 0$, so the definition above is non-trivial.
\end{frame}

% \begin{frame}
%     \begin{center}
%         \Huge \textbf{Q\&A} 
%     \end{center}
% \end{frame}


\begin{frame}{Thanks}
\centering
        $$\mathbb{ THANK \; YOU }$$
        
        
\end{frame}


\begin{frame}{References}
    \begin{enumerate}
    \item One-Dimensional Ergodic Shrödinger Operators by David Damanik and Jake Fillman
    \item Ergodic Theory by Einsiedler and Ward.
    \item Real Analysis by Folland 
    \item Uniform Positivity of the Lyapunov Exponent for Monotone Potentials Generated by the Doubling Map by Zhenghe Zhang
    \item Uniform Hyperbolicity and its Relation with Spectral Analysis of 1D Discrete Shrödinger Operators by Zhenghe Zhang
\end{enumerate}
\end{frame}

\end{document}


\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{mdframed}
\usepackage{physics}
\usepackage{amsthm}
\usepackage{proof}
\geometry{top=1.5cm, bottom=1.5cm, left=1cm, right=1cm}
\setlength\parindent{0pt}

\begin{document}

\section*{Problems}

\subsection*{Problem 1}
Let $f \in K[t]$ and $\alpha \in \mathbb{C}$. Assume that $f$ is nonconstant and $f(\alpha) = 0$. Let $p$ be a nonconstant polynomial over $K$ of least degree such that $p(\alpha) = 0$.
\begin{enumerate}
    \item[(a)] Prove that $p$ is irreducible over $K$.
    \begin{mdframed}
\begin{proof}
    Suppose $p$ is not irreducible over $K$, then there exists nonconstant $g,h\in K[t]$ such that
    \[
    p = gh \implies p(\alpha) = g(\alpha)h(\alpha) \implies g(\alpha) = 0 \text{ or } h(\alpha) = 0
    \]
    Without loss of generality, suppose $g(\alpha) = 0$ then since $g,h$ are nonconstant
    \[
    \deg gh = \deg p \implies \deg g < \deg p
    \]
    Which means that $g$ is degree lower than $p$ and $g(\alpha) = 0$ which is a contradiction.
\end{proof}

\end{mdframed}
    \item[(b)] Prove that $f = pq$ for some $q \in K[t]$.
    \begin{mdframed}
        \begin{proof}
            We know that there must exists $p,r \in K[t]$ with $\deg r < \deg p$ so 
            \[
            f(\alpha) = p(\alpha)q(\alpha) + r(\alpha) = 0
            \]
            Since $p$ is of minimal degree such that $p(\alpha) = 0$ this forces $r = 0$. Therefore we obtain $f = pq$ as desired.

        \end{proof}
    \end{mdframed}
    \item[(c)] Suppose that $f$ is irreducible over $K$. Prove that $f = cp$ for some $c \in K$.
    \begin{mdframed}
        \begin{proof}
            We already have that $f = pq$. We have shown that if $f$ is irreducible and $f=pq$ then either $p$ or $q$ must be constant. Since we know that $p$ is nonconstant, $q = c$ for some $c \in K$. Therefore, $f = cp$
        \end{proof}
    \end{mdframed}
\end{enumerate}


\subsection*{Problem 2}
Let $f \in K[t]$. Assume that $f$ is irreducible over $K$ and
\[
f(t) = c(t - \alpha_1) \cdots (t - \alpha_n)
\]
where $c \in K$ and $\alpha_1, \ldots, \alpha_n \in \mathbb{C}$. Prove that $\alpha_1, \ldots, \alpha_n$ are pairwise distinct.

\begin{mdframed}
    \begin{proof}
        Suppose they are not all pairwise distinct. Without loss of generality, suppose $\alpha = \alpha_1 = \alpha_2$ then we have 
        \[
        f(t) = c(t-\alpha)^2 g(t)
        \]
        Since $f$ has repeated roots, $f(\alpha) = 0$ means that $f'(\alpha) = 0$. In addition, we have that $\deg f' = \deg f - 1 \geq 1$. Therefore, $f'$ has degree less than $f$ yet $f'(\alpha) = 0$ which is a contradiction to $f$ being irreducible.
    \end{proof}
\end{mdframed}

\subsection*{Problem 3}
Let $F$ be a field.
\begin{enumerate}
    \item[(a)] Let $\beta \in \mathbb{C}$. Assume that $\beta$ is algebraic over $F$ and $\beta \neq 0$. Prove that $\beta^{-1} = g(\beta)$ for some $g \in F[t]$.
    \begin{mdframed}
        \begin{proof}
            Let $f$ be the irreducible polynomial of $\beta$ over $K$. Then we have that
            \[
            c_0 + c_1\beta + \cdots + c_{n-1}\beta^{n-1} + c_n\beta^n = 0
            \]
            Since $f$ is irreducible, $c_0\neq 0$ therefore we can proceed as follows
            \[
            -\frac{c_1}{c_0} - \cdots - \frac{c_{n-1}}{c_0} \beta^{n-1} = \beta^{-1}
            \]
        \end{proof}
    \end{mdframed}
    \item[(b)] Let $\alpha \in \mathbb{C}$. Assume that $\alpha$ is algebraic over $F$. Let $n$ be the degree of $\alpha$ over $F$, and let $E$ be the $F$-linear span of $1, \alpha, \ldots, \alpha^{n-1}$ in $\mathbb{C}$. Prove that $E$ is a field and $1, \alpha, \ldots, \alpha^{n-1}$ form a basis of $E$ as a vector space over $F$; in particular, $[E : F] = n$.
    \begin{mdframed}
        \begin{proof}
            
        \end{proof}
    \end{mdframed}
\end{enumerate}

\subsection*{Problem 4 (Primitive element theorem; Lang, page 272)}
Let $F$ be a field.
\begin{enumerate}
    \item[(a)] Let $\alpha, \beta \in \mathbb{C}$ be algebraic over $F$, and let $n = [F(\alpha, \beta) : F]$. Let $\sigma_1, \ldots, \sigma_n$ be the $n$ embeddings of $F(\alpha, \beta)$ into $\mathbb{C}$ extending the identity map on $F$. Prove that there exists an element $c \in F$ such that
    \[
    \sigma_i(\alpha + c\beta) \neq \sigma_j(\alpha + c\beta) \text{ whenever } i \neq j.
    \]
    Deduce that $F(\alpha, \beta) = F(\alpha + c\beta)$.
    \item[(b)] Let $E/F$ be a finite extension. Prove that $E = F(\gamma)$ for some $\gamma \in E$.
\end{enumerate}
\begin{mdframed}
    \begin{proof}[Part A]
         We can choose $c$ to be 
         \[
         c \neq \frac{\sigma_i \alpha - \sigma_j \alpha}{\sigma_j \beta - \sigma_i \beta}
         \]
         This is well defined as $\sigma_i \beta - \sigma_j \beta \neq 0$ whenever $i \neq j$. Choosing this $c$ allows shows us that $\sigma_i(\alpha + c\beta) \neq \sigma_j(\alpha + c\beta)$ for all $i \neq j$.\\


         Now, we note that $\alpha + c\beta \in F(\alpha, \beta)$ as these are basic field operations therefore $F(\alpha + c\beta) \subset F(\alpha, \beta)$. As $\sigma_1, ... , \sigma_n$ are all embeddings of $F(\alpha, \beta)$ into $\mathbb C$, and $\sigma_i(\alpha + c\beta) \neq \sigma_j(\alpha + c\beta)$ for all $i\neq j$, $\sigma_1, ... , \sigma_n$ are embeddings of $F(\alpha + c\beta)$ into $\mathbb C$. This means that $[F(\alpha + c\beta) : F] \geq n$, but since $F(\alpha + c\beta) \subset F(\alpha, \beta)$
         \[
         [F(\alpha, \beta) : F] = [F(\alpha, \beta) : F(\alpha + c\beta)][F(\alpha + c\beta) : F]
         \]
        But as $[F(\alpha + c\beta):F] \geq n$, $[F(\alpha, \beta) : F(\alpha + c\beta)] = 1$. Hence $F(\alpha, \beta) = F(\alpha + c\beta)$.
        
    \end{proof}

    \begin{proof}[Part B]
        We prove the argument by induction. The base case is immediate from Part A.\\

        Now assume $F(\alpha_1, ..., \alpha_{n-1}) = F(\beta)$ for some $\beta$. Then $F(\alpha_1, ... , \alpha_n) = F(\beta, \alpha_n)$. We can apply Part A to attain $F(\beta, \alpha_n) = F(\alpha_n + c\beta) = F(\gamma)$.
    \end{proof}
\end{mdframed}

\subsection*{Problem 5 (Lang, pages 280-281)}
Let $K/F$ be a finite normal extension and $G = \text{Gal}(K/F)$. Then $\text{Fix}(G) = F$.

\begin{mdframed}
    \begin{proof}.\\
        Let $x \in F$ and $\sigma \in \text{Gal}(K/F)$, then by definition, $\sigma x = x$ so $x \in \text{Fix}(G)$. This means that $\text{Fix}(G) \supseteq F$\\

        \noindent Now suppose $\alpha \in K$ such that $\alpha \notin F$. Then the degree of $\alpha$ over $F$ must be greater than $1$. This means that there exists an embedding $\sigma: F(\alpha) \to \mathbb C$ such that $\sigma$ is not the inclusion map. We can extend $\sigma$ to $\tau: K \to \mathbb C$ and since $K/F$ is normal, $\tau \in G$ yet $\tau \alpha = \sigma\alpha \neq \alpha$ meaning $\alpha \notin \text{Fix}(G)$. This means that $\text{Fix}(G) \subseteq F$.\\

        \noindent Since $\text{Fix}(G)$ and $F$ are subsets of each other, they must be equal.
    \end{proof}
\end{mdframed}

\subsection*{Problem 6 (Lang, page 281)}
Let $K/F$ be a finite normal extension, let $H$ be a subgroup of $\text{Gal}(K/F)$, and let $E = \text{Fix}(H)$.
\begin{enumerate}
    \item[(a)] By the primitive element theorem, there exists $\alpha \in K$ such that $K = F(\alpha)$. Let $\sigma_1, \ldots, \sigma_r$ be the elements of $H$, and let
    \[
    f(t) = (t - \sigma_1\alpha) \cdots (t - \sigma_r\alpha) \in K[t].
    \]
    Prove that $f \in E[t]$. Deduce that $[K : E] \leq |H|$.
    \item[(b)] Prove that $\text{Gal}(K/E) = H$.
\end{enumerate}

\begin{mdframed}
    \begin{proof}[Part A]
        Let $\sigma \in H$ then 
        \[
        \sigma f(t) = (t - \sigma\sigma_1 \alpha) \cdots (t-\sigma\sigma_r \alpha)
        \]
        The list $\sigma\sigma_1,...,\sigma\sigma_r$ is just a permutation of $\sigma_1,...,\sigma_r$ meaning, $\sigma f = f$. Since $\sigma$ fixes the coefficients of $f$, $f\in E[t]$.\\

        \noindent The extension $K/E = E(\alpha)/E$ and $\alpha$ is the root of the $f$ with degree $r$. Thus the degree of the extension $[K:E] = [E(\alpha):E] \leq r = \abs{H}$.
    \end{proof}
    \begin{proof}[Part B]
        Let $\sigma \in H$, then $\sigma x = x$ for all $x \in E$. Since $\sigma \in \text{Gal}(K/F)$ and $\sigma$ fixes elements in $E$, $\sigma \in \text{Gal}(K/E)$. Therefore we have that $H \subseteq \text{Gal}(K/E)$ but we know that $[K:E] \leq \abs{H}$ so it forces $H = \text{Gal}(K/E)$.
    \end{proof}
    

\end{mdframed}

\subsection*{Problem 7}
Let $K/F$ be a finite normal extension. Let $E$ be an intermediate field of $K/F$ and $H = \Phi(E)$.
\begin{enumerate}
    \item[(a)] Let $\tau \in \text{Gal}(K/F)$. Prove that $\Phi(\tau E) = \tau H \tau^{-1}$.
    \begin{proof}
        Let $\sigma \in H$, then
        \[
            \sigma\tau x = \tau x \quad \forall x \in E
            \iff \tau^{-1} \sigma \tau x = x \quad \forall x \in E
            \iff \tau^{-1}\sigma \tau \in H
            \iff \sigma \in \tau H\tau^{-1}
        \]
    \end{proof}
    \item[(b)] Prove that $E/F$ is normal if and only if $H$ is a normal subgroup of $\text{Gal}(K/F)$.
    \begin{proof}
        Suppose that $E/F$ is normal, let $\tau \in \text{Gal}(K/F)$ then $\tau E = E$ so 
        \[
        \tau H\tau^{-1} = \Phi(\tau E) = \Phi(E) = H
        \]
        So $H$ is a normal subgroup.\\


        Now suppose that $H$ is a normal subgroup of Gal$(K/F)$, let $\sigma$ be an embedding of $E$ into $\mathbb C$ we can extend $\sigma$ to $\tau: K\to \mathbb C$, $\tau$ is an automorphism since $K/F$ is normal. Therefore $\tau H\tau^{-1} = H$ means that
        \[
        \Phi(\tau E) = \Phi(E)
        \]
        Which means that $\tau E = \sigma E = E$ therefore $E/F$ is a normal extension.
    \end{proof}
    \item[(c)] Assume that $E/F$ is normal. Prove that the restriction map
    \[
    \text{Gal}(K/F) \to \text{Gal}(E/F), \tau \mapsto \tau|_E
    \]
    induces an isomorphism
    \[
    \text{Gal}(K/F)/H \cong \text{Gal}(E/F).
    \]
\end{enumerate}

\subsection*{Problem 8}
Let $F$ be a field and $n$ an integer $> 1$. Let $\zeta$ be a primitive $n$-th root of unity.
\begin{enumerate}
    \item[(a)] Prove that $F(\zeta)/F$ is a finite normal extension.
    \begin{mdframed}
        \begin{proof}[Finite Normal Extension]
            $\zeta$ is the root of the polynomial $t^n - 1 \in F[t]$ thus $F(\zeta)/F$ is finite. To show $F(\zeta) / F$ is normal, let $\sigma : F(\zeta) \to \mathbb C$ be an embedding over F, then $\sigma(\zeta)$ must be a root of $\sigma f = f$. Therefore $\sigma(\zeta) \in \{1, \zeta, ... , \zeta^{n-1}\}$. Hence $\sigma(\zeta) \in F(\zeta)$, so $\sigma F(\zeta) \subset F(\zeta)$.
        \end{proof}
    \end{mdframed}
    \item[(b)] Prove that $\text{Gal}(F(\zeta)/F)$ is abelian.
    \begin{mdframed}
        \begin{proof}[Gal$(F(\zeta)/F)$ is abelian.]
        It suffices to show it is abelian for $\zeta$. Let $\tau, \sigma \in \text{Gal}(F(\zeta)/F)$ then $\tau\zeta = \zeta^r$, $\sigma\zeta = \zeta^s$.
        \[
        (\tau \circ \sigma) (\zeta) = \tau(\zeta^s) = \tau(\zeta)^s = \zeta^{rs} = (\sigma \circ \tau)(\zeta)
        \]
        Since $\sigma, \tau$ are automorphisms, $\sigma\tau = \tau\sigma$ so $G$ is abelian.
        \end{proof}
    \end{mdframed}
\end{enumerate}

\subsection*{Problem 9}
Let $F$ be a field and $n$ an integer $> 1$. Assume that $F$ contains a primitive $n$-th root of unity. Let $\alpha \in \mathbb{C}$ such that $\alpha^n \in F$.
\begin{enumerate}
    \item[(a)] Prove that $F(\alpha)/F$ is a finite normal extension.
    \begin{mdframed}
        \begin{proof}
            $\alpha$ is the root of the polynomial $t^n - \alpha^n \in F[t]$ therefore $F(\alpha)/F$ is finite. Let $\sigma : F(\alpha) \to \mathbb C$ be an embedding over $F$. Then we must have that $\sigma(\alpha) = \zeta^r\alpha$ for some $r$. Therefore $\sigma(\alpha)\in F(\alpha)$ so $F(\alpha)/F$ is normal.
        \end{proof}
    \end{mdframed}
    \item[(b)] Prove that $\text{Gal}(F(\alpha)/F)$ is abelian.
    \begin{mdframed}
        \begin{proof}
            Let $\sigma, \tau \in G$ then we have that $\sigma(\alpha) = \zeta^r \alpha$ and $\tau(\alpha) = \zeta^s\alpha$.
            \[
            (\tau \circ \sigma)(\alpha) = \tau(\zeta^r\alpha) = \zeta^r\tau(\alpha) = \zeta^{r+s}\alpha = (\sigma \circ \tau)(\alpha)
            \]
            Since $\tau,\sigma$ are automorphisms, $\sigma\tau = \tau\sigma.$
        \end{proof}
    \end{mdframed}
\end{enumerate}

\subsection*{Problem 10}
Let $K/F$ be a normal $n$-radical extension. In this problem, we prove that $\text{Gal}(K/F)$ is a solvable group. Let $\zeta \in \mathbb{C}$ be a primitive $n$-th root of unity.
\begin{enumerate}
    \item[(a)] Prove that $K(\zeta)/F$ is normal.
    \begin{mdframed}
        \begin{proof}
            Since $K/F$ is normal we know that $\sigma K \subset K$ for any embedding $\sigma$ over $F$. For $K(\zeta)$, we know that $1 = \sigma(1) = \sigma(\zeta^n) = \sigma(\zeta)^n$ so $\sigma\zeta = \zeta^l$ for some $l \in \{1,..., n\}$. Therefore $\sigma K(\zeta) \subset K(\zeta)$ so $K(\zeta)/F$ is normal.
        \end{proof}
    \end{mdframed}
    \item[(b)] Prove that $\text{Gal}(K(\zeta)/F)$ is solvable.
    \begin{mdframed}
        

    \begin{proof}
        Since $K/F$ is a normal radical extension we have that
        \[
        K = F(\alpha_1, ... , \alpha_r) \text{ and } \alpha_i^n \in F(\alpha_1, ... , \alpha_{i-1})
        \]
        Therefore 
        $$
        K(\zeta) = F(\zeta, \alpha_1, ... , \alpha_r)
        $$
        Let $F_0 = F, F_1 = F(\zeta), F_i = F(\zeta, \alpha, ... , \alpha_{i-1})$ so 
        \[
        F = F_0 \subset F_1 \subset \cdots \subset F_{r+1} = K(\zeta)
        \]
        Now let $H_i = \text{Gal}(K(\zeta)/F_i)$ so 
        \[
        \text{Gal}(K(\zeta)/F) = H_0 \supset H_1 \supset \cdots \supset H_{r+1} = \{e\}
        \]
        Now, since each $F_i$ is an intermediate field of $K(\zeta) / F$ we know that $F_i/F$ is normal so $H_{i+1}$ is normal in $H_i$ and 
        \[
        H_i/H_{i+1} \cong \text{Gal}(F_{i+1}/F_{i})
        \]
        Which is abelian.
    \end{proof}

    \end{mdframed}
    \item[(c)] Deduce that $\text{Gal}(K/F)$ is solvable.
\end{enumerate}

\subsection*{Problem 11}
Let $K/F$ be an $n$-radical extension. In this problem, we prove that there exists a normal $n$-radical extension $N/F$ such that $K \subset N$. We know that there exist $\alpha_1, \ldots, \alpha_r \in K$ such that $K = F(\alpha_1, \ldots, \alpha_r)$ and $\alpha_i^n \in F(\alpha_1, \ldots, \alpha_{i-1})$ for each $i$. Let $\sigma_1, \ldots, \sigma_m$ be all the embeddings of $K$ into $\mathbb{C}$ over $F$, and let
\[
N = F(\sigma_1(\alpha_1), \ldots, \sigma_m(\alpha_1), \ldots, \sigma_1(\alpha_r), \ldots, \sigma_m(\alpha_r)).
\]
One of the embeddings, say $\sigma_1$, is the inclusion map of $K$ into $\mathbb{C}$, so we have $\sigma_1(\alpha_i) = \alpha_i$ for all $i$ and thus $K \subset N$.
\begin{enumerate}
    \item[(a)] Prove that $N/F$ is an $n$-radical extension.
    \begin{mdframed}
        \begin{proof}
            For any $i \in 1,...,r$ and $j \in 1,...,m$ we have that 
            \[
            \sigma_j(\alpha_i)^n = \sigma_j(\alpha_i^n)\in F(\sigma_j(\alpha_1), ..., \sigma_j (\alpha_{i-1})) \subset N
            \]
        \end{proof}
    \end{mdframed}
    \item[(b)] Prove that $N/F$ is a normal extension.
    \begin{mdframed}
        \begin{proof}
            Let $\tau : N \to \mathbb C$ be an embedding. Fix $j = 1,...,m$ then 
            \[
            \sigma_jK = F(\sigma_j\alpha_1, ..., \sigma_j \alpha_r)
            \]
            We can define the composition $\tau \circ \sigma_j : K \to\mathbb C$ and it is an embedding over $F$. Hence we must have \[
            \tau \circ \sigma_j = \sigma_h \quad h \in 1,...,m
            \]
            Therefore, for each $i$ we have 
            \[
            \tau \circ \sigma(\alpha_i) = \sigma_h(\alpha_i)
            \]
            So $\tau N \subset N$
         \end{proof}
    \end{mdframed}
\end{enumerate}

\subsection*{Problem 12}
Let $f \in F[t]$. Assume that $f$ is solvable by radicals over $F$. Prove that the Galois group of $f$ is a solvable group.

\begin{mdframed}
    \begin{proof}
        Let $E$ be the splitting field of $f$. Then $E \subset K$ where $K$ is an n-radical extension. We proved that there exists a normal radical extension $N$ such that $K\subset N$ and $N/F$ is normal. Since $N/F$ is a normal radical extension, $\text{Gal}(N/F)$ is a solvable group.\\

        $E$ is an intermediate field of $N/F$ and $E/F$ is normal, so the restriction $\text{Gal}(N/F) \to \text{Gal}(E/F)$ is a surjective homomorphism. Meaning $\text{Gal}(E/F)$ is solvable.
    \end{proof}
\end{mdframed}

\subsection*{Problem 13}
Let $G$ be a subgroup of $S_n$. Define a relation $\sim$ on the set $\{1, \ldots, n\}$ by $i \sim j$ if:
either $i = j$ or the transposition $(ij)$ lies in $G$.
\begin{enumerate}
    \item[(a)] Prove that $\sim$ is an equivalence relation.
    \item[(b)] Let $\sigma \in G$ and $i, j \in \{1, \ldots, n\}$. Prove that if $j$ lies in the equivalence class of $i$, then $\sigma(j)$ lies in the equivalence class of $\sigma(i)$.
    \item[(c)] Let $\sigma \in G$ and $i \in \{1, \ldots, n\}$. Prove that the equivalence class of $i$ and the equivalence class of $\sigma(i)$ have the same number of elements.
    \item[(d)] Assume that:
    \begin{itemize}
        \item $n$ is a prime number,
        \item $G$ is a transitive subgroup of $S_n$,
        \item $G$ contains a transposition.
    \end{itemize}
    Prove that $G = S_n$.
\end{enumerate}

\subsection*{Problem 14}
Let $f(t) = 2t^5 - 10t + 5 \in \mathbb{Q}[t]$.
\begin{enumerate}
    \item[(a)] Prove that $f$ is irreducible over $\mathbb{Q}$.
    \begin{mdframed}
        $f$ is irreducible over $\mathbb Q$ by Eisenstein with prime $5$.
    \end{mdframed}
    \item[(b)] Prove that $f$ has exactly 3 real roots.
    \begin{mdframed}
        $f'(t) = 10t^4 - 10$ has roots $-1,1$. With
        \[
            f'(-2) > 0, \quad f'(0) < 0, \quad f'(2) > 0
        \]
        This means that $f$ has $3$ real roots.
    \end{mdframed}
    \item[(c)] Prove that $G_f = S_5$.
    \begin{mdframed}
        Since $f$ is irreducible over $\mathbb Q$ we know that $G_f$ is a transitive subgroup of $S_5$. Let $\tau : \mathbb C \to \mathbb C$ be the conjugation mapping, then $\tau$ fixes the real roots of $f$ and transposes the complex roots. $\tau$ restricts to an automorphism of the splitting field of $f$ over $\mathbb Q$ and we get that $\pi_\tau$ is a transposition in $G_f$. Thus $G_f = S_5$.
    \end{mdframed}
    \item[(d)] Deduce that $f$ is not solvable by radicals over $\mathbb{Q}$.
    \begin{mdframed}
        The Galois group of $f$ is isomorphic to $G_f$ which is equal to $S_5$, and $S_5$ is not a solvable group. Therefore, $f$ is not solvable by radicals.
    \end{mdframed}
\end{enumerate}

\end{document}

\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathabx}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{pslatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{epsf}
\usepackage{tikz}
\usepackage{framed}
\usepackage{hyperref}
\geometry{top=1in, bottom=1in, left=1cm, right=1cm}
%\pagestyle{empty}

\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Tomas Sbardelotto Dos Santos}
\rfoot{Page \thepage}


\def\Z{\mathbb Z}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\title{}
\date{}

\begin{document}


\begin{enumerate}
\item For $x \in \mathbb{R}^n$, define
$$
\|x\|_{\infty}:=\max \left\{\left|x_1\right|,\left|x_2\right|, \ldots,\left|x_n\right|\right\},
$$

sometimes called the sup or the max norm.
\begin{enumerate}
\item Show that $\|\cdot\|_{\infty}$ is a norm on $\mathbb{R}^n$ (defining a
different distance).
\item What is the unit ball $B(0,1)$ in this norm?
\end{enumerate}

\begin{mdframed}

\textbf{Solution:}\\

To show that \(\|\cdot\|_{\infty}\) is a norm on \(\mathbb{R}^n\), we need to verify the following properties for \(x, y \in \mathbb{R}^n\) and \(\alpha \in \mathbb{R}\):

\begin{enumerate}
    \item \(\|x\|_{\infty} \geq 0\) and \(\|x\|_{\infty} = 0 \iff x = 0\).\\
    
    Since \(\|x\|_{\infty} = \max \left\{\left|x_1\right|,\left|x_2\right|, \ldots,\left|x_n\right|\right\}\), it is clear that \(\|x\|_{\infty} \geq 0\) because the absolute value of each component \(x_i\) is non-negative.
    
    Suppose \(\|x\|_{\infty} = 0\). This implies \(\max \left\{\left|x_1\right|,\left|x_2\right|, \ldots,\left|x_n\right|\right\} = 0\). Therefore, \(\left|x_i\right| = 0\) for all \(i\), which means \(x_i = 0\) for all \(i\). Hence, \(x = 0\).
    
    Conversely, if \(x = 0\), then \(\|x\|_{\infty} = \max \left\{0, 0, \ldots, 0\right\} = 0\).

    \item \(\| \alpha x \|_{\infty} = \left| \alpha \right| \| x \|_{\infty} \).\\
    
    We have
    \[
    \| \alpha x \|_{\infty} = \max \left\{\left| \alpha x_1 \right|, \left| \alpha x_2 \right|, \ldots, \left| \alpha x_n \right|\right\} = \max \left\{\left| \alpha \right| \left| x_1 \right|, \left| \alpha \right| \left| x_2 \right|, \ldots, \left| \alpha \right| \left| x_n \right|\right\} = \left| \alpha \right| \| x \|_{\infty}.
    \]

    \item \(\| x + y \|_{\infty} \leq \| x \|_{\infty} + \| y \|_{\infty}\).\\
    
    For any \(i\), we have \(\left| x_i + y_i \right| \leq \left| x_i \right| + \left| y_i \right|\) by the triangle inequality for absolute values. Thus,
    \[
    \| x + y \|_{\infty} = \max \left\{ \left| x_1 + y_1 \right|, \left| x_2 + y_2 \right|, \ldots, \left| x_n + y_n \right| \right\} \leq \max \left\{ \left| x_1 \right| + \left| y_1 \right|, \left| x_2 \right| + \left| y_2 \right|, \ldots, \left| x_n \right| + \left| y_n \right| \right\}.
    \]
    Since \(\max \left\{a_1 + b_1, a_2 + b_2, \ldots, a_n + b_n \right\} \leq \max \left\{a_1, a_2, \ldots, a_n \right\} + \max \left\{b_1, b_2, \ldots, b_n \right\}\), we have
    \[
    \| x + y \|_{\infty} \leq \| x \|_{\infty} + \| y \|_{\infty}.
    \]
\end{enumerate}

Thus, \(\|\cdot\|_{\infty}\) is a norm on \(\mathbb{R}^n\).

To find the unit ball \(B(0,1)\) in this norm, we consider all \(x \in \mathbb{R}^n\) such that \(\|x\|_{\infty} \leq 1\). This means
\[
\max \left\{\left|x_1\right|, \left|x_2\right|, \ldots, \left|x_n\right|\right\} \leq 1.
\]
Thus, \(|x_i| \leq 1\) for all \(i = 1, 2, \ldots, n\). Therefore, the unit ball in the \(\|\cdot\|_{\infty}\) norm is the set of all points \(x \in \mathbb{R}^n\) such that each component \(x_i\) satisfies \(-1 \leq x_i \leq 1\). This set forms an \(n\)-dimensional cube with side length 2 centered at the origin.

\end{mdframed}

\item A set $E \subseteq \mathbf{R}^n$ is said to be {\bf polygonally connected} if
and only if any two points $a, b \in E$ can be connected by a polygonal path in
$E$; that is, there exist points $x_k \in E, k=1, \ldots, N$, such that $x_0= a,
x_N = b$ and the line segment between $x_{k-1}$ and $ x_k $ (denoted by $L\left(x_{k-1} ; x_k\right)$ is contained in $E$ for $k=1, \ldots, N$. Prove that
every polygonally connected set in $\mathbf{R}^n$ is connected.
\begin{mdframed}
\textbf{Solution:}

To prove that every polygonally connected set in $\mathbb{R}^n$ is connected, we need to show that for any two points $a, b \in E$, there exists a continuous path within $E$ that connects $a$ to $b$. 

Given that $E \subseteq \mathbb{R}^n$ is polygonally connected, for any two points $a, b \in E$, there exist points $x_k \in E, k=1, \ldots, N$, such that $x_0 = a$, $x_N = b$, and the line segment $L(x_{k-1}; x_k)$ is contained in $E$ for $k=1, \ldots, N$.

We can construct a continuous path from $a$ to $b$ by joining these line segments. Define the path $\gamma : [0,1] \to E$ as follows:

1. Divide the interval $[0,1]$ into $N$ subintervals: $[0, \frac{1}{N}], [\frac{1}{N}, \frac{2}{N}], \ldots, [\frac{N-1}{N}, 1]$.
2. For each subinterval $[\frac{k-1}{N}, \frac{k}{N}]$, define $\gamma$ to linearly interpolate between $x_{k-1}$ and $x_k$:
   \[
   \gamma(t) = x_{k-1} + (t - \frac{k-1}{N})N(x_k - x_{k-1}), \quad \text{for } t \in [\frac{k-1}{N}, \frac{k}{N}]
   \]

The function $\gamma$ is continuous on each subinterval and connects the points $x_{k-1}$ to $x_k$. Since $\gamma$ is piecewise linear and $L(x_{k-1}; x_k) \subseteq E$ for all $k$, $\gamma(t) \in E$ for all $t \in [0,1]$.

Thus, $\gamma$ is a continuous path in $E$ from $a$ to $b$, proving that $E$ is connected.

\end{mdframed}

\item Let $C([a, b], \mathbb{R})$ be the set of continuous functions and $C^1([a,
b], \mathbb{R})$ the set of once continuously differentiable functions on $[a, b]$.
Define
$$
d_C(f, g):=\|f-g\|_{[a, b]} \quad \text { and } \quad d_{C^1}(f, g):=\|f-g\|_{[a,
b]}+\left\|f^{\prime}-g^{\prime}\right\|_{[a, b]},
$$
where $\|\cdot\|_{[a, b]}$ is the uniform norm. By problem 6 in Hw 3, we know that
$C([a, b], \mathbb{R})$ with $d_C$ is a metric space and so is $C^1([a, b], \mathbb{R})$ with $d_{C^1}$.
Prove that the derivative operator $D: C^1([a, b], \mathbb{R}) \rightarrow C([a,
b], \mathbb{R})$ defined by $D(f):=f^{\prime}$ is continuous.

\begin{mdframed}

\textbf{Solution:}\\

Let \( D: C^1([a, b], \mathbb{R}) \to C([a, b], \mathbb{R}) \) be the derivative operator defined by \( D(f) = f' \). We want to prove that \( D \) is continuous when \( C^1([a, b], \mathbb{R}) \) is equipped with the metric \( d_{C^1} \) and \( C([a, b], \mathbb{R}) \) is equipped with the metric \( d_C \).

To show that \( D \) is continuous, we need to prove that for any sequence \(\{f_n\}\) in \( C^1([a, b], \mathbb{R}) \) such that \( f_n \to f \) in \( d_{C^1} \), it follows that \( D(f_n) \to D(f) \) in \( d_C \). That is, we need to show that
\[
d_{C^1}(f_n, f) \to 0 \implies d_C(f_n', f') \to 0.
\]

Recall the definitions of the metrics:
\[
d_C(f, g) = \|f - g\|_{[a, b]},
\]
\[
d_{C^1}(f, g) = \|f - g\|_{[a, b]} + \|f' - g'\|_{[a, b]},
\]
where \(\| \cdot \|_{[a, b]}\) denotes the uniform norm on \([a, b]\).

Given \( d_{C^1}(f_n, f) \to 0 \), we have
\[
\|f_n - f\|_{[a, b]} + \|f_n' - f'\|_{[a, b]} \to 0.
\]
This implies that both \(\|f_n - f\|_{[a, b]} \to 0\) and \(\|f_n' - f'\|_{[a, b]} \to 0\).

Since \(\|f_n' - f'\|_{[a, b]} \to 0\), we have \( d_C(f_n', f') = \|f_n' - f'\|_{[a, b]} \to 0 \). Therefore, \( D(f_n) \to D(f) \) in the metric \( d_C \), proving that \( D \) is continuous.

\end{mdframed}


\item Prove that the Bolzano-Weierstrass Property does not hold for $\mathcal{C}[a,
b]$ and $\|f\| = \sup_{x\in[a, b]} |f(x)|$. Namely, prove that if $f_n(x)=x^n$, then
$\left\|f_n\right\|$ is bounded but $\left\|f_{n_k}-f\right\|$ does not converge
for any $f \in \mathcal{C}[0,1]$ and any subsequence $\left\{n_k\right\}$.
\begin{mdframed}

\textbf{Solution:}\\

First, we verify that the sequence is bounded:
\[
\|f_n\| = \sup_{x \in [0, 1]} |x^n| = 1.
\]
This shows that \( \|f_n\| \) is indeed bounded, as the supremum is achieved when \( x = 1 \) for all \( n \).

Assume by way of contradiction that there exists some function \( f \in \mathcal{C}[0,1] \) and a subsequence \( \{n_k\} \) such that
\[
\lim_{k \to \infty} \|f_{n_k} - f\| = 0.
\]
This implies that
\[
\lim_{k \to \infty} \sup_{x \in [0,1]} |x^{n_k} - f(x)| = 0.
\]
In particular, for any \( x \in [0,1) \), we know \( x^{n_k} \to 0 \) as \( k \to \infty \) since \( x^{n_k} \) approaches zero for any \( x \) less than one. Therefore, \( f(x) \) must be \( 0 \) for all \( x \in [0,1) \) for the supremum norm of \( x^{n_k} - f(x) \) to approach zero.

At \( x = 1 \), we have \( f(1) \) must equal \( 1 \) because \( 1^{n_k} = 1 \) for all \( k \). Hence, \( f(x) \) should be discontinuous at \( x = 1 \), contradicting the assumption that \( f \) is continuous over \([0,1]\).

Therefore, no such \( f \) and \( \{n_k\} \) exist, and the sequence \( \{f_n\} \) does not have any convergent subsequence in the given norm, disproving the Bolzano-Weierstrass property for \(\mathcal{C}[0,1]\) with the supremum norm.

\end{mdframed}


\end{enumerate}




\end{document}


\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathabx}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{pslatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{epsf}
\usepackage{tikz}
\usepackage{framed}
\usepackage{hyperref}
\geometry{top=1in, bottom=1in, left=1cm, right=1cm}
%\pagestyle{empty}

\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Tomas Sbardelotto Dos Santos}
\rfoot{Page \thepage}


\def\Z{\mathbb Z}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\title{}
\date{}

\begin{document}
\begin{enumerate}
\item Suppose $\left(X, d_X\right),\left(Y, d_Y\right)$ are metric spaces and $f: X
\rightarrow Y$ is continuous. Let $A \subset X$.
\begin{enumerate}
\item Show that $f(\bar{A}) \subset \overline{f(A)}$.
\item Show that the subset can be proper.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
    
    Suppose \(y \in \overline{A}\). By definition of the closure, for every \( \epsilon > 0 \), there exists \( x \in A \) such that \( d_X(x, y) < \epsilon \). Consider a sequence \(\{x_n\}\) in \(A\) converging to \(y\). By the continuity of \(f\), the sequence \(\{f(x_n)\}\) converges to \(f(y)\) in \(Y\). Since each \(f(x_n) \in f(A)\), and \(\{f(x_n)\}\) converges to \(f(y)\), it follows that \(f(y) \in \overline{f(A)}\). Hence, every element of \(f(\overline{A})\) is in \(\overline{f(A)}\), proving that \(f(\overline{A}) \subset \overline{f(A)}\).

Suppose \(f\) is not injective, meaning there exist distinct elements \(x_1, x_2 \in X\) such that \(f(x_1) = f(x_2)\). Consider the sets \(A = \{x_1\}\) and \(\overline{A} = \{x_1\}\), as singletons are closed. Then \(f(A) = \{f(x_1)\}\) and \(f(\overline{A}) = \{f(x_1)\} = f(A)\). However, if \(x_2\) is not a limit point of \(A\) but maps to the same value, \(\overline{f(A)}\) could potentially include other limit points of sequences from \(f(A)\) that do not necessarily come from points in \(\overline{A}\).
    
\end{mdframed}

\item Suppose $f: X \rightarrow Y$ is continuous for metric spaces $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$. Show that if $X$ is connected, then $f(X)$ is
connected.
\begin{mdframed}
    \textbf{Solution:}\\

    Suppose $f$ is continuous, $X$ is connected, and $f(X)$ is not connected.\\

    Then there are $U,V \subset Y$ which are nonempty, disjoint open sets and 
    \[
    f(X) = f(X) \cap U\cup V
    \]
    By the properties of preimages of unions we have
    \[
    X = f^{-1}(f(X) \cap U\cup V) = X \cap f^{-1}(U) \cup f^{-1}(V)
    \]
    
    Since $f$ is continuous, the preimages of $U,V$ are also open, nonempty, and disjoint which means that $X$ is disconnected. A contradiction. Thus if $X$ is connected, $f(X)$ is connected.

    
\end{mdframed}

\item Prove the following version of the intermediate value theorem. Let $(X, d)$
be a connected metric space and $f: X \rightarrow \mathbb{R}$ a continuous
function. Suppose that there exist $x_0, x_1 \in X$ and $y \in \mathbb{R}$ such
that $f\left(x_0\right)<y<f\left(x_1\right)$. Then prove that there exists a $z \in
X$ such that $f(z)=y$. \\{\it Hint: See Exercise 2.}
\begin{mdframed}
    \textbf{Solution:}\\
    Assume for contradiction that no such \(z\) exists where \(f(z) = y\). Consider the sets:
\[
A = \{x \in X : f(x) < y\} \quad \text{and} \quad B = \{x \in X : f(x) > y\}
\]
Since \(f\) is continuous and \(f(x_0) < y\) while \(f(x_1) > y\), \(A\) and \(B\) are non-empty. Moreover, both \(A\) and \(B\) are open in \(X\). To see why, consider any \(x \in A\). Since \(f\) is continuous, there exists an \(\epsilon > 0\) such that for all \(x' \in X\) within \(\epsilon\)-distance from \(x\), \(f(x') < y\), implying \(x' \in A\). A similar argument holds for \(B\).

Given that \(A\) and \(B\) are both open, non-empty, and \(X = A \cup B\) (since there is no \(x \in X\) with \(f(x) = y\)), \(X\) is expressed as a union of two disjoint non-empty open sets. This contradicts the assumption that \(X\) is connected.

Therefore, the assumption that there is no \(z \in X\) such that \(f(z) = y\) must be false. Hence, there exists some \(z \in X\) such that \(f(z) = y\), which completes the proof.

    
\end{mdframed}

\item A continuous function $f: X \rightarrow Y$ for metric spaces $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$ is said to be proper if for every compact set
$K \subset Y$, the set $f^{-1}(K)$ is compact. Suppose a continuous $f:(0,1) \rightarrow(0,1)$ is proper and $\left\{x_n\right\}$ is a sequence in $(0,1)$ that
converges to 0 . Show that $\left\{f\left(x_n\right)\right\}$ has no subsequence
that converges in $(0,1)$.
\begin{mdframed}
    \textbf{Solution:}\\
    Assume for contradiction that there exists a subsequence \( \{f(x_{n_k})\} \) converging to some \( y \in (0,1) \). Let \( K = \{y\} \), which is a compact set in \( (0,1) \) because it is closed and bounded in the metric space \( (0,1) \).

Since \( f \) is proper, the preimage \( f^{-1}(K) \) must also be compact. Thus, the set \( f^{-1}(\{y\}) \) is compact in \( (0,1) \). Being compact in \( (0,1) \), it must be closed and bounded, and hence it must contain its limit points.

However, since \( \{x_{n_k}\} \) is a subsequence of \( \{x_n\} \) and \( \{x_n\} \) converges to \(0\), which is outside of \( (0,1) \), the sequence \( \{x_{n_k}\} \) cannot have limit points in \( (0,1) \). This contradicts the fact that \( f^{-1}(\{y\}) \) is compact, as it would imply \( f^{-1}(\{y\}) \) should contain the limit point \( 0 \), which it cannot since \( 0 \notin (0,1) \).

Therefore, the assumption that there exists a converging subsequence \( \{f(x_{n_k})\} \) must be incorrect. Hence, the sequence \( \{f(x_n)\} \) cannot have any convergent subsequences within \( (0,1) \).

    
\end{mdframed}

\item Take the metric space of continuous functions $C([0,1], \mathbb{R})$. Let $k:
[0,1] \times[0,1] \rightarrow \mathbb{R}$ be a continuous function. Given $f \in
C([0,1], \mathbb{R})$ define
$$
\varphi_f(x):=\int_0^1 k(x, y) f(y) d y .
$$
\begin{enumerate}
\item Show that $T(f):=\varphi_f$ defines a function $T: C([0,1], \mathbb{R}) \rightarrow C([0,1], \mathbb{R})$.
\item Show that $T$ is continuous.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
The map \( \varphi_f \) is continuous by the continuity of \( k \) and \( f \), and the linearity and boundedness of integration. For each \( x, x' \in [0,1] \):
\[
|\varphi_f(x) - \varphi_f(x')| \leq \int_0^1 |k(x, y) - k(x', y)| |f(y)| dy.
\]
Since \( k \) is uniformly continuous on a compact set and \( f \) is bounded, this integral shows \( \varphi_f \) is continuous, and thus, \( T(f) = \varphi_f \) defines a function from \( C([0,1], \mathbb{R}) \) to itself.

Assume \( \{f_n\} \) in \( C([0,1], \mathbb{R}) \) converges uniformly to \( f \). For every \( x \in [0,1] \):
\[
\begin{aligned}
|\varphi_{f_n}(x) - \varphi_f(x)| &= \left|\int_0^1 k(x, y) (f_n(y) - f(y)) dy\right| \\
&\leq \int_0^1 |k(x, y)| |f_n(y) - f(y)| dy \\
&\leq \sup_{y \in [0,1]} |f_n(y) - f(y)| \int_0^1 |k(x, y)| dy.
\end{aligned}
\]
Given \( k \) is bounded, \( T(f_n) \rightarrow T(f) \) uniformly, proving \( T \) is continuous.


    
\end{mdframed}

\item Let $(X, d)$ be a compact metric space, let $C(X, \mathbb{R})$ be the set of
real-valued continuous functions. Define
$$
d(f, g):=\|f-g\|_u:=\sup _{x \in X}|f(x)-g(x)| .
$$

\begin{enumerate}
\item Show that d makes $C(X, \mathbb{R})$ into a metric space.
\item Show that for any $x \in X$, the evaluation function $E_x: C(X, \mathbb{R}) \
rightarrow \mathbb{R}$ defined by $E_x(f):=f(x)$ is a continuous function.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
By definition, \( |f(x) - g(x)| \geq 0 \) for any \( x \in X \), so \( \sup_{x \in X} |f(x) - g(x)| \geq 0 \). If \( f = g \), then \( |f(x) - g(x)| = 0 \) for all \( x \), so \( d(f, g) = 0 \). Conversely, if \( d(f, g) = 0 \), then \( \sup_{x \in X} |f(x) - g(x)| = 0 \), implying \( |f(x) - g(x)| = 0 \) for all \( x \) and thus \( f = g \).

 \( d(f, g) = \sup_{x \in X} |f(x) - g(x)| = \sup_{x \in X} |g(x) - f(x)| = d(g, f) \).

 For any \( x \in X \),
\[
|f(x) - h(x)| \leq |f(x) - g(x)| + |g(x) - h(x)|,
\]
so taking supremums,
\[
d(f, h) = \sup_{x \in X} |f(x) - h(x)| \leq \sup_{x \in X} (|f(x) - g(x)| + |g(x) - h(x)|) \leq d(f, g) + d(g, h).
\]

Thus, \( d \) is a metric on \( C(X, \mathbb{R}) \).


Consider the evaluation function \( E_x: C(X, \mathbb{R}) \rightarrow \mathbb{R} \) defined by \( E_x(f) = f(x) \). To show \( E_x \) is continuous, consider a sequence \( \{f_n\} \) in \( C(X, \mathbb{R}) \) converging to \( f \) under \( d \), i.e., \( \|f_n - f\|_u \rightarrow 0 \).

By definition of \( d \), for any \( \epsilon > 0 \), there exists \( N \) such that for all \( n \geq N \), \( \sup_{x \in X} |f_n(x) - f(x)| < \epsilon \). Thus, \( |f_n(x) - f(x)| < \epsilon \) for all \( x \) and particularly for the fixed \( x \), showing \( E_x(f_n) \rightarrow E_x(f) \) and hence \( E_x \) is continuous.

    
\end{mdframed}

\item Let $(X, d)$ be a metric space, $S \subset X$, and $p \in X$. Prove that $p$
is a cluster point of $S$ if and only if $p \in \overline{S \backslash\{p\}}$.
\begin{mdframed}
    \textbf{Solution:}\\

\textbf{$\implies$} Suppose \(p\) is a cluster point of \(S\). By definition, for every \(\epsilon > 0\), there exists some \(x \in S\) with \(x \neq p\) such that \(d(p, x) < \epsilon\). This implies that each neighborhood of \(p\) contains points from \(S\) other than \(p\) itself, namely from \(S \setminus \{p\}\). Hence, \(p\) is an accumulation point of \(S \setminus \{p\}\) and thus \(p \in \overline{S \setminus \{p\}}\).

\textbf{$\impliedby$} Now assume \(p \in \overline{S \setminus \{p\}}\). This means that every neighborhood of \(p\) contains at least one point from \(S \setminus \{p\}\). Specifically, for every \(\epsilon > 0\), there exists \(x \in S \setminus \{p\}\) such that \(d(p, x) < \epsilon\). Since these \(x\) are distinct from \(p\) and belong to \(S\), \(p\) is a cluster point of \(S\).

Thus, \(p\) being a cluster point of \(S\) is equivalent to \(p \in \overline{S \setminus \{p\}}\).

    
\end{mdframed}

\item Let $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$ be metric spaces, $S \subset X, p \in X$ a cluster point of $S$, and let $f: S \rightarrow Y$ be a
function. Prove that $f: S \rightarrow Y$ is continuous at $p$ if and only if
$$
\lim _{x \rightarrow p} f(x)=f(p) .
$$
\begin{mdframed}
    \textbf{Solution:}\\


\textbf{$\implies$} Assume that \(f\) is continuous at \(p\). By definition, this means for every \(\epsilon > 0\), there exists a \(\delta > 0\) such that for all \(x \in S\) with \(d_X(x, p) < \delta\), we have \(d_Y(f(x), f(p)) < \epsilon\). Since \(p\) is a cluster point of \(S\), for every \(\delta > 0\), there exists \(x \in S\) such that \(x \neq p\) and \(d_X(x, p) < \delta\). Therefore, \(f(x)\) gets arbitrarily close to \(f(p)\) as \(x\) approaches \(p\) from within \(S\). This shows that \(\lim_{x \to p} f(x) = f(p)\).

\textbf{$\impliedby$} Now assume that \(\lim_{x \to p} f(x) = f(p)\). This means that for every \(\epsilon > 0\), there exists a \(\delta > 0\) such that for all \(x \in S\) with \(d_X(x, p) < \delta\), \(d_Y(f(x), f(p)) < \epsilon\). This definition directly matches the definition of continuity at \(p\). Thus, \(f\) is continuous at \(p\).
    
    
\end{mdframed}

\end{enumerate}




\end{document}



\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{microtype} 
\usepackage{geometry}
\usepackage{hyperref} 
\usepackage{fancyhdr}

\newcommand{\mycomment}[1]{}

\geometry{top=1in, bottom=1in, left=1in, right=1in}


\pagestyle{fancy}
\fancyhf{}
\lhead{Weierstrass Approximation Theorem}
\rhead{Math 151B}
\rfoot{Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\begin{document}

\title{Exploring the Weierstrass and Stone-Weierstrass Theorems: Foundations, Proofs, and Applications in Mathematical Analysis}
\author{Tomas Sbardelotto Dos Santos, Jason Palos, Darius Mahamedi\footnote{Group Representative}, 
\\Letian Zhang, Daniel Suh}
\date{March 16, 2024}
\maketitle









\begin{abstract}
The following is an analysis of the Weierstrass Approximation Theorem and its generalization, the Stone-Weierstrass Theorem. Introduced by Karl Weierstrass in 1885, the Weierstrass Approximation Theorem establishes the principle that any continuous function defined over a closed interval can be uniformly approximated by polynomial functions with arbitrary precision. This theorem has become a cornerstone in several mathematical disciplines, including numerical analysis, approximation theory, and functional analysis, highlighting the central role of polynomials as simple yet powerful tools for computation and theoretical exploration. Weierstrass's original demonstration of this theorem, followed by Sergei Bernstein's proof using Bernstein polynomials in 1911, marked significant advancements in the field. Furthermore, we will go over Marshall H. Stone's subsequent generalization which expanded the theorem's scope to encompass any compact Hausdorff space and a broader class of functions. We will explore the preliminary theorems and definitions needed to understand Weierstrass' approximation theorem. We will then show Weierstrass' original proof of the theorem with a simple application, as well as Bernstein's later proof and its applications to graphics and modeling. Lastly, we will explore the Stone-Weierstrass theorem and its proof, along with some applications in approximation theory.
\end{abstract}

\newpage
\tableofcontents
\newpage
























\section{The Weierstrass Approximation Theorem}

\subsection{Introduction}

Being so powerful, there have been many updates, corollaries, proofs, and the like involving the Weierstrass Approximation Theorem. We will discuss Weierstrass' original proof of his theorem as it is important to understand its origins and the historical techniques that were employed during his time. Weierstrass' proof uses clever methods that we haven't gone over in depth in 151B but are not far beyond our understanding. We also include Bernstein's later proof as it underpins function approximation in many real-world applications. Bernstein's proof created a formula for approximating functions using Bernstein polynomials. Because of this, computationally efficient algorithms were created to evaluate Bernstein polynomials which led to their adoption in computation and graphics.


\subsection{Preface to the Proofs}

When looking at both Bernstein's and Weierstrass' proofs, compare their proofs with examples we've worked on in 151A and 151B. The layout of the proof is similar to the work we've done in showing the convergence of sequences of functions. The key difference in Weierstrass and Bernstein's proofs is in the construction of the polynomials:\\

Weierstrass' original proof used more indirect methods to construct polynomials that would converge to a given function. We begin by convolving the Gaussian function with our original function to create an approximation that is locally analytic. This allows us to use Taylor's theorem to find a polynomial approximation for our function.\\

Bernstein's proof provides a direct method to approximate a continuous function on a closed interval by polynomials. He introduced what are now known as Bernstein polynomials. This proof is slightly more intuitive and accessible, as it constructs a sequence of polynomials explicitly designed to converge to the given function.

\subsection{Weierstrass' Proof}

The hardest part of Weierstrass' original proof is setting up the proper inequalities to show convergence. We will lay out the preliminary theorems and definitions before diving into the proof.

\subsubsection{Preliminaries}
To prove Weierstrass theorem we must first list some key definitions used in the proof.


\begin{definition}[Uniform Continuity \cite{Lebl}]
    A function is said to be uniformly continuous on an interval $I$ if for all $\varepsilon>0$ there is $\delta>0$ such that whenever $x,y\in I$
    \[
    \abs{x-y}<\delta \implies \abs{f(x)-f(y)}<\varepsilon
    \]
\end{definition}

\begin{definition}[Uniform Convergence \cite{Lebl}]
    Let $f_n:S\to\mathbb{R}$ and $f:S\to\mathbb{R}$ be functions. The sequence $\{f_n\}_{n=1}^\infty$ converges uniformly to $f$ if for every $\varepsilon>0$ there is $N\in\mathbb{N}$ such that for all $n\geq N$ 
    \[
    \abs{f_n(x)-f(x)}<\varepsilon
    \]
\end{definition}

\begin{definition}[Sequence of Functions \cite{Lebl}]
    A sequence of functions \((f_n)\) from a set \(X\) to a set \(Y\) is a collection of functions \(f_1, f_2, f_3, \ldots\) such that for each \(n \in \mathbb{N}\), \(f_n\) is a function from \(X\) to \(Y\). Formally, we can write:
\[
(f_n)_{n=1}^{\infty} = \{f_1, f_2, f_3, \ldots\}
\]
where \(f_n: X \rightarrow Y\) for all \(n \in \mathbb{N}\).
\end{definition}

\begin{definition}[Convolution \cite{Hipp}]
    The convolution of two functions \(f\) and \(g\), denoted by \(f * g\), is defined as the integral of the product of the two functions after one is reversed and shifted. For functions defined on the real line, it is given by:
    \[
    (f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) \, d\tau
    \]
    where \(\tau\) is the variable of integration, and \(t\) is the location in which the integral is being evaluated. 
\end{definition}


\begin{definition}[Approximate Identity \cite{Hipp}]
    Let \(\varphi \in L_1(\mathbb{R})\). Then \(\varphi\) is called an \textit{approximate identity} if
\[
\int_{\mathbb{R}} \varphi(t) \, dt = 1.
\]
Where $L_1$ denotes absolutely integrable functions on $\mathbb{R}.$
\end{definition}
    



\begin{theorem}[Heine-Cantor Theorem \cite{Lebl}]
    If \(f: I \rightarrow \mathbb{R}\) is a continuous function on a compact set \(I\) in \(\mathbb{R}\), then \(f\) is uniformly continuous on \(I\).
\end{theorem}

\begin{theorem}[Taylor's Theorem \cite{Lebl}]
    For an $n$ times differentiable function f defined near a point $x_0$ we define the $n^{th}$ order Taylor Polynomial for $f$ at $x_0$ as \[
    P^{x_0}_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
    \]
\end{theorem}

\subsubsection{Weierstrass' Original Proof}

We begin by noting for $\varphi : \mathbb{R} \to \mathbb{R}$
\[
\varphi (t) = \frac{1}{\sqrt{\pi}}e^{-t^2}
\]
Is an approximate identity. We then form the family of functions $\{\varphi_h\}_{h>0}$ where
\[
\varphi_h(t) = \frac{1}{h}\varphi\left(\frac{t}{h}\right) = \frac{1}{h\sqrt{\pi}}\int_{-\infty}^{\infty} e^{-(\frac{t}{h})^2} \; dt
\]


\noindent For a bounded uniformly continuous function \( f : \mathbb{R} \to \mathbb{R} \) define for \( h > 0 \)
\[
S_hf(x) = (f * \varphi_h)(x) = \frac{1}{h\sqrt{\pi}} \int_{-\infty}^{\infty} f(u)e^{-\left(\frac{u-x}{h}\right)^2} du
\]

To prove Weierstrass' Theorem we must first show $(f*\varphi_h)$ converges uniformly to $f$ as $h\to 0^+$.

\begin{theorem}
    Let \( f : \mathbb{R} \to \mathbb{R} \) be a bounded uniformly continuous function. Then \( S_hf \) converges uniformly to \( f \) as \( h \to 0 \).
\end{theorem} 
\begin{proof}
Let \( \varepsilon > 0 \). Then there exists \( \delta > 0 \) such that \( |f(x) - f(y)| \leq \frac{\varepsilon}{2} \) for all \( x,y \in \mathbb{R} \) with \( |x - y| < \delta \). Assume \( |f(x)| \leq M \) on \( \mathbb{R} \). Using that \( \int_{-\infty}^{\infty} e^{-v^2} dv = \sqrt{\pi} \), one also verifies easily that
\[
\frac{1}{h\sqrt{\pi}} \int_{-\infty}^{\infty} e^{-\left(\frac{u-x}{h}\right)^2} du = 1.
\]
This implies that we can write
\[
f(x) = \frac{1}{h\sqrt{\pi}} \int_{-\infty}^{\infty} f(x)e^{-\left(\frac{u-x}{h}\right)^2} du.
\]
Now let \( h_0 > 0 \) such that \( h_0 < \frac{\delta\varepsilon}{2M\sqrt{\pi}} \), then
\[
|S_hf(x) - f(x)| \leq \frac{1}{h\sqrt{\pi}} \int_{-\infty}^{\infty} |f(u) - f(x)|e^{-\left(\frac{u-x}{h}\right)^2} du
\]
\[
\leq \frac{\varepsilon}{2} + \frac{1}{h\sqrt{\pi}} \int_{|u-x|\geq\delta} |f(u) - f(x)|e^{-\left(\frac{u-x}{h}\right)^2} du
\]
\[
\leq \frac{\varepsilon}{2} + \frac{2M}{h\sqrt{\pi}} \int_{|u-x|\geq\delta} e^{-\left(\frac{u-x}{h}\right)^2} du
\]
\[
= \frac{\varepsilon}{2} + \frac{2M}{\sqrt{\pi}} \int_{|v|\geq\frac{\delta}{h}} |v|e^{-v^2} dv
\]
\[
\leq \frac{\varepsilon}{2} + \frac{4Mh}{\sqrt{\pi}} \int_{0}^{\infty} ve^{-v^2} dv
\]
\[
= \frac{\varepsilon}{2} + \frac{2Mh}{\delta\sqrt{\pi}} < \varepsilon
\]
for all \( 0 < h \leq h_0 \) and all \( x \in \mathbb{R} \).
\end{proof}


Now we have shown that $(f * \varphi_h)$ converges to $f$. Obviously, $(f * \varphi_h)$ is not a polynomial, but our new function is now analytic which means we can use Taylor's theorem to find the power series expansion of our $\varphi$ term. Since the power series converges uniformly to $f$ on bounded intervals, we will then have a sequence of polynomials which converge to $f$. \cite{Hipp}

\begin{theorem}[Weierstrass Approximation Theorem]
    Let $f:[a,b]\to\mathbb{R}$ be continuous then there is a sequence of polynomials $P_n(x)$ that converges uniformly to $f$.
\end{theorem}

\begin{proof}
We begin by extending \( f \) to a bounded uniformly continuous function on \( \mathbb{R} \) by defining \( f(x) = f(a)(x - a + 1) \) on \( [a - 1, a] \), \( f(x) = -f(b)(x - b - 1) \) on \( (b, b + 1] \), and \( f(x) = 0 \) on \( \mathbb{R} \setminus [a - 1, b + 1] \). In particular there exists \( R > 0 \) such that \( f(x) = 0 \) for \( |x| > R \). Let \( \varepsilon > 0 \) and \( M \) such that \( |f(x)| \leq M \) for all \( x \). Then by the above theorem there exists \( h_0 > 0 \) such that for all \( x \in \mathbb{R} \) we have \( |S_{h_0} f(x) - f(x)| < \frac{\varepsilon}{2} \). Since \( f(u) = 0 \) for \( |u| > R \), we can write

\[
S_{h_0} f(x) = \frac{1}{h_0 \sqrt{\pi}} \int_{-R}^{R} f(u)e^{-\frac{(u-x)^2}{h_0}} du.
\]

On \( \left[ -\frac{2R}{h_0}, \frac{2R}{h_0} \right] \) the power series of \( e^{-v^2} \) converges uniformly, so there exists \( N \) such that

\[
\left| \frac{1}{h_0 \sqrt{\pi}} - \sum_{k=0}^{N} \frac{(-1)^k}{k!} \left( \frac{u - x}{h_0} \right)^{2k} \right| < \frac{\varepsilon}{4RM}
\]

for all \( |x| \leq R \) and all \( |u| \leq R \), since in that case \( |u - x| \leq 2R \). This implies that

\[
\left| S_{h_0} f(x) - \frac{1}{h_0 \sqrt{\pi}} \int_{-R}^{R} f(u) \left[ \sum_{k=0}^{N} \frac{(-1)^k}{k!} \left( \frac{u - x}{h_0} \right)^{2k} \right] du \right| < \frac{\varepsilon}{2}
\]

for all \( |x| \leq R \). If we put

\[
P(x) = \frac{1}{h_0 \sqrt{\pi}} \int_{-R}^{R} f(u) \left[ \sum_{k=0}^{N} \frac{(-1)^k}{k!} \left( \frac{u - x}{h_0} \right)^{2k} \right] du,
\]

then \( P(x) \) is a polynomial in \( x \) of degree at most \( 2N \) such that \( |S_{h_0} f(x) - P(x)| \) for all \( |x| \leq R \). This implies that \( |f(x) - P(x)| < \varepsilon \) for all \( x \in [a, b] \). \cite{Hipp}

\end{proof}





    \subsubsection{A Simple Mathematical Application}

Suppose $f:[0,1] \to \mathbb{R}$ is a continuous function, and that $f$ satisfies $$\int_0^1 f(t)t^n \mathrm{d}t = 0$$ for all $n \in \mathbb{N} \cup \{0\}.$ Then $f$ is identically $0.$ \vspace{7px}

    \begin{proof} Since $f$ is continuous on a $[0,1]$ interval, then apply Weierstrass Theorem to find a sequence of polynomials $\{P_n(x)\}_{n\in\mathbb{N}}$ converging uniformly to $f(x)$ on $[0,1]$. Using the assumption $\int_0^1 f(t)t^n \mathrm{d}t = 0$ with the linearity of the integral, we have $$\int_0^1 f(t)P_n(t) \mathrm{d}t = 0 \implies 0 = \lim_{n\to\infty} \int_0^1 f(t)P_n(t) \mathrm{d}t = \int_0^1 f^2(t)\mathrm{d}t.$$ Since $f^2$ must be non-negative and $\int_0^1 f^2(t)\mathrm{d}t = 0$, it follows that $f(t)=0$ for all $t\in [0,1].$ \cite{timmerman} \end{proof}




\subsection{Bernstein's Proof}

Now we compare the proof above to Bernstein's proof. The proof is tedious, but we are rewarded with a concrete method to construct convergent polynomials for any continuous function. To begin we will explain Bernstein polynomials and their derivation.

\subsubsection{Bernstein Basis Polynomial}
A Bernstein polynomial is a linear combination of Bernstein basis polynomials which are of the form:
\[
b_{k,n}(x) \mathrel{:}\mathrel{=} \binom{n}{k} x^{k} \left( 1 - x \right)^{n - k}, \quad k = 0, \ldots, n,
\]
Where \(\binom{n}{k}\) is a binomial coefficient. The first few Bernstein basis polynomials look like
\begin{align*}
b_{0,0}(x) & = 1, \\
b_{0,1}(x) & = 1 - x, & b_{1,1}(x) & = x \\
b_{0,2}(x) & = (1 - x)^2, & b_{1,2}(x) & = 2x(1 - x), & b_{2,2}(x) & = x^2 \\
b_{0,3}(x) & = (1 - x)^3, & b_{1,3}(x) & = 3x(1 - x)^2, & b_{2,3}(x) & = 3x^2(1 - x), & b_{3,3}(x) & = x^3
\end{align*}
The Bernstein basis polynomials form a basis of the vector space of polynomials of degree \(n\). A linear combination of these polynomials then create Bernstein polynomials.

\subsubsection{Bernstein Polynomial}
\begin{definition}
    The n-th Bernstein polynomial for a function $f \in C([0,1], \mathbb{R})$ is defined as:
\[ B_n(f)(x) = \sum_{k=0}^{n} f\left(\frac{k}{n}\right) \binom{n}{k} x^k (1-x)^{n-k}. \]
\end{definition}

We now have the tools to prove Weierstrass' theorem. Our goal in this proof is to show that for any function \(f\) the Bernstein polynomial for $f$ is such that \(\displaystyle{\lim_{n\to\infty}B_n(f) = f}\) uniformly on the interval \([0,1]\).


A simple example of a Bernstein polynomial would be the constant function $f\colon [0,1] \to \mathbb{R}$ where $f(x) = 1.$ Then 
$$ B_n(x, 1) = \sum_{k = 0}^n {n \choose k} x^k(1-x)^{n - k} = (x + (1-x))^n = 1.$$ The second equality follows from Newton's Binomial Formula, $$\sum_{k=0}^n {n \choose k}a^{n - k}b^k = (a+b)^n.$$ 



\subsubsection{Proof Using Bernstein Polynomials}
\begin{theorem}[Weierstrass Approximation Theorem]
For every continuous function $f$ defined on a closed interval $[a, b]$, and for every $\varepsilon > 0$, there exists a polynomial $P$ such that for all $x \in [a, b]$,
\[ |f(x) - P(x)| < \varepsilon. \]
\end{theorem}


\begin{proof}
    Consider $f \in C([0,1], \mathbb{R})$. Since $[0,1]$ is compact, the continuity of $f$ implies uniform continuity on $[0,1]$. Thus $\forall \varepsilon > 0, \exists\delta$ such that for any $x,y \in [0,1]$
    \[
    \abs{x-y} < \delta \implies \abs{f(x) - f(y)} < \frac{\varepsilon}{2}
    \]
    Now, since $f$ is a continuous function on a compact interval, $\abs{f(x)} \leq M$ for some $M$. Fix $c \in [0,1]$. If $\abs{x-c}<\delta$ then $\abs{f(x)-f(c)}<\frac{\varepsilon}{2}$. The converse would show that if $\abs{x-c}\geq\delta$ then
    \[
    \abs{f(x) - f(c)} \leq 2M \leq 2M\left( \frac{x-c}{\delta}\right)^2 + \frac{\varepsilon}{2}
    \]
    Combining the two inequalities gives us
    \[
    \abs{f(x) - f(c)} \leq 2M\left( \frac{x-c}{\delta}\right)^2 + \frac{\varepsilon}{2} \quad \forall x\in[0,1]
    \]
    We now apply Bernstein Polynomials to approximate $f$ on $[0,1]$. Note that:
    \[
    B_n(x, f-f(c)) = \sum_{k=0}^n (f-f(c)) \left(\frac{k}{n}\right) \binom{n}{k} x^k(1-x)^{n-k} = B_n(x,f) - f(c)B_n(x,1) 
    \]
    Thus
    \begin{align*}
        \abs{B_n(x,f) - f(c)} &= \abs{B_n(x,f- f(c))}\\
        &\leq B_n\left(x, 2M\left( \frac{x-c}{\delta}\right)^2 + \frac{\varepsilon}{2}\right)\\
        &\leq \frac{2M}{\delta^2}B_n(x, (x-c)^2) + \frac{\varepsilon}{2}
    \end{align*}
    Further it can be shown that 
    \[
    B_n(x, (x - c)^2) = x^2 + \frac{1}{n}(x - x^2) - 2cx + c^2
    \]
    So
    \[
    \abs{B_n(x,f) - f(c)} < \frac{\varepsilon}{2} + \frac{2M}{\delta^2}\frac{1}{n}(x-c)^2 + \frac{2M}{\delta^2}\frac{1}{n}(x-x^2).
    \]
    In particular,
    \[
    \abs{B_n(x,f) - f(c)} < \frac{\varepsilon}{2} + \frac{2M}{\delta^2}\frac{1}{n}(c-c^2).
    \]
    The maximum of $c-c^2$ on $[0,1]$ is $\frac{1}{4}$ thus we get
    \[
    \abs{B_n(x,f) - f(c)} < \frac{\varepsilon}{2} + \frac{M}{2\delta^2}\frac{1}{n}
    \]
    Thus take $N > \frac{M}{2\delta^2}$ then for any $n\geq N$
    \[
    \norm{B_n(c,f) - f(c)}_\infty < \varepsilon.
    \]
    To generalize the theorem for any interval $[a,b]$, consider the function $\varphi: [0,1] \to [a,b]$ given by $\varphi : x \mapsto x(b-a) - a $. Which is clearly a homeomorphism which extends the above to any interval $[a,b]$. \cite{Young}
\end{proof}

Now we have proved that any continuous function on a compact interval can be approximated by Bernstein's polynomials. Bernstein polynomials may not always be the most efficient or convenient way to approximate a function, but they are theoretically capable of approximating any continuous function.

\subsection{Real World Application}

An influential application of Weierstrass' Theorem and Bernstein polynomials is in the area of computer graphics, where we must find functions to model real world shapes. 

\subsubsection{Bezier Curves}

A Bézier curve is used in computer graphics and related fields. Usually, they model curves that might not have a mathematical representation, or whose representation is too complicated or unknown.\\

Bézier curves are parametric curves that are defined using a set of control points. The shape of the curve is influenced by these control points, but the curve itself is generated through a specific mathematical process that involves Bernstein polynomials. Essentially, a Bézier curve is a weighted sum of its control points, where the weights are determined by Bernstein polynomials.\\

The formula for Bézier curves is as follows:
\[
B(t) = \sum_{i=0}^n P_i B_{i,n}(t)
\]
Here $B(t)$ is the Bézier curve, $B_{i,n}(t)$ is the Bernstein Polynomial, and $P_i$ are the control points.\cite{timmerman}
\subsubsection{De Casteljau Algorithm}

In 1959, mathematician Paul de Castelijau developed the De Castelijau algorithm for numerically evaluating Bézier curves. This led to adoption in computer graphics and modeling. This is a recursive algorithm which takes in the control points and calculates the Bernstein polynomials as depicted below.\\
\textbf{Example:}
Given the Bernstein coefficients:
\begin{align*}
    \beta_{0}^{(0)} &= P_{0}, \\
    \beta_{1}^{(0)} &= P_{1}, \\
    \beta_{2}^{(0)} &= P_{2},
\end{align*}
we aim to evaluate the Bernstein polynomial at the point $t_0$.
We start the recursion with:
\begin{align*}
    \beta_{0}^{(1)} &= \beta_{0}^{(0)}(1 - t_0) + \beta_{1}^{(0)}t_0 = P_{0}(1 - t_0) + P_{1}t_0, \\
    \beta_{1}^{(1)} &= \beta_{1}^{(0)}(1 - t_0) + \beta_{2}^{(0)}t_0 = P_{1}(1 - t_0) + P_{2}t_0.
\end{align*}
In the second iteration, the recursion stops with:
\begin{align*}
    \beta_{0}^{(2)} &= \beta_{0}^{(1)}(1 - t_0) + \beta_{1}^{(1)}t_0 \\
    &= P_{0}(1 - t_0)^2 + P_{1}2t_0(1 - t_0) + P_{2}t_0^2,
\end{align*}
which is the expected Bernstein polynomial of degree 2.
This algorithm is computationally efficient and numerically stable which allows Bernstein Polynomials to be applied in computer software.


\subsubsection{Computer Graphics}


In animation and motion graphics, Bézier curves are used to define the paths along which objects move. The ability to precisely control the motion by adjusting the curves’ control points allows for the creation of smooth and nuanced animations. Keyframe interpolation in animation software often utilizes Bézier curves to control the timing and pacing of motion, providing animators with the tools to craft dynamic and expressive movements.\\

In vector graphics software (like Adobe Illustrator or Inkscape), Bézier curves are the backbone of drawing and illustration tools. Artists and designers use these curves to create and manipulate complex shapes with precision and control. The curves allow for smooth, scalable graphics that can be easily adjusted by moving the control points, without losing quality when zoomed or resized.


\section{The Stone-Weierstrass Theorem}

\subsubsection{Introduction}


This theorem generalizes the concept to a broader class of functions and spaces, articulating that not only real-valued but also complex-valued functions defined on compact spaces can be approximated as closely as desired by elements of certain subalgebras of continuous functions.




\subsubsection{Preliminaries}

\begin{definition}[Algebra over $C(X, \mathbb{C})$ \cite{Lebl_2}]
twh   A set $\mathcal{A}$ over the set of continuous functions from $X$ to $\mathbb{C}$, denoted $C(X, \mathbb{C})$, is called an \textit{algebra} if the following hold:
    \begin{enumerate}
        \item for any $f, g \in \mathcal{A}$ and $\alpha, \beta \in \mathcal{C}$, we have that $\alpha f + \beta g \in \mathcal{A}$.
        \item if $f, g \in \mathcal{A}$, then $fg \in \mathcal{A}$.
        \item $1 \in \mathcal{A}$.
    \end{enumerate}
\end{definition} 

\begin{definition} \cite{Lebl_2}
Let $\mathcal{A}$ be a set of complex-valued functions over a set X. Then
\begin{enumerate}
    \item $\mathcal{A}$ \textit{seperates points} if for every $x,y \in X$, where $x \neq y$, there is some $f \in \mathcal{A}$ such that $f(x) \neq f(y)$.
    \item $\mathcal{A}$ \textit{vanishes at no point} if for all $x \in X$, there is an $f \in \mathcal{A}$ such that $f(x) \neq 0$.
\end{enumerate}
\end{definition}
Note that if $\mathcal{A}$ is an algebra, then it vanishes at no point since $1 \in \mathcal{A}$. 


\begin{proposition} \cite{Lebl_2}
    If $\mathcal{A}$ is an algebra of complex valued functions over a compact metric space $X$, then $\overline{\mathcal{A}}$ is also an algebra.
\end{proposition}
\begin{proposition} \cite{Lebl_2}
    Let $\mathcal{A}$ be an algebra of complex-valued functions over a set $X$ that separates points and vanishes at no point. Suppose $x,y \in X$ and that $x \neq y$, and $c,d \in \mathbb{C}$. Then there is an $f \in \mathcal{A}$ such that $f(x) = c$ and $f(y) = d$. The conclusion also holds if $\mathcal{A}$ is a real algebra and $c,d \in \mathbb{R}$.
\end{proposition}
\begin{proof}
    There exists $g,h,k \in \mathcal{A}$ such that $g(x) \neq g(y)$, $h(x) \neq 0$, and $k(y) \neq 0$. Take
    \begin{align*}
        f(t) &= c\frac{(g(t)-g(h))h(t)}{(g(x)-g(y))h(x)} + d\frac{(g(t)-g(x))k(t)}{((g(y)-g(x))k(y)}\\
        &= c \frac{g(t)h(t) - g(y)h(t)}{g(x)h(x)-g(y)h(x)} + d \frac{g(t)k(t)-g(x)k(t)}{g(y)k(y)-g(x)k(y)}
    \end{align*}
    Then by the closure properties of $\mathcal{A}$, we have that $f \in \mathcal{A}$. Additionally, $f(x)=c$ and $f(y)=d$, which completes the proof \cite{Lebl_2}.
\end{proof}


\subsubsection{Stone-Weierstrass Theorem for Real-Valued Functions}

\begin{theorem}[Stone-Weierstrass Theorem, real version \cite{Lebl_2}]
    Let $\mathcal{A}$ be an algebra of real-valued continuous functions over a compact metric space $X$, such that $\mathcal{A}$ separates points and vanishes nowhere. Then the closure $\overline{\mathcal{A}} = C(X, \mathbb{R})$.
\end{theorem}

\textbf{Proof outline:} The proof can be broken up into several claims:
\begin{enumerate}
    \item \textbf{Claim 1:} If $f \in \mathcal{A}$, then $|f| \in \mathcal{A}$
    \item \textbf{Claim 2:} If $f, g \in \overline{\mathcal{A}}$, then $max(f,g), min(f,g) \in \overline{\mathcal{A}}$
    \item \textbf{Claim 3:} Given $f \in C(X, \mathbb{R})$, $x \in X$, and $\varepsilon > 0$, there exists some $g_x \in \overline{\mathcal{A}}$ such that $g_x(x) = f(x)$ and $g_x(t) > f(t) - \varepsilon$ for all $t \in X$
    \item \textbf{Claim 4:} If $f \in C(X, \mathbb{R})$ and $\varepsilon > 0$, then there exists a $\varphi \in \overline{\mathcal{A}}$ such that $|f(x)-\varphi(x)| < \varepsilon$
\end{enumerate}
The Stone-Weierstrass theorem follows from claim 4, since it tells us that any continuous real-valued function over $X$ is in $\overline{\mathcal{A}}$. Therefore $C(X, \mathbb{R}) \subset \overline{\mathcal{A}}$, and it is trivial that $\overline{\mathcal{A}} \subset C(X, \mathbb{R})$, which means the two sets are equal, as desired.

\begin{proof}[Claim 1] Let $f \in \mathcal{A}$. Then since $f$ is a continuous function over a compact set, it is bounded. So there is a positive real number $B$ such that $|f(x)| \leq B, \forall x \in X$. Now let $\varepsilon > 0$. It follows from the Weierstrass-Approximation theorem that there is a polynomial $\sum_{i=1}^{n} a_i y^i$ which vanishes at $y = 0$, such that $||y|-\sum_{i=1}^{n}a_i y^i| < \varepsilon$ for any $y \in [-B, B]$. Using the fact $\overline{\mathcal{A}}$ is an algebra, and the polynomial has no constant terms we have by closure that
\[\sum_{i=1}^{n}a_i f^i \in \overline{\mathcal{A}}\]
Since $|f(x)| \leq B$, we have that for any $x \in X$,
\[||f(x)| - \sum_{k=1}^{n}a_k (f(x))^k| < \varepsilon\]
This means $|f| \in \overline{\mathcal{A}}$, since for any $\varepsilon > 0$, $|f|$ is within $\varepsilon$ of an element of $\overline{\mathcal{A}}$ \cite{Lebl_2}.
\end{proof}

\begin{proof}[Claim 2]
    We may write $max(f,g) = \frac{f+g}{2} + \frac{|f-g|}{2}$ and $min(f, g) = \frac{f+g}{2} - \frac{|f-g|}{2}$. Then the proof follows simply from our definitions of an algebra: $f,g \in \overline{\mathcal{A}} \implies f-g \in \overline{\mathcal{A}} \implies |f-g| \in \overline{\mathcal{A}}$, where the last statement follows from claim 1. Thus, by closure, $\pm \frac{1}{2}|f-g| \in \overline{\mathcal{A}}$. Again using closure, we get $\frac{f+g}{2} \pm \frac{|f-g|}{2} \in \overline{\mathcal{A}}$.
    This claim also holds by induction for the minimum or maximum of a finite collection of functions \cite{Lebl_2}.
\end{proof}

\begin{proof}[Claim 3]
    Let $f \in \overline{\mathcal{A}}$, $x \in X$, and $\varepsilon > 0$. By our earlier propositon, for every $y \in X$, there is a $h_y \in \mathcal{A}$ such that $h_y(x) = f(x)$ and $h_y(y) = f(y)$. Both $h_y$ and $f$ are continuous, and therefore $h_y - f$ is also continuous. Therefore,
    \[U_y = \{t \in X | h_y(t) > f(t) - \varepsilon \} = (h_y - f)^{-1}((-\varepsilon, \infty))\]
    since it is the pre-image of an open set under a continuous function. Also, we have that $y \in U_y$ because $h_y(y) = f(y) > f(y) - \varepsilon$. It follows that $\{U_y\}_{y \in X}$ is an open cover of $X$. Because $X$ is compact, there is a finite collection of elements $y_1,...,y_n$ such that $\bigcup_{k = 1}^{n} U_{y_k} = X$.
    Let $g_x = max\{h_{y_1},...,h_{y_n}\}$. Note that $g_x \in \overline{\mathcal{A}}$ by claim 2. Now let $t \in X$. Then there is a $y_k$ such that $t \in U_{y_k}$ and therefore $h_{y_k}(t) > f(t) - \varepsilon$. It follows that $g_x(t) > f(t) - \varepsilon$ for any $t \in X$. Finally, we defined each $h_y$ so that $h_y(x) = f(x)$, which means that $g_x(x) = f(x)$, completing the proof \cite{Lebl_2}.
\end{proof}

\begin{proof}[Claim 4]
    Let $f \in C(X, \mathbb{R})$. For each $x \in X$, we can find $g_x$ as in claim 3, and take $V_x = \{t \in X | g_x(t) < f(t) + \varepsilon\}$. Each $V_x$ is open since $f$ and $g_x$ are continuous, and they are the pre-image of open sets. Because $g_x(x) = f(x)$, $x \in V_x$, which tells us $\{V_x\}_{x \in X}$ is an open cover of $X$. As $X$ is compact, there is a finite collection of points $x_1,...,x_n$ such that $X = \bigcup_{k=1}^{n}V_{x_k}$. Letting $\varphi = min(g_{x_1},...,g_{x_n})$, we have by claim 2 that $\varphi \in \overline{\mathcal{A}}$, and by a similar argument as in claim 3, $\varphi(t) < f(t) + \varepsilon$ for any $t \in X$. Additionally, $\varphi(t) > f(t) - \varepsilon$ since $g_x(t) > f(t) - \varepsilon$ for every $g_x$. Putting it all together, we have $f(t)-\varepsilon < \varphi(t) < f(t)-\varepsilon$ and hence $-\varepsilon < \varphi(t) - f(t) < \varepsilon$ for any $t \in X$. Thus, $|\varphi(t) - f(t)| < \varepsilon$, proving the claim \cite{Lebl_2}.
\end{proof}

\subsection{Trigonometric Polynomials Example}
One interesting example of the Stone-Weierstrass Theorem is with Fourier series. For example, consider the Fourier cosine series
\[a_0 + \sum_{n=1}^{N} a_ncos(nt)\]
This forms an algebra, and the Stone-Weierstrass Theorem will apply on the interval $[0, \pi]$. However, it is not true that every continuous function on $[0, \pi]$ has a uniformly convergent Fourier cosine series \cite{Lebl_2}.

\subsubsection{Stone-Weierstrass Theorem for Complex-Valued Functions}
Before we can discuss the Stone-Weierstass theorem for complex functions, we must introduce a new definition.

\begin{definition} \cite{Lebl_2}
    An algebra $\mathcal{A}$ is said to be \textbf{self-adjoint} provided that, for all $f \in \mathcal{A}$, the function $\overline{f}$, where $\overline{f}(x)$ is the complex conjugate of $f(x)$, is $\overline{\mathcal{A}}$.
\end{definition}

\begin{theorem}[Stone-Weierstrass Theorem, complex version \cite{Lebl_2}]
    Let $\mathcal{A}$ be an algebra of complex-valued functions over a compact metric space $X$, such that $\mathcal{A}$ separates points, vanishes nowhere, and is self-adjoint. Then $\overline{\mathcal{A}}=C(X,\mathbb{C})$.
\end{theorem}

\begin{proof}
    Supposed $\mathcal{A}_{\mathbb{R}} \subset \mathcal{A}$ is the set of real-valued elements of $\mathcal{A}$. For $f \in \mathcal{A}$, we may write $f = u + iv$, where $u,v \in \mathbb{R}$. Then we know $u = \frac{f+\overline{f}}{2}$ and $v = \frac{f - \overline{f}}{2i}$. Then because $\mathcal{A}$ is self-adjoint, $u, v \in \mathcal{A}$ and in particular $u, v \in \mathcal{A}_{\mathbb{R}}$.\\
    If $x \neq y$, then because $\mathcal{A}$ separates points we may find an $f \in \mathcal{A}$ such that $f(x) \neq f(y)$. If $f=u+iv$, then clearly $u(x) \neq u(y)$ or $v(x) \neq v(y)$, and therefore $\mathcal{A}_{\mathbb{R}}$ separates points. Likewise,  for every $x \in X$, we may find $f \in \mathcal{A}$ such that $f(x) \neq 0$. If $f = u + iv$, then we have either $u(x) \neq 0$ or $v(x) \neq 0$ and therefore $\mathcal{A}_{\mathbb{R}}$ vanishes nowhere. Thus, $\mathcal{A}_{\mathbb{R}}$ satisfies the requirements for the real version of the Stone-Weierstrass theorem.\\
    Given $f \in C(X, \mathbb{C})$, where $f = u + iv$, and $\varepsilon > 0$, we may find $g, h \in \mathcal{A}_{\mathbb{R}}$ such that $|u(t)-g(t)| < \frac{\varepsilon}{2}$ and $|v(t)-h(t)| < \frac{\varepsilon}{2}$ for any $t \in X$. Then $g + ih \in \mathcal{A}$, and
    \[|f(t)-(g(t)+i\cdot h(t)| = |u(t) + i \cdot v(t) - (g(t)+i\cdot h(t))| \leq |u(t)-g(t)| + |v(t) - h(t)| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon\]
    for all $t \in X$. Thus, $\overline{\mathcal{A}} = C(X, \mathbb{C})$ \cite{Lebl_2}.
\end{proof}

The Stone-Weierstrass is a much 'stronger' statement than the original Weierstrass-Approximation theorem. It tells us that an algebra of complex-valued functions defined over a compact metric space, which also separates points and vanishes nowhere, is dense in the set of continuous functions from the space to $\mathbb{C}$. This implies the Weierstrass-Approximation theorem, since the set of polynomials $\mathcal{P}$ forms an appropriate algebra. However, because our algebra does not necessarily have to be $\mathcal{P}$, the Stone-Weierstrass theorem tells us that continuous functions can be approximated by other many other families of functions besides just polynomials. It also has the advantage of working over any compact metric space $X$.


\subsection{Real World Application}



\subsubsection{Universal Approximator}

A real world application of the Stone-Weierstrass theorem is the concept of a universal approximator. Essentially we state that if a class of functions is dense in the space of continuous functions. Then that class is a universal approximator. This can be written as the following definition.


\begin{definition}[Universal Approximator \cite{neural}] Let $\mathcal{F}$ be a given class. Then, $\mathcal{F}$ is a universal approximator over some domain $S$ if for every continuous function $g : S \rightarrow \mathbb{R}$ and approximation parameter $\varepsilon > 0$, there exists $f \in \mathcal{F}$ such that:
  $\sup_{x \in S} |f(x) - g(x) | \leq \varepsilon .$
\end{definition}

The Weierstrass Theorem showed that polynomials are universal approximators of continuous functions, and the Stone-Weierstrass theorem showed other classes of functions can also be universal approximators. This notion can be used to show that in many circumstances neural networks can be universal approximators. 

At their core neural networks are simply compositions of continuous functions, and with the Stone-Weierstrass Theorem, we can show that they are universal approximators. This result had a large influence in the eventual Universal Approximation Theorems which showed that single hidden layer neural network are universal approximators. These theorems are essential in the theory of artificial neural networks, and show us what they can theoretically learn.

\subsubsection{Neural Networks}
Neural networks are computational models inspired by biological neurons' structure and function made to recognize patterns and model real world behavior. They consist of layers of nodes called neurons which multiplied by weights and biases. They then undergo training which adjusts these weights and biases to improve their accuracy over time. 

A feedforward neural network with $L$ layers can be represented as follows:\\
The activations for the first layer (input layer) are simply the inputs:
\begin{equation*}
    a_i^{(1)} = x_i
\end{equation*}
For each subsequent layer $l = 2, 3, \ldots, L$, the following computations occur:
\begin{enumerate}
    \item Calculate the weighted sum of inputs (plus bias) for each neuron:
    \begin{equation*}
        z_i^{(l)} = \sum_j w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)}
    \end{equation*}
    
    \item Apply an activation function to each weighted sum to get the neuron's activation:
    \begin{equation*}
        a_i^{(l)} = f^{(l)}(z_i^{(l)})
    \end{equation*}
\end{enumerate}
The output of the network is the activation of the neurons in the last layer ($L$).


\subsubsection{Universal Approximation Theorems}

Let $f(x)$ be defined as a single neuron:
\begin{equation*}
    f_{\alpha,w,b} : x \mapsto \alpha \psi(\langle w, x \rangle + b),
\end{equation*}
where $\alpha \in \mathbb{R}$, $w$ is the weight vector, $b$ is the bias term, and $\psi$ is the activation function. We define $\mathcal{F}$ as:
\begin{equation*}
    \mathcal{F} = \text{span}_{\alpha,w,b} \lbrace f_{\alpha,w,b} \rbrace,
\end{equation*}
which represents the space of all possible single-hidden-layer networks with activation $\psi$. \\

\noindent\textbf{Theorem:} If we use the cosine activation $\psi(\cdot) = \cos(\cdot)$, then $\mathcal{F}$ is a universal approximator.

\begin{proof}
The universality of $\mathcal{F}$ with cosine activation can be established by demonstrating that $\mathcal{F}$ satisfies the four conditions of the Stone-Weierstrass theorem. These conditions are continuity, the identity property, separation of points, and closure under addition and multiplication.

\begin{enumerate}
    \item \textit{Continuity:} The continuity of $\mathcal{F}$ follows directly from the continuity of the cosine function. Since $\cos(\cdot)$ is continuous and the operations of vector dot product and addition are continuous, $f_{\alpha,w,b}(x)$ is continuous for all $\alpha, w, b$. Hence, $\mathcal{F}$ is continuous.
    
    \item \textit{Identity:} For the identity property, consider $\cos(\langle 0, x \rangle) = \cos(0) = 1$. This shows that there exists a function in $\mathcal{F}$ (specifically, with $w=0$, $\alpha=1$, and any $b$ such that $\psi(b)=1$) that does not identically vanish, satisfying the identity condition.
    
    \item \textit{Separation:} To satisfy the separation condition, for any distinct $x, x' \in \mathbb{R}^n$, we can construct a function in $\mathcal{F}$ such that:
    \[
    f(z) = \cos\left(\frac{1}{\lVert x-x' \rVert_2^2}\langle x-x', z-x' \rangle \right),
    \]
    which clearly separates $x$ and $x'$. This is because $f(x) \neq f(x')$ by the distinctness of $x$ and $x'$.
    
    \item \textit{Closure:} Finally, for closure, we need to show that $\mathcal{F}$ is closed under addition and multiplication. The closure under addition is straightforward since linear combinations of functions in $\mathcal{F}$ remain in $\mathcal{F}$. For multiplication, consider two cosine functions $\cos(\langle u, x \rangle)$ and $\cos(\langle v, x \rangle)$. Using trigonometric identities, we can express the product of these two functions as:
    \[
    \cos(\langle u, x \rangle) \cos(\langle v, x \rangle) = \frac{1}{2} \left(\cos(\langle u+v, x \rangle) + \cos(\langle u-v, x \rangle)\right),
    \]
    which is a sum of functions in $\mathcal{F}$, thereby satisfying closure under multiplication. \cite{neural}
\end{enumerate} 
\end{proof}

This formal proof establishes that $\mathcal{F}$, with the cosine activation function, satisfies all conditions of the Stone-Weierstrass theorem, proving it to be a universal approximator. These same calculations can be done with the myriad of continuous functions used as activation functions for neural networks showing that in many cases, single hidden layer neural networks are universal approximators. 





\section{Conclusion}


In conclusion, the Weierstrass Approximation Theorem and its generalization, the Stone-Weierstrass Theorem, play a pivotal role in the foundation of modern mathematical analysis, approximation theory, and various applied fields such as computer graphics and numerical analysis. The Weierstrass Approximation Theorem demonstrates that continuous functions defined on a closed interval can be uniformly approximated by polynomial functions to any degree of accuracy. This theorem not only showcases the power and simplicity of polynomials but also highlights the dense nature of polynomial functions in the space of continuous functions over a closed interval.

\newpage

\bibliographystyle{plain}
\bibliography{refs} % No need to include the .bib extension





\end{document}



trigonometric polynomials


\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usepackage{quiver}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{stmaryrd}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usetikzlibrary{arrows}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{definition}{Definition}[subsection]
\newtheorem{lemma}{Lemma}[subsection]
\newtheorem{remark}{Remark}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{proposition}{Proposition}[subsection]
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}[subsection]
\allowdisplaybreaks


\renewcommand{\AA}{\mathbb{A}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\MM}{\mathbb{M}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\UU}{\mathbb{U}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\nuh}{\N\U\H}
\newcommand{\uh}{\U\H}
\title{194 - Introduction to Shr\"odinger Operators}

\date{Spring 24'}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Part I}

\subsection{Probability Measures}
In this section we will use the notation $\Omega$ which is the space we are in, while $\A$ can be thought of as all the possible events that can occur.

\begin{definition}
    We say $\A$ is an \textbf{algebra} if it is a collection of sets of $\Omega$ that is closed under finite union, intersection, and complements.
\end{definition}

\begin{definition}
    We say $\A$ is a \textbf{$\mathbf{\sigma}$-algebra} if it is an algebra and closed under countable union and intersections. We say that $X$ is countable if there is a map from $\NN\rightarrow X$ that is surjective.
\end{definition}

\begin{example}
    Consider $\Omega = \{1, 2, 3\}$ and then $\A$ could be all possible subsets. 
\end{example}

\begin{definition}
    A \textbf{probability measure} is a map $\mu: \A \rightarrow [0,1]$ where $\Omega$ satisfying
    \begin{enumerate}
        \item $\mu(X) = 1$
        \item $\mu(\varnothing) = 0$ where $\varnothing$ represents the empty set.
        \item If $\{E_j\}_{j=1}^\infty$ is a countable disjoint family of elements in $\A$, then
        \[\mu(\bigcup_{j=1}^\infty E_j) = \sum_{j=1}^\infty \mu(E_j)\]
    \end{enumerate}
\end{definition}

\begin{example}
    Consider the situation where we are rolling a 6-sided dice. Then $\Omega = \{1,2,3,4,5,6\}$ and let $\A$ be the set of all possible subset of $\Omega$. Then consider the subset say $E = \{1,3\}$. Then $\mu(E)$ measures the probability that if I roll a dice it will land on either 1 or 3. Therefore if we consider condition one of the definition it states the the probability I roll $1,2,3,4,5,$ or $6$ is 1, which means no matter how I roll the dice one of the events must occur. While condition 2 is sort of like saying the probability that I roll a dice and nothing occurs is zero. The last condition is to say that if I have the event $\{1\}$ and the event $\{3\}$. Then the probability of the event $\{1,3\}$ occurring should be the sum of the event $\{1\}$ and the event $\{3\}$. In other words, the probability the event to roll a $\{1,3\}$ is the same as the independent events of rolling a $\{1\}$ and rolling $\{3\}$ together.
\end{example}

\begin{definition}
    We say the triple $(\Omega, \A, \mu)$ is a \textbf{probability space}. In general, we call this space a measurable space where the difference is $\mu$ need not be a probability measure.
\end{definition}

\begin{definition}
    We say a set $E$ is \textbf{measurable} if $E\in \A$.
\end{definition}

\begin{definition}
    Given a probability space $(\Omega, \A, \mu)$. We say a map $\phi: \Omega \rightarrow \Omega$ is measurable if $E\in \A$ then $\phi^{-1}(E) \in \A$.
\end{definition}

This definitions above are not formally correct and we are simply using this as the definition, as we do not have the ability to explore this concept in depth. Even though the definitions above are slight lies, the definitions should allow us to explore this topic with some decent understanding.

\begin{definition}
    Let us consider two probability spaces $(X, \B, \mu), (Y, \C, \nu)$ where $\B, \C$ are like the $\A$ from above and $\mu, \nu$ are probability measures. Then consider a measurable map $\phi: X\rightarrow Y$. We say that this map is \textbf{measure preserving} if
    \[\mu(\phi^{-1}E) = \nu (E) \text{ for all } E\in \C\]
    If the map is inverible, then we call the map $\phi$ and \textbf{invertible measure-preserving map}. Lastly, if we consider the map $T:X \rightarrow X$, then we say that the measure $\mu$ is \textbf{T-invariant} if $T$ is measure preserving. In this case, we can call the measurable space $X$ a \textbf{measure-preserving system} and $T$ a \textbf{measure-preserving transformation}.
\end{definition}

\begin{example}
    (Rotation of Circle) Consider the space $\RR/\ZZ$ where we can think of $\RR/\ZZ$ as the interval $[0,1)$. Equipped with the map
    \[T(\omega) = \omega + \alpha\]
    Where $\alpha\in [0,1)$. Where this space has a measure called the Lebesgue measure denoted as $m$. In this case, this measure is a probability measure. We need not too much detail except that if $(a,b)$ is an interval where $0\leq a \leq b < 1$, then $m((a,b)) = b-a$.
\end{example}

\begin{example}
    (Rotation of Torus) Consider the space $\RR^n/\ZZ^n$, $n\geq \NN$, with Lebesgue measure. This space is like $[0,1)\times \cdots \times [0,1]$ where the product is $n$ times. Equipped with the map
    \[T(\omega_1, \omega_2, \dots, \omega_n) = (\omega_1 + \alpha_1,\dots, \omega_n + \alpha_n)\]
    Where $\alpha_i \in [0,1)$.
\end{example}

\begin{example}
    (Circle Doubling Map) Consider the space $\RR/\ZZ$, with Lebesgue measure. Equipped with the map
    \[T(\omega) = 2\omega\]
    Where $\alpha\in [0,1)$.
\end{example}

\begin{example}
    (Circle Expanding Map) Consider the space $\RR/\ZZ$, with Lebesgue measure. Equipped with the map
    \[T(\omega) = m\omega\]
    Where $\alpha\in [0,1)$.
\end{example}

\subsubsection{Exercises}
\begin{enumerate}
    \item If $\Omega$ has $n$ elements and $\A$ is the collection of all possible subsets of $\Omega$ then what is the cardinality of $\A$? Why?

    \item Consider the example 1.1.2. Explicitly define what the map $\mu$ is.

    \item Consider the example 1.1.2. Define a map that is measure preserving. Is it possible to define a map that is not measure-preserving? If so define one.
\end{enumerate}

\newpage
\subsection{Banach and Hilbert Spaces}
The main space that we will focus on is $\ell^2(\NN)$ or $\ell^2(\ZZ)$. We will define the space later, as we need to explore some definitions first. We remark that this space is a \textbf{Hilbert} space, which implies being a \textbf{Banach Space}. Hence, why we need to discuss what these spaces mean. This file will be mostly self-contained, so we will introduce almost every definition needed. Please feel free to skip over any definitions that you have already learned.

\begin{definition}
    We say a space $V$ is a \textbf{vector space} over a field $F$ (where $F = \RR$ or $\CC$) if $V$ is equipped with the operations of addition and multiplication and satisfies
    \begin{enumerate}
        \item Commutative addition
        \item Associative addition
        \item Additive identity
        \item Additive inverse
        \item Multiplicative identity
        \item Distributive properties
    \end{enumerate}
\end{definition}

Throughout we will use this canonical notation of $V$ as a vector space and $F$ as the field. Please keep in mind that $F$ in our cases will always be $\RR$ or $\CC$. 

\begin{definition}
    A sequence $\{x_n\}_{n\in \NN}\subset F$ is \textbf{cauchy} if for any $\epsilon>0$ there exists an $N\in \NN$ so that for all $n,m\geq \NN$ then $|x_n - x_m| < \epsilon$
\end{definition}

\begin{definition}
    A space is called \textbf{complete} if every Cauchy sequence converges.
\end{definition}

\begin{definition}
    Let $V$ be a vector space over a field $F$. Then a \textbf{norm} is a map $||\cdot||: V\rightarrow \RR$ that satisfies the following
    \begin{enumerate}
        \item For any $x\in V$, then  $||x|| \geq 0$ and $||x|| = 0 \iff x = 0$
        \item Given $\lambda \in F$, then $||\lambda x|| = |\lambda| ||x||$
        \item Given $x,y\in V$ then $||x + y|| \leq ||x|| + ||y||$
    \end{enumerate}
\end{definition}

\begin{definition}
    We say a space $(V, ||\cdot||)$ is a \textbf{Banach Space} if the space is a complete with respect to the norm.
\end{definition}

We remark that in definition $1.1.2$ the norm used is the absolute value or modulus depending on $\RR$ or $\CC$. The modulus of a complex number is
\[z = x+iy\in \CC \text{ then } |z| = \sqrt{x^2 + y^2}\]
Note that the absolute value and modulus are both norms. Therefore if we wanted to show a space is Banach, then we would need to use the same definition, but replace it with the respective norm that is used. Let us see an example of a Banach space. Therefore an easy example of a Banach space is $\RR$ with respect to the absolute value norm or $\CC$ with the modulus.

\begin{definition}
    Let $V$ be a vector space over a field $F$. An \textbf{inner product} is a map $\langle \cdot, \cdot \rangle: V \rightarrow F$ that satisfies
    \begin{enumerate}
        \item Given $x\in V$ then $\langle x, x \rangle \geq 0$ and $\langle x, x\rangle = 0 \iff x = 0$
        \item Given $x,y\in V$ then $\langle x, y \rangle = \overline{\langle y, x\rangle}$
        \item Given $x,y_1,y_1\in V$ and $\lambda_1,\lambda_2\in F$ then  $\langle x, \lambda_1 y_1 + \lambda_2 y_2 \rangle = \lambda_1\langle x, y_1\rangle + \lambda_2\langle x, y_2\rangle$
    \end{enumerate}
\end{definition}

\begin{definition}
    A space $\H$ is called a \textbf{Hilbert Space} if complete with respect to the norm induced by the inner product. In other words, complete with respect to the norm $||\cdot|| = \sqrt{\langle\cdot, \cdot \rangle}$.
\end{definition}

In definition $1.1.6$ the third condition you may find holds in the first component for some texts. We adopt the notation for the second component to correspond to the main textbook we follow. From my understanding this notation seems to be used more by physicists, while the other notation by mathematicians. So I guess we are physicists in this setting...

Furthermore, we need to actually show that the induced norm is actually a norm. It turns out to prove this we need a important inequality. We use some facts from complex analysis and some basics of positive quadratic equations. Specifically, that given any $z\in \CC$ we can write
\[z = |z|e^{i\arg(z)}\]
Where $\arg(z)$ is the angle formed if we think of $z$ as a vector in $\RR^2$ and $|z|$ is the modulus.

\begin{theorem}
    (Cauchy-Schwartz Inequality) $|\langle x, y\rangle| \leq ||x|| ||y||$
\end{theorem}
\begin{proof}
    We assume that we are over $F = \CC$. Note that if the left hand side is $0$ then we are naturally done. Therefore we can assume WLOG that $\langle x, y\rangle \neq  0$. Since we are in $\CC$ then we can write
    \[\langle x, y\rangle = re^{i\theta} \implies \langle x, e^{i\theta}y\rangle = r > 0\]
    Now ISTS that $\langle x, y\rangle \leq ||x|| ||y||$ where we can replace $y$ via $e^{i\theta}y$ if needed. Therefore now we are in the position where $\langle x, y\rangle \in \RR$. Now consider any $t\in \RR$ and compute the following
    \begin{align*}
        0&\leq \langle x +ty, x+ty\rangle\\
        &= \langle x + ty, x\rangle + t\langle x + ty, y\rangle\\
        &= ||x||^2 + t\langle x, y\rangle + t \langle y, x\rangle + t^2||y||^2\\
        &= ||x||^2 + 2t\langle x,y \rangle + t^2||y||^2
    \end{align*}
    Where in the second line we inherently use the fact that $\langle x, y\rangle \in \RR$, since if not we would have complex conjugates. Now note that this is a strictly positive quadratic equation, which implies the discriminant is $\leq 0$. Therefore
    \[\Delta = 4\langle x, y\rangle^2 - 4||x||^2||y||^2 \implies |\langle x, y\rangle| \leq ||x|| ||y||\]
\end{proof}

\begin{proposition}
    Given an inner product space $(V,\langle \cdot, \cdot\rangle)$ show that $||\cdot|| = \sqrt{\langle \cdot, \cdot \rangle}$ is a norm.
\end{proposition}
\begin{proof}
    We need to check the 3 conditions.
    \begin{enumerate}
        \item Let $x\in V$ then $||x|| = \sqrt{\langle x, x\rangle} \geq 0$ which is clear since the inside of the square root is positive. Furthermore, $\sqrt{x} = 0 \iff x =0$ and by assumption $\langle x, x \rangle = 0 \iff x =0$.

        \item Let $\lambda\in F$, then
        \[||\lambda x|| = \sqrt{\langle \lambda x, \lambda x \rangle} = \sqrt{\lambda^2 \langle x, x \rangle} = \lambda \sqrt{\langle x, x \rangle} = \lambda ||x||\]

        \item Let $x,y\in V$ then
        \begin{align*}
            ||x+y||^2 &= \langle x+y, x+y\rangle\\
            &= ||x||^2 + 2Re\langle x, y\rangle + ||y||^2\\
            &\leq ||x||^2 + 2||x|| ||y|| + ||y||^2\\
            &= (||x|| + ||y||)^2
        \end{align*}
        Where we use Cauchy-Schwartz in the second to last line. Squaring both sides we have the desired inequality.
    \end{enumerate}
\end{proof}

\begin{definition}
    We define
    \[\ell^2(\NN) := \{\psi: \NN \rightarrow \CC: ||\psi||_2 = \sum_{j=1}^\infty |\psi(j)|^2 < \infty\}\]
    Where we equip the space with the inner product
    \[\langle\phi, \psi\rangle = \sum_{j=1}^\infty\overline{\phi(j)} \psi(j)\]
\end{definition}

\begin{definition}
    Given a measurable space $(\Omega, \A, \mu)$
    \[L^2(X, \mu) := \{f:\Omega \rightarrow \CC: f \text{ is measurable }, ||f||_2 = (\int_X |f|^2 d\mu)^{1/2}< \infty\}\]
    and $\mu$ is a measure. We equip the space with the inner product
    \[\langle f, g\rangle = \int_\Omega \overline{f} gd\mu \]
\end{definition}

The first space defined above is extremely important and we will be coming back to this space a lot. A few cool remarks that are complex, but deserve to be mentioned.
\begin{enumerate}
    \item $\ell^2$ turns out to be $L^2$ space equipped with counting measure. This can be thought of as integrating at the points $\ZZ$ then this is essentially $\ell^2(\ZZ)$. This idea of integrating only over $\ZZ$ is like the idea of equipping counting measure!
    
    \item If $\H$ is any Hilbert space with a countable orthonormal basis, then $\H \cong \ell^2(\ZZ)$. This says that studying any Hilbert space with a countable orthonormal basis is the same thing as if we just studied $\ell^2(\ZZ)$! Where an orthonormal basis is a basis with norm one and all elements of the basis are perpendicular to each other.
\end{enumerate}

\subsubsection{Exercises}

\begin{enumerate}
    \item What are simple examples of Hilbert spaces? 
    
    \item Show that the inner produce equipped to $\ell^2(\NN)$ is indeed an inner product. Similarly, for $L^2$.

    \item (Hard) Show that $\ell^2(\NN)$ is complete. Hint: Assume that you have a sequence that is Cauchy, then construct a function via pointwise limits

    \item What should the equivalent definition be for the space $\ell^2(\ZZ)$?
\end{enumerate}




\newpage
\section{Part II}
In all honesty this section is quite difficult. There is a heavy amount of measure theory laying around in this section, so I really do not expect full understanding of the details of everything. I just want us to get the general idea of what the theorems say and some important basic examples that will guide us in this understanding. Of course, I will still include most of the proofs for the theorems and propositions. Please feel free to omit reading the proofs if you would like, though please try to understand the gist of what the theorems are stating.

\subsection{Recurrence}
The most basic question that can be asked after given one of the systems in the previous section is: Does a point return to itself? If so, how often? If we don't think about a point, but rather a set we can ask similar questions. To answer the latter question we explore what I think of as the pigeon-hole principle of dynamics. 

\begin{theorem}
    \textbf{(Poincare Recurrence)} Given a measure preserving system $(X, \B, \mu, T)$, then consider a measurable set $E$. Then almost every $x\in E$ will return to the set $E$ infinitely many times.
\end{theorem}
\begin{proof}
    First, let us define:
    \[B:= \{x\in E: T^nx\notin E \text{ for all } n\geq 1\}\]
    Notice that we can write $B$ as:
    \[B = E \cap T^{-1}E^c \cap T^{-2}E^c \cap \cdots\]
    Certainly, $B$ is measurable, since $E^c$ is measurable and $T$ is measurable. Now we claim that the sets $B,T^{-1}B,T^{-1}B,\dots$ are disjoint. Assume not, then we have $n,m\in \NN$, WLOG $n>m$, such that $x\in T^{-n}B \cap T^{-m}B$. It follows that $T^nx, T^mx \in B$, so $T^{n-m}(T^mx) = T^nx \in B$. A contradiction, since we have found an element of $B$ that returns to $B$ and $B\subset E$. Hence, the element is an element of $E$ returning to $E$, which contradicts what it means to live in $B$. Now we claim that $\mu(B) =0$. Consider:
    \[\bigcup_{\NN} T^{-n}B \subset X \implies \mu(\bigcup_{\NN} T^{-n}B) = \sum_{\NN} \mu(T^{-n}B) \leq \mu(X) < \infty\]
    We know that $T$ is measure preserving so $\mu(B) = \mu(T^{-n}B)$ for all $n$. If $\mu(B)>0$ we would find a contradiction from the above sum. Hence, $\mu(B) = 0$. We can now define:
    \[F_1 := \{x\in E: T^nx\in E \text{ for some } n\geq 1\}\]
    Naturally, $F_1 = E\setminus B$, so $\mu(F_1) = \mu(E)$ and $F_1\subset E$. We can find in general:
    \[F_i:= \{x\in E: (T^i)^nx \in E \text{ for some } n\geq 1\}\]
    By doing the entire process above with:
    \[B_i := \{x\in E: (T^i)^nx\notin E \text{ for all } n\geq 1\}\]
    We can find $F_i\subset E$ with $\mu(F_i) = \mu(E)$. Now let $F = \bigcap_{n\geq 1} F_n$ and note that $F\subset E$. It follows that:
    \[\mu(E\setminus F) = \mu(E\setminus (\bigcap_{n\geq 1} F_n) \leq \mu(\bigcup_{n\geq 1} E\setminus F_n) \leq \sum_{n\geq 1} \mu(E\setminus F_n) = 0\]
    Hence, $\mu(E) = \mu(F)$ and every point in $F$ returns to $E$ infinitely many times.
\end{proof}

\subsubsection{Exercises}
\begin{enumerate}
    \item Give an example of a space that fails to have the recurrence property. Hint: Think about $\RR$ with Lebesgue measure.
\end{enumerate}

\newpage
\subsection{Ergodic Definitions and Equivalences}

\begin{definition}
    We say that the measure-preserving transformation $T$ for the space $(X, \B, \mu)$ is ergodic if for any $B\in \B$
    \[T^{-1}B = B \implies \mu(B) = \{0,1\}\]
\end{definition}

This states that an invariant set must be the entire space or essentially none of the space. Alternatively, we can say that a space is ergodic if we cannot split the space into two invariant pieces. Let us see the many equivalent definitions to being ergodic. 

\begin{theorem}
    TFAE for measure preserving transformations $T$ of a $(X, \B, \mu)$.
    \begin{enumerate}
        \item $T$ is ergodic.
        \item For any $B\in \B$ 
        \[\mu(T^{-1}B\Delta B) \implies \mu(B) = \{0,1\}\]
        \item For $B\in \B$ with $\mu(B)>0$, then 
        \[\mu(\bigcup T^{-n} B) = 1\]
        \item For any $A,B\in \B$ such that $\mu(A)\mu(B) >0$, then there exists some $n\geq 1$ so that
        \[\mu(T^{-n}A\cap B) > 0 \]
        \item For $f:X\rightarrow \CC$ measurable, $f\circ T = f$ almost everywhere, then $f$ is a constant a.e.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $(1)\implies (2):$ Let $T$ be ergodic and consider some $B\in \BB$ with $\mu(T^{-1}B\triangle B) = 0$. Let us consider:
    \[C = \bigcap_{N=0}^\infty \bigcup_{n\geq N} T^{-n} B = \limsup T^{-n}B\]
    We can define $C_N = \bigcup_{n\geq N} T^{-n}B$ and notice:
    \[C_N \triangle B = \bigcup_{n\geq N} T^{-n}B \triangle B \subset \bigcup_{n\geq N} (T^{-n}B \triangle B)\]
    We can find that for every $k\geq 1$:
    \[T^{-n}B \triangle B \subset \bigcup_{k=0}^{n-1} (T^{-k -1}B \triangle T^{-k} B)\]
    Using the fact that $T$ is measure preserving we find:
    \[\mu(T^{-n}B \triangle B) \leq\sum_{n=0}^{N-1} \mu(T^{-n -1}B \triangle T^{-n} B) = 0\]
    Hence, we must have that $\mu(C_n \triangle B) = 0$. We can note that $C_0 \supset C_1 \supset C_2 \supset \cdots$ and it follows that:
    \[C_n \setminus B \supset C_{n+1}\setminus B \text{ and } B\setminus C_n \subset B\setminus C_{n+1} \text{ for all n }\]
    Applying continuity from above and below and using the excision property of measurable sets we have:
    \[\mu(C\triangle B) = \mu(C\setminus B) + \mu(B\setminus C) = \lim \mu(C_n\setminus B) + \lim \mu(B\setminus C_n) = \lim \mu(C_n\triangle B)\]
    Hence, we have $\mu(C\triangle B) = 0$. Furthermore, we have:
    \[T^{-1}C = \bigcap_{N=0}^\infty \bigcup_{n\geq N} T^{-n-1}B = \bigcap_{N=0}^\infty \bigcup_{n\geq N+1} T^{-n}B = C\]
    Hence, $T^{-1}C = C$, so by ergodicity we have $\mu(C) =0$ or $1$. Also, we know that $\mu(C\triangle B) =0$, so the same follows for $\mu(B)$.
    \\
    $(2)\implies (3):$ Let $A$ be a set with $\mu(A)>0$. Now define $B = \bigcup_1^\infty T^{-n}A$ and note that $T^{-1}B\subset B$. We know by $T$ invariance that $\mu(T^{-1}B) = \mu(B)$, so we must hence $\mu(T^{-1}B\triangle B) = 0$. Applying, $(2)$ we must have $\mu(B)=0$ or $1$. If $\mu(B)=0$, then this would imply that $\mu(T^{-n}A) =0$ for all $n$, but this cannot occur since $\mu(T^{-n}A) = \mu(A)$. Hence, we must have $\mu(B) = 1$.
    \\
    $(3)\implies (4):$ Let $A,B\in \BB$ such that $\mu(A)\mu(B) >0$. Then $\mu(A),\mu(B)$ are both positive. Applying $(3)$ onto $A$ we know that $\mu(\bigcup_1^\infty T^{-n}A) =1$. It follows that:
    \[0<\mu(B) = \mu(\bigcup_1^\infty B\cap T^{-n}A) \leq \sum_1^\infty \mu(B\cap T^{-n}A)\]
    Hence, there must exist some $n\geq 1$ such that $\mu(B\cap T^{-n}A)>0$.
    \\
    $(4) \implies (1):$ By the contrapositive of $(4)$ we have if for all $n\geq 1$ we have $\mu(T^{-n}A\cap A) =0$, then $\mu(A)\mu(B)=0$. Let us consider some $A$ and note $\mu(A) = \mu(T^{-n}A)$. Furthermore, we know:
    \[0 =\mu(A\cap X\setminus A) = \mu(T^{-1}A\cap X\setminus A)\]
    Thus, we have either $\mu(A) =0$ or $\mu(X\setminus A) =0$. In the latter, case it implies that $\mu(A) = 1$.
    \\
    $(2)\implies (5):$  We know that if $(2)$ holds, then we have that $T$ is ergodic. We can let $f$ be a complex function on $X$ and invariant under $T$, which implies that $f$ is invariant with respect to real and imaginary parts. Thus, we restrict to $f$ real valued on $X$. Now we fix some $k\in \ZZ$ and $n\geq 1$:
    \[A^k_n = \{x\in X: f(x) \in [\frac{k}{n},\frac{k+1}{n}) \}\]
    Then we claim that $B = T^{-1}A^k_n \triangle A^k_n \subset \{x\in X: f\circ T(x) \neq f(x) \} = E$. If $x\in B$, then $x\in T^{-1}(A^k_n) \setminus A^k_n$ or reversed. In the first case it is easy to see that this implies $x\notin A^k_n$, but $T(x) \in A^k_n$, so $f(T(x)) \neq f(x)$. The latter holds by similar argument. Also, note by assumption that $f$ is $T$ invariant, so the set $E$ is null. Hence, we have $\mu(T^{-1}A^k_n \triangle A^k_n))= 0$. By the assumption of $(2)$ this implies that $\mu(A^k_n) \in \{0,1\}$. For every fixed $n$, it is clear that $A^k_n$ is disjoint from one another, as $k$ is varied. But it is clear that $X = \bigsqcup_{k\in \ZZ} A^k_n$. We also, know that the measure of $X$ is positive, so there must exist exactly one $k_n$ such that $\mu(A^{k_n}_n) = 1$. Repeat the above process for each $n$ and we can define $Y = \bigcap_{n\in \NN} A^{k_n}_n$ and certainly, $\mu(Y) = 1$. Hence, $f$ must be constant almost everywhere.
    \\
    $(5) \implies (2):$ Consider some set $B$ such that $\mu(T^{-1}B\triangle B) =0$ and let $f=\chi_b$. Note that $f$ is certainly $T$ invariant, so $f$ is constant almost everywhere by $(5)$. Hence, $\mu(B) = 0 $ or $1$, since $f$ is either constant a.e. on $0$ or $1$.
\end{proof}

In fact, some choose to take $(2)$ as the definition of ergodicity. Of course after the above it does not matter what we choose to take, since they are all equivalent, but if we started with $(2)$ the proofs of equivalence would have been much shorter. I opted to introduce the notion that was first introduced to me hence the above.

\begin{proposition}
    Circle rotation is ergodic with Lebesgue measure $\iff$ $\alpha$ is irrational.
\end{proposition}

\begin{proposition}
    The circle doubling map is ergodic with respect to Lebesgue measure.
\end{proposition}

It turns out that proposition $1,2$ can both be proved using an elegant proof using Fourier series. The argument is to take a measurable function $f$ then observe the Fourier coefficients. With $T$ invariance it can be shown that the coefficients are equal to $0$ everywhere, but $n=0$ which implies constant function.

\subsubsection{Exercises}
\begin{enumerate}
    \item Show that if $\alpha$ is irrational then the circle rotation is ergodic. Use the following as facts
    \begin{enumerate}
        \item Let $f$ measurable, then $f$ can be written as
        \[f(t)= \sum_{n\in \ZZ} c_n e^{2\pi i nt}\]
        \item For $N$ large enough $c_n = 0$ 
    \end{enumerate}
    Hint: Use $T$-invariance and theorem 2.2.1.
    \item Show the circle doubling map is ergodic. Use the following as facts
    \begin{enumerate}
        \item Let $f$ measurable, then $f$ can be written as
        \[f(t)= \sum_{n\in \ZZ} c_n e^{2\pi i nt}\]
        \item For $N$ large enough $c_n = 0$
    \end{enumerate}
    Hint: Use a similar argument to $(1)$.
\end{enumerate}

\newpage
\subsection{Ergodic Theorems}
For completeness I introduce the mean and maximal ergodic theorems. We technically need them to prove Birkhoff's ergodic theorem though, but as stated in the beginning the goal is to understand the statements not the proof. The proofs are non-trivial and I will not discuss much about the proofs of the mean and maximal ergodic theorem. 

\subsubsection{Mean Ergodic Theorem}
\begin{theorem}
    Given $(X,\B, \mu, T)$ and the orthogonal projection $P_T$ onto 
    \[I = \{g\in L^2: U_Tg =g\} \subset L^2\]
    where $I$ is a closed subspace and $U_T$ is the operator
    \begin{align*}
        U_T: L^2 &\rightarrow L^2\\
        f &\mapsto f\circ T
    \end{align*}
    Then for any $f\in L^2$ we find the following convergence in the $||\cdot||_2$ norm
    \[\frac{1}{N} \sum_{k=0}^{N-1} U^k_T(f) \rightarrow P_T f\]
\end{theorem}
\begin{proof}
    Let us define $B = \{U_Tg-g: g\in L^2_\mu\}$ and we show that $B^\perp = I$. First, consider when $f\in I$, then we have $U_Tf = f$, so we can consider:
    \[\langle f,U_Tg - g\rangle = \langle f, U_Tg\rangle - \langle f, g\rangle =  \langle U_T f, U_Tg\rangle - \langle f, g\rangle = 0\]
    Now assume the converse, so we have $f\in B^\perp$, then it follows that:
    \[\langle U_Tg,f\rangle - \langle g, f\rangle = 0 \implies \langle U_Tg , f\rangle = \langle g,f \rangle \implies U^*_T f = f\]
    From this it follows that:
    \[|| U_Tf,f||_2 = \langle U_T f - f, U_Tf - f\rangle = ||U_Tf||_2 -\langle f, U_Tf\rangle - \langle f, U_Tf\rangle +||f||_2 = 2||f||_2 - \langle U^*_Tf, f\rangle - \langle f, U*_Tf\rangle = 0\]
    Therefore, we have that $B^\perp = I$. It follows by a fact about Hilbert Spaces, that $L^2_\mu = \overline{B} \oplus B^\perp$. Thus, for any $f\in L^2_\mu$ we can write $f$ as:
    \[f = P_Tf + h\]
    Now we claim that:
    \[\frac{1}{N} \sum_{n=0}^{N-1} U^n_T h \xrightarrow{L^2_\mu} 0\]
    If $h\in B$, then we know that $h = U_Tg - g \in L^2_\mu$, so it follows that:
    \begin{align*}
        ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T h||_2 &= ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T(U_Tg-g)||_2 = \frac{1}{N} \\
        &= \frac{1}{N}||U^N_Tg - g||_2 \text{ by the fact the above is telescoping sum }\\
        &\leq \frac{1}{N}( ||U^N_Tg||_2 + ||g||_2)\\
        &= \frac{2}{N}||g||_2 \xrightarrow{N\rightarrow \infty} 0
    \end{align*}
    Now if $h\in \overline{B}$, then $h$ is a limit point, so there is a sequence $\{h_i\} \subset B$ where $h_i = U_Tg_i - g_i \rightarrow h$. We can pick an $\epsilon$ and $N$ large enough, so the following holds:
    \[|h-h_i| < \epsilon \text{ and } ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T h_i||_2 < \epsilon\]
    Now consider the following:
    \begin{align*}
        ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T h||_2 &\leq ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T(h - h_i)||_2 + ||\frac{1}{N} \sum_{n=0}^{N-1} U^n_T h_i||_2\\
        &\leq \frac{1}{N} \sum_{n=0}^{N-1} ||U^n_T(h- h_i)||_2 + \epsilon\\
        &= \frac{1}{N} \sum_{n=0}^{N-1} ||h- h_i||_2 + \epsilon\\
        &\leq \frac{1}{N} \sum_{n=0}^{N-1} \epsilon + \epsilon = 2\epsilon\\
    \end{align*}
    Thus, we have what we desire. Using the fact that $f = P_Tf + h$, we find that:
    \[||\frac{1}{N}\sum_{n=0}^{N-1} U^n_T f||_2 \xrightarrow{L^2_\mu} P_Tf\]
\end{proof}

\begin{corollary}
        Given $(X,\B, \mu, T)$ measure-preserving system. Then for any $f\in L^1$
        \[\frac{1}{N} \sum_{k=0}^{N-1} U^k_T(f) \rightarrow f'\]
        in the $||\cdot||_1$ norm. Where $f'$ is a $T$-invariant $L^1$ function.
\end{corollary}
\begin{proof}
    We first show that if $L^\infty_\mu \subset L^2_\mu$. Let $g\in L^\infty_\mu$, then we have that $||g||_\infty = C$ where $|g(x)| \leq C$ for almost every $x$. Now consider:
    \[\int |g(x)|^2 d\mu \leq \int C^2 d\mu = C^2 \implies (\int |g(x)|^2 d\mu)^\frac{1}{2} \leq C\]
    Thus, $g \in L^2_\mu$. Then applying the Mean Ergodic Theorem we have that $A^g_N \rightarrow g' \in L^2_\mu$. We now claim that $g' \in L^\infty_\mu$. We can first notice:
    \[||A^g_N||_\infty = ||\frac{1}{N} \sum_{n=0}^{N-1} g(T^n(x))||_\infty \leq \frac{1}{N} \sum_{n=0}^{N-1} ||g(x))||_\infty =||g||_\infty\]
    Now using the fact that $||A^g_N||_\infty \leq ||g||_\infty$ we can find:
    \[|\langle A^g_N, \chi_B\rangle| = |\int_B A^g_N d\mu| \leq \int_B||g||_\infty d\mu = ||g||_\infty \mu(B)\]
    Furthermore, by properties in a Hilbert space, we know that:
    \[|\langle g', \chi_B\rangle| \leq ||g||_\infty \mu(B)\]
    We claim that $||g'||_\infty \leq ||g||_\infty$. Next, we show that $||\cdot||_1 \leq ||\cdot||_2$. Consider applying Cauchy-Shwarz as follows:
    \[||f||_1 = \int |f| d\mu = |\langle |f|,\chi_X\rangle\rangle \leq ||f||_2||\chi_X||_2 = ||f||_2\]
    We let $f\in L^1_\mu$ and for $\epsilon>0$ we can pick a $g\in L^\infty_\mu$ such that $||f-g||_1<\epsilon$. Now consider:
    \[||\frac{1}{N} \sum_{n=0}^{N-1} f \circ T^n - \frac{1}{N} \sum_{n=0}^{N-1} g\circ T^n||_1 \leq \frac{1}{N} \sum_{n=0}^{N-1} ||f\circ T^n - g\circ T^n||_1 < \epsilon \]
    Furthermore, we know by what we just proved that $A^g_N \rightarrow g$ in $L^1_\mu$, so there is some $N_0$ large such that:
    \[||\frac{1}{N} \sum_{n=0}^{N-1} g\circ T^n - g'||_1 \leq \frac{1}{N} \sum_{n=0}^{N-1} ||g\circ T^n - g'||_1 < \epsilon\]
    Putting everything together we find for $N,N'>N_0$:
    \begin{align*}
        ||\frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n - \frac{1}{N'} \sum_{n=0}^{N'-1} f\circ T^n||_1 &\leq ||\frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n - \frac{1}{N} \sum_{n=0}^{N-1} g\circ T^n||_1 \\ 
        &+ ||\frac{1}{N} \sum_{n=0}^{N-1} g\circ T^n - g'||_1 \\
        &+ ||g' - \frac{1}{N} \sum_{n=0}^{N'-1} g\circ T^n||_1 \\
        &+ || \frac{1}{N} \sum_{n=0}^{N'-1} g\circ T^n - \frac{1}{N'} \sum_{n=0}^{N'-1} f\circ T^n||_1 \\
        &<4\epsilon
    \end{align*}
    Thus, the ergodic averages are Cauchy and by Riesz-Fischer we know that the space is complete, so there exists an $f'\in L^1_\mu$ such that the sequence of averages converges. Now a small calculation can show:
    \begin{align*}
        ||(\frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n)\circ T^n - \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n||_1 &= \frac{1}{N}||f\circ T^N - f||\\
        &\leq \frac{2}{N}||f||_1
    \end{align*}
    The above holds for all $N\geq 1$, so the above converges to $0$. This implies that $f'$ must be $T$-invariant.
\end{proof}

\subsubsection{Maximal Ergodic Theorem}
\begin{theorem}
    (Maximal Inequality) Let $U:L^1_\mu \rightarrow L^1_\mu$ be a positive linear operator such that $||U||\leq 1$. Let $f\in L^1_\mu$ real-valued and define:
    \[f_0 = 0 \text{ and } f_n = f +Uf +\cdots + U^{n-1}f\]
    Define $F_N = \max\{f_n: 0\leq n \leq N\}$ pointwise. Then we have:
    \[\int_{\{x: F_N(x) > 0\}} f \geq 0\]
    For all $N\geq 1$.
\end{theorem}
\begin{proof}
    We can see for all $N$ that $F_N \in L^1_\mu$, so we omit this proof. Certainly, by definition we have:
    \[F_N \geq f_n \text{ for all } 0\leq n \leq N\]
    We have the $U$ is a positive operator, so it preserves inequalities:
    \[UF_N + f \geq Uf_n + f = f_{n+1}\]
    Taking the max on both sides over $0\leq n \leq N-1$ we find:
    \[UF_N +f \geq \max_{0\leq n \leq N-1} f_{n+1} = \max_{1\leq n \leq N} f_n\]
    We now define a set:
    \[E = \{x: F_N(x) > 0\}\]
    Now we can note that if $x\in E$:
    \[F_N = \max_{0\leq n\leq N} f_n = \max_{1\leq n\leq N} f_n \text{ since } f_0 =0\]
    Therefore, we have on the set $E$:
    \[f\geq F_N - UF_N\]
    By monotonicity of integrals we find:
    \begin{align*}
        \int_E f &= \int_E F_N d\mu- \int_E UF_Nd\mu\\
        &= \int_X F_N d\mu - \int_E UF_Nd\mu\\
        &\geq \int_X F_N d\mu- \int_X UF_N d\mu\\
        &= ||F_N||_1 - ||UF_N||_1 \geq 0
    \end{align*}
    Where $F_N(x),UF_N(x) \geq 0$ by definition.
\end{proof}

\begin{theorem}
    (Maximal Ergodic Theorem)Given $(X,\B, \mu, T)$ measure-preserving system. Let $g\in L^1$, real values, and define
    \[E_\alpha = \{x\in X: \sup_{n\geq 1} \frac{1}{n} \sum_{k=0}^{n-1} g(T^k(x))> \alpha \}\]
    for $\alpha \in \RR$. Then
    \[\alpha \mu(E_\alpha) \leq \int_{E_\alpha} gd\mu \leq ||g||_1\]
    Moreover if $A$ is invariant under $T$, then
    \[\alpha \mu(E_\alpha \cap A\} \leq \int_{E_\alpha \cap A} g d\mu\]
\end{theorem}
\begin{proof}
    We consider some $g\in L^1_\mu$ and define $f = (g-\alpha) \in L^1_\mu$. Define a set:
    \[E_\alpha = \bigcup_{N=0}^\infty \{x: F_N(x) > 0\} = \bigcup_{N=0}^\infty \{x: \max_{0\leq n \leq N} f_n(x) > 0\} = \bigcup_{N=0}^\infty \{x: \max_{0\leq n \leq N} \sum_{k=0}^{n} f\circ T^k > 0\}\]
    Now we can use the same notation as in the Maximal Inequality and apply the theorem. Hence, we find:
    \[\int_{E_\alpha} f d\mu \geq 0 \implies \int_{E_\alpha} g - \alpha\mu(E_\alpha) \geq 0 \implies \int_{E_\alpha} g \geq \alpha \mu(E_\alpha)\]
\end{proof}

\subsubsection{Birkhoff Ergodic Theorem}
The famous Birkhoff ergodic theorem is next. This statement is quite well known and deservingly so. This gives time average is equal to space average along orbits of points. In the previous ergodic theorems we have never looked at a specific orbit a point. Not only that, but the convergence was with respect to $p$-norms. The pointwise ergodic theorem omits the $p$-norms and has classical convergence. This is quite remarkable and Birkhoff proves this theorem in (1923). A remark is that the published date for the Mean ergodic theorem predates this theorem, but was in fact the precursor to the pointwise theorem.

\begin{theorem}
    Given $(X,\B, \mu, T)$ measure-preserving system and $f\in L^1$. Then
    \[\lim_{n\rightarrow \infty}\frac{1}{n} \sum_{k=0}^{n-1} f(T^k(x)) = g\]
    converges almost everywhere. Furthermore, if the convergence is thought of in $L^1$ sense, then the left hand side converges to a $T$-invariant function $g$ and
    \[\int g d\mu = \int fd\mu\]
    On top of all of this, if $T$ is ergodic, then
    \[g(x) = \int f d\mu\]
    almost everywhere.
\end{theorem}
\begin{proof}
    WLOG we can assume that $f$ is real valued, since we can always split a complex function into real valued functions. Now define:
    \[f^*(x) = \limsup_{n\in \NN} \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n\]
    \[f_*(x) = \liminf_{n\in \NN} \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n\]
    Now we claim that $f^* = f_*$ almost everywhere and is $T$-invariant. First, define:
    \[a_N = \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n\]
    Notice the following:
    \[\frac{1}{N} \sum_{n=0}^{N-1} f(T^n(T(x)) = \frac{N+1}{N} \frac{1}{N+1} \sum_{n=0}^{N} f\circ T^n - \frac{1}{N}f(x) = \frac{N+1}{N} a_{N+1} - \frac{1}{N}f(x)\]
    Now consider:
    \[f^*(T(x)) = \limsup [\frac{N+1}{N} a_{N+1} - \frac{1}{N}f(x)] = \lim \frac{N+1}{N} \limsup a_{N+1} = f^*(x)\]
    Thus, $f^*\circ T = f$ almost everywhere. Similar argument can be done for $\liminf$. Now define $\alpha > \beta$ for $\alpha, \beta \in \QQ$ and define:
    \[E^\beta_\alpha := \{x: f_*(x) < \beta \text{ and } f^*(x) > \alpha\}\]
    Now notice that:
    \[E_\alpha := \{x: f^*(x) > \alpha \} \supset E^\beta_\alpha\]
    We can also notice that:
    \[E_\alpha = \{x : \limsup_{n\in \NN} \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n > \alpha\}\]
    Thus, we can apply the maximal ergodic theorem to find:
    \[\alpha \mu(E^\beta_\alpha) \leq \alpha \mu(E_\alpha) \leq \int_{E_\alpha} f d\mu\]
    Similarly, we can define sets:
    \[E^\beta := \{x: f_*(x) < \beta\} = \{x: \liminf_{n\in \NN} \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n<\beta \}\]
    If we let $f = -f$,  the above set becomes a $\limsup > \beta$, so we can again apply the maximal theorem:
    \[\alpha \mu(E^\beta_\alpha) \leq \beta \mu(E^\beta) \leq \int_{E^\beta} f d\mu \implies -\alpha \mu(E^\beta_\alpha) \geq -\beta \mu(E^\beta) \geq - \int_{E^\beta} f d\mu\]
    Putting the two together we find:
    \[(\alpha-\beta)\mu(E^\beta_\alpha) \leq 0\]
    This holds for all $\alpha>\beta$ in $\QQ$, so we can let the difference be arbitrarily small. Hence, we have that $\mu(E^\beta_\alpha) = 0$ 
    Now it follows that:
    \[\mu(\bigcup_{\alpha > \beta} \mu(E^\beta_\alpha)) = 0\]
    Hence, we have that $f^*(x) = f_*(x)$  almost everywhere. Thus, it follows that:
    \[a_N = \frac{1}{N} \sum_{n=0}^{N-1} f\circ T^n \rightarrow f^*\]
    almost everywhere and by applying corollary 1 we know that $a_N \rightarrow f'$ in $L^1_\mu$. Thus, there exists a subsequence $a_{N_k} \rightarrow f'$ almost everywhere. This implies that $f*=f'\in L^1_\mu$, since subsequences must converge to the same limit as the sequence. Furthermore, convergence in $L^1_\mu$ holds too, since $f'=f*$ almost everywhere.
\end{proof}

\subsubsection{Kingman's Subadditive Ergodic Theorem}
Now if we upgrade our space to an ergodic space we can relax the additivity condition to subadditivity. This next theorem will play a key role in our discussion later on Lyapunov exponents.

\begin{theorem}
    Given $(X,\B, \mu, T)$ ergodic. If $\{f_n\}: X \rightarrow \RR$ are $L^1$ and satisfy 
    \[f_{n+m}(\omega) \leq f_n(\omega) + f_m(T^n\omega)\]
    then there is a constant function $f$ so that
    \[\lim_{n\rightarrow \infty} \frac{1}{n} f_n(\omega) = f\geq -\infty\]
    for a.e. $\omega \in X$. Moreover, if we have a uniform bound $||f_n||_\infty \leq Cn$ with constant $C$ then
    \[f = \inf_{n\geq 1} \frac{1}{n}\int f_n d\mu \]
\end{theorem}


\newpage
\section{Part III}
\subsection{Spectral Properties}

Now given $A\in \B(\V)$ we define the following.
\begin{definition}
    The \textbf{resolvent set} of $A$ is
    \[\rho(A) :=\{z\in \CC: A-zI \text{ is invertible}\}\]
\end{definition}

\begin{definition}
    The \textbf{spectrum} of $A$ is
    \[\sigma(A) := \CC\setminus \rho(A) = \{z\in \CC: A-zI \text{ is not invertible}\}\]
\end{definition}

\begin{definition}
    We say $z\in \CC$ is an \textbf{eigenvalue} if 
    \[Av = zv \text{ for some nonzero } v\in \V\]
\end{definition}

\begin{definition}
    The \textbf{discrete spectrum} of $A$ is 
    \[\sigma_{disc}(A) := \{z\in \CC: 0< \dim\ker(A-zI) < \infty \text{ and } \exists \delta >0 \text{ st } B(z,\delta)\cap \sigma(A) = \{z\}\}\]
\end{definition}

\begin{definition}
    The \textbf{essential spectrum} of $A$ is
    \[\sigma_{ess} = \sigma(A) \setminus \sigma_{disc}(A)\]
\end{definition}

\begin{definition}
    The \textbf{spectral radius} of $A$ is
    \[spr(A) := \sup_{z\in \sigma(A)} |z|\]
\end{definition}

\begin{definition}
    The \textbf{resolvent} of $A$ is
    \begin{align*}
        R(A,\cdot): \rho(A) &\rightarrow \B(\V)\\
        z&\mapsto (A-zI)^{-1}
    \end{align*}
\end{definition}

Most of these definitions will be put onto the back burner. The importance of this section is to really know what the spectrum of an operator is. Notice that in the definition we discuss the spectrum of a single operator. A good question is how does the spectrum of an operator behave under operations? If I have a family of operators do they have a spectrum that they all share? There are many questions that can be asked now about the spectrum of operators. Though in the next part we will see the question we are interested in.


\newpage
\subsection{Special Linear 2x2 Matrices}

\begin{definition}
    We define the set of \textbf{special linear 2x2 matrices} as
    \[SL_2(F) := \{A\in Mat_{2\times 2}(F): \det(A) = 1\}\]
    Often this can be notated as $SL(2,F)$. 
\end{definition}

In particular, we usually consider $F = \RR$. These matrices play a key role and we will see how with the relation to Schr\"{o}dinger Operators. Furthermore, we can equip this space with the operator norm.

\begin{definition}
    Given an operator $A$ we can define the \textbf{operator norm} as follows
    \[||A||_{op} := \sup_{||x||=1} ||Ax||\]
    We generally omit the subscript $op$.
\end{definition}

We state the norm in the general sense, but we can just state that given a matrix $A$ blah blah. Furthermore, computing operator norms can be tedious especially if we have infinitely many matrices. Therefore we define the notion of equivalent norms and an important theorem using this definition.
\begin{definition}
    Given $||\cdot||_1, ||\cdot||_2$ two norms on a space $X$, then the norms are equivalent if there exists $c,C>0$ such that
    \[c||\cdot||_1\leq ||\cdot||_2 \leq C||\cdot||_1\]
\end{definition}
\begin{theorem}
    All matrix norms are equivalent.
\end{theorem}
\begin{proof}
    ISTS that all norms on $\RR^n$ are equivalent. Why? We can embed matrices into $\RR^n$, more specifically for us we care about $2\times 2$ matrices, so we look at $\RR^4$. We simply consider the map
    \begin{align*}
        \phi: Mat_{2\times 2}(F) &\rightarrow \RR^4\\
        \begin{pmatrix}
            a&b\\
            c&d
        \end{pmatrix} &\mapsto (a,b,c,d)
    \end{align*}
    Therefore let us show that all norms are equivalent on $\RR^n$. Before continuing we must define two norms of importance
    \begin{align*}
        ||\cdot||: \RR^n &\rightarrow \RR  & ||\cdot||_\infty: \RR^n &\rightarrow \RR\\
        (x_1,\dots, x_n)&\mapsto \sqrt{\sum_{1}^n x_j^2} & (x_1,\dots, x_n)&\mapsto \max_{j} |x_j|
    \end{align*}
    We will show equivalence of all norms in the following steps
    \begin{enumerate}
        \item The two norms above are equivalent.
        \item Any norm is equivalent to the $\infty$ norm.
    \end{enumerate}
    Let us begin with step $1$. Notice
    \[||x||^2 = \sum_{1}^n x_j^2 \leq \sum_{1}^n ||x||^2_\infty = n||x||_\infty \]
    Hence, $C = \sqrt{n}$. On the other hand let us say WLOG that $||x||_\infty = |x_j|$.
    \[||x||^2_\infty = |x_k|^2 = x_k^2 \leq \sum_{1}^n x^2_j = ||x||^2\]
    Hence, $c = 1$ which implies
    \[||\cdot||_\infty \leq ||\cdot|| \leq \sqrt{n}||\cdot||_\infty\]

    Now for step 2 let $||\cdot||_?$ be any norm equipped on $\RR^n$. Let $x\in \RR^n$, then we can write
    \[x = \sum_1^n x_ie_i \text{ where } e_i \text{ is standard basis of } \RR^n\]
    Then
    \begin{align*}
        ||x||_? &\leq \sum_1^n |x_i| ||e_i||_?\\
        &\leq \sum_{1}^n ||x||_\infty ||e_i||_?\\
        &= ||1||_? ||x||_\infty
    \end{align*}
    Thus, $C = ||1||_?$. By the same argument as before we are bounded below by $c = 1$.
\end{proof}

This theorem essentially tells us that we need not use the operator norm and can use any norm. In most calculations it will be much easier to use a different norm. Explicitly notated in mathematical terms we use the following.
\[||A|| = \max_{i,j} |a_{ij}|\]
By norm equivalence we know that there are some constants that gives us and upper and lower bound on the operator norm. 

In this setting of $SL_2(F)$ we can now consider what eigenvalues will look like. Consider $A\in SL_2(F)$ where
\[A = \begin{pmatrix}
    a & b\\
    c & d
\end{pmatrix}\]
Using the fact that $ad - bc = 1$ we can compute and find
\begin{align*}
    \det(\lambda I - A) &= \det \begin{pmatrix}
    \lambda - a & -b\\
    -c & \lambda - d
    \end{pmatrix}\\
    &= (\lambda - a)(\lambda - d) - bc \\
    &= \lambda^2 - (a+d)\lambda +(ad - bc)\\
    &= \lambda^2 -Tr(A)\lambda + 1
\end{align*}
Thus, the eigenvalues of $SL_2(F)$ matrices are completely determined by the trace of the matrix. This implies eigenvalues are of the form
\[\lambda = \frac{Tr(A)}{2} \pm \frac{\sqrt{Tr(A)^2 - 4}}{2}\]
This essentially tells us that depending on the values of $Tr(A)$ we will get repeated real, two distinct reals, complex conjugated. Giving us the following definition.
\begin{definition}
    Given a matrix $A\in SL_2(\RR)$ we say that
    \begin{enumerate}
        \item $A$ is \textbf{elliptic} if $Tr(A) \in (-2,2)$.
        \item $A$ is \textbf{hyperbolic} if $Tr(A) \in \RR\setminus (-2,2)$
        \item $A$ is \textbf{parabolic} if $Tr(A) \in \{-2,2\}$
    \end{enumerate}
\end{definition}
In fact, depending on which definition $A$ satisfies, then you can determine the eigenvalues (Ex. 1). Now we move on to a very important proposition that will be utilized heavily when discussing uniform hyperbolicity. The below uses $\CC$ and $\CC\PP^1$. Let us define $\CC\PP^1$ briefly, and we will discuss $\RR\PP^1$ (real projective) in more detail later. Complex projective space for us is $\CC\PP^1$ where we equip an equivalence relation on $\CC^2\setminus \{0\}$ via
\[v\sim v' \iff v = zv' \text{ for some nonzero } z\in \CC\]
Let us discuss one of our most important propositions.
\begin{proposition}
    If $A\in SL_2(\CC)$ and $||A||>1$, then there exists unique subspaces $S = S(A), U = U(A)$ that satisfying the following
    \begin{enumerate}
        \item For all $v_s\in S, v_u\in U$ one has
        \[||Av_s|| = ||A||^{-1} ||v_s|| \hspace{5mm} ||Av_u|| = ||A|| ||v_u||\]
        \item The subspaces are orthogonal.
        \item The subspaces are real whenever $A$ is.
        \item Given $V\subset \CC\PP^1$ if $||Av|| = R||v||$ for $v\in V$, then $\theta = \angle (V,S)$ satisfies
        \[\sqrt{\frac{R^2 - ||A||^{-2}}{||A||^2 - ||A||^{-2}}} \leq \theta \leq \frac{\pi}{2} \sqrt{\frac{R^2 - ||A||^{-2}}{||A||^2 - ||A||^{-2}}}\]
        Also,
        \[\theta \leq \frac{\pi}{2}R ||A||^{-1}\]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Use SVD (Singular Value Decomposition), which handles $1-3$. To prove $1-3$ is essentially to prove SVD. To specialize to this case we can let $|A|^2 := A^*A$ and then note that eigenvalues are $||A||^{\pm 2}$. Letting $S$ be eigenspace with respect to $||A||^{-2}$ and $U$ with respect to $||A||^2$.

    Let us prove $4$. Let $v\in V$ unit vector. Then there are unit vectors $v_s\in S, u_s\in U$ so that we can write
    \[v = \cos(\theta)v_s + \sin(\theta)u_s\]
    By orthogonality we can write
    \[R^2 = ||Av||^2 = ||A\cos(\theta)v_s + A\sin(\theta)u_s||^2 = ||A||^{-2}\cos^2(\theta) + ||A||^2\sin^2(\theta)\]
    It follows 
    \begin{align*}
        R^2 &= ||A||^{-2}\cos^2(\theta) + ||A||^2\sin^2(\theta)\\
        &= ||A||^{-2}(1- \sin^2(\theta)) + ||A||^2\sin^2(\theta)\\
        &= ||A||^{-2}-||A||^{-2}\sin^2(\theta) + ||A||^2\sin^2(\theta)\\
        &= ||A||^{-2} + (-||A||^{-2} + ||A||^2) \sin^2(\theta)\\
    \end{align*}
    Hence,
    \[\sin^2(\theta) = \frac{R^2 - ||A||^{-2}}{||A||^2 - ||A||^{-2}}\]
    Remark that $\sin(x) \leq x \leq \pi/2 \sin(x)$ on $[0,\pi/2]$ we have the first inequality. Since $R\leq ||A||$, we have $0\leq R^2/||A||\leq 1$ which implies
    \[\frac{R^2 - ||A||^{-2}}{||A||^2 - ||A||^{-2}} \leq \frac{R^2}{||A||^2}\]
    So using the first inequality and the above we have the last inequality.
\end{proof}

\subsubsection{Exercises}
\begin{enumerate}
    \item Show the following for $A\in SL_2(\RR)$
    \begin{enumerate}
        \item If $A$ is elliptic, then eigenvalues are $e^{i\theta}$.
        \item If $A$ is hyperbolic, then eigenvalues are $\lambda, 1/\lambda$ for $|\lambda| > 1$.
        \item If $A$ is parabolic, then $A$ is similar to
        \[\begin{pmatrix}
            1 &\pm1\\
            0 & 1
        \end{pmatrix}\]
    \end{enumerate}
\end{enumerate}

\newpage
\subsection{Projective Special Linear}
Before we can define what projective special linear matrices look like we first need to define projective space. We consider the $\RR^2\setminus \{0\}$ and we define an equivalence relation $\sim$. We say $x\sim y$ for $x,y\in \RR^2\setminus \{0\}$ if there exists some $c\in \RR$ nonzero such that $x = cy$. Under this relation this induces real projective space, which is denoted as $\RR\PP^1$. What will the equivalence classes look like? It turns out under this equivalence relation we are saying that everything that lies on the same line is equivalent. In other words, we consider all lines passing through the origin and each line represents an equivalence class. Furthermore, it turns out that $\RR\PP^1$ can be thought of as $[0,1]/\sim$ where $0\sim 1$, which means the endpoints are glued. This further implies that $\RR\PP^1$ is the same as $S^1$. Again we can go further as $S^1$ can be thought of as $\RR/\ZZ$ and scaled we can write $S^1$ as $\RR/\pi \ZZ$. The point of this space is that the dynamics of what we want to study will be nicer to explore in $\RR\PP^1$. Let us now define $PSL_2(\RR)$.
\begin{definition}
    We define:
    \[PSL_2(\RR) := \{\Tilde{A}: \RR\PP^1 \rightarrow \RR\PP^1: \text{ where } \Tilde{A} \text{ is induced via } A\in SL_2(\RR)\}\]
\end{definition}
This definition is not that satisfying, as we know need to know what $\Tilde{A}$ looks like. The main question is how can we make the following diagram commute: 
\[\begin{tikzcd}
	{\RR^2} & {\RR^2} \\
	{\RR\PP^1} & {\RR\PP^1}
	\arrow["A", from=1-1, to=1-2]
	\arrow["U"', dashed, from=1-1, to=2-1]
	\arrow["U", dashed, from=1-2, to=2-2]
	\arrow["{\tilde{A}}"', from=2-1, to=2-2]
\end{tikzcd}\]
What does diagram commuting mean? Formally, it means that $UA = U\tilde{A}$. Informally imagine we are starting in the top left and want to end in the bottom right. Then what this is saying is it does not matter if we follow the path of $A$ or $U$ first, in either case the path ends at the same spot. The question is how can we derive such an equation. Remember that $\RR^2$ is a vector, while $\RR\PP^1$ is the angle of this vector. So we need to define a map $U$ that allows us to move from vectors to angles. What types of maps do this? Trigonometric functions like $\sin, \cos, \tan$. It is left as an exercise to define $U$.

\subsubsection{Exercises}
\begin{enumerate}
    \item Define $U$ in the diagram above. Is this $U$ unique?
\end{enumerate}

\newpage
\subsection{Special Linear Cocycles and relation to Schr\"{o}dinger Operators.}
At this point we have made significant progress to our end goal. We are now finally going to see the setting we are in for the primary goal of the reading course. We first introduce $SL_2$-cocycles. Then after this introduction we will give the setup of Schr\"{o}dinger Operators and see the relation between the two. Let us consider some \textbf{base dynamics}. In other words, we consider a probability space $(\Omega, \B, \mu, T)$ where $T: \Omega \rightarrow \Omega$. Next, consider a map $A: \Omega \rightarrow SL_2(\RR)$, which essentially says that each element $\Omega$ has a corresponding matrix associated to it. From this we can define the $SL_2$-cocycles, which are
\begin{align*}
    (T,A): \Omega \times \RR^2 &\rightarrow \Omega \times \RR^2\\
    (\omega,\Vec{v}) &\mapsto (T\omega, A(\omega) \Vec{v})
\end{align*}
We like dynamics, so what do we do? We iterate the map... If $T$ is invertible, then define
\begin{align*}
    A^n: \Omega &\rightarrow SL_2(\RR)\\
    \omega &\mapsto \begin{cases}
        A(T^{n-1}\omega) \cdots A(T\omega) A(\omega) & \text{ if } n\geq 1\\
        I & \text{ if } n = 0\\
        A(T^n\omega)^{-1}\times \cdots \times A(T^{-1}\omega)^{-1} & \text{ if } n\leq 1\\
    \end{cases}
\end{align*}
If $T$ was not invertible we can define the above, but for $n\leq 1$ we would only consider positive $n$. The same setup can be done for $SL_2(\CC)$. Furthermore, by properties of $SL_2(\RR)$ we can projectivise the cocycles to receive
\[(T,A): \Omega \times \RR\PP^1 \rightarrow \Omega \times \RR\PP^1\]
This idea of projectivising is key as it turns out to understand dynamics of these maps well, it suffices to understand how the angle behaves, which is understanding how the dynamics behaves in $\RR\PP^1$. Of course, we can do the same for $\CC\PP^1$. Note that similar to last section we can ask what is the map $U$ so that the following diagram commutes?
\[\begin{tikzcd}
	{\Omega\times \RR^2} & {\Omega\times \RR^2} \\
	{\Omega\times \RR\PP^1} & {\Omega\times \RR\PP^1}
	\arrow["{(T,A)}", from=1-1, to=1-2]
	\arrow["U"', from=1-1, to=2-1]
	\arrow["U", from=1-2, to=2-2]
	\arrow["{(T,\tilde{A})}"', from=2-1, to=2-2]
\end{tikzcd}\]


Let us define Schr\"{o}dinger Operators and see the relation to cocycles now. First, let us consider an ergodic probability space $(\Omega, \B, \mu, T)$ and some $f: \Omega \rightarrow \RR$ that is bounded and measurable. Let us consider \textbf{Schr\"{o}dinger Operators} defined on $\ell^2(\ZZ)$ as follows
\[[H_w\psi](n) = \psi(n+1) + \psi(n-1) + V_\omega(n) \psi(n)\]
Where $\psi \in \ell^2(\ZZ)$ and
\[V_w(n) = f(T^n\omega)\]
is called the \textbf{potential}. In fact, with how we have defined the potential gives rise to the name of \textbf{dynamically defined potential}. We are interested in solutions to the eigenvalue equation $H_w \psi = E\psi$. Let us first definewhat we call \textbf{transfer matrices}
\[A_E(w) := \begin{pmatrix}
    z - f(Tw) & -1\\
    1 & 0
\end{pmatrix}= \begin{pmatrix}
    z - V_w(1) & -1\\
    1 & 0
\end{pmatrix}\]
It turns out the following holds and will be left as an exercise
\[H_w \psi = E\psi \iff \begin{pmatrix}
    \psi(n+1)\\
    \psi(n)
\end{pmatrix} = 
A^n_E
\begin{pmatrix}
    \psi(1)\\
    \psi(0)
\end{pmatrix}\]
At this point please actually stop reading now and do this exercise. This exercise will explicitly show you how $SL_2(\RR)$ cocycles play a role in solving the eigenvalue equations.

\subsubsection{Exercises}
\begin{enumerate}
    \item Show 
    \[H_w \psi = E\psi \iff \begin{pmatrix}
    \psi(n+1)\\
    \psi(n)
    \end{pmatrix} = 
    A^n_E
    \begin{pmatrix}
        \psi(1)\\
        \psi(0)
    \end{pmatrix}\]

    \item Show that for any $\psi\in \ell^2(\ZZ)$ and $\omega\in \Omega$, then $H_\omega\psi \in \ell^2(\ZZ)$.
\end{enumerate}

\newpage
\section{Part IV}

\subsection{Uniformly Hyperbolic}
Now that we have the setting from section 3.3 we can now give the actual key definitions of interested. This definition is very complicated, as there are many equivalent definitions. Note that the cocycle identity, which will be referred to in the proof, is
\[A^{-n}(T^n\omega) = A^n(\omega)^{-1}\]
The equivalent conditions are
\begin{theorem}
    Given an $SL_2(\RR)$-cocyle $(T,A)$. TFAE
    \begin{enumerate}
        \item $(T,A)$ satisfies the exponential growth condition. This means that there exists constants $c>0, \lambda> 1$, so that
        \[||A^n(w)||\geq c\lambda^{|n|} \text{ for all } n\in \ZZ \text{ and for all } \omega\in \Omega\]

        \item $(T,A)$ admits an invariant exponential splitting. This means that there exists two maps \[\Lambda_s, \Lambda_u: \Omega \rightarrow \RR\PP^1\]
        and constants $c>0, \lambda >1$ satisfying
        \begin{enumerate}
            \item Invariance. For any $\omega\in \Omega$ 
            \[A(\omega)\Lambda_s(\omega) = \Lambda_s(T\omega) \text{ and } A(\omega)\Lambda_u(\omega) = \Lambda_u(T\omega)\]

            \item Exponential. For any $\omega\in \Omega$, $n\in \NN$, $\Vec{v}_s\in \Lambda_s(\omega)$, $\Vec{v}_u\in \Lambda_u(\omega)$, assume unit length, we have
            \[||A^n(\omega)\Vec{v}_s||\leq c\lambda^{-n} \text{ and } ||A^{-n}(\omega)\Vec{v}_u||\leq c\lambda^{-n}\]
        \end{enumerate}
        We say that $\Lambda_s$ is stable direction and $\Lambda_u$ is unstable. The above states that the stable direction expands exponentially with respect to reverse time and the unstable direction expands exponentially with respect to forward time.

        \item $(T,A)$ has no bounded orbit. We say $(T,A)$ has bounded orbit if for some $\omega\in\Omega$ and some $v\in \SS^1$ then
        \[||A^n(\omega)v||\leq 1\]
        for all $n\in \ZZ$.

        \item $(T,A)$ is projectively conjugate to a hyperbolic diagonal cocycle. This means that there are constants $\alpha>0, \beta >1$ and a continuous function $B:\Omega \rightarrow PSL(2,\RR)$ and $r: \Omega \rightarrow \RR_+$ such that
        \[B(T\omega)^{-1}A(\omega)B(\omega) = \begin{pmatrix}
            r(\omega) & 0\\
            0 & r(\omega)^{-1}
        \end{pmatrix} \text{ for all } \omega\in \Omega\]
        Furthermore,
        \[r_n(\omega) := \prod_{j=0}^{n-1} r(T^j\omega) \geq \alpha \beta^n\]
    \end{enumerate}
    If any of the above are satisfied then $(T,A)$ is said to be \textbf{uniformly hyperbolic}, $(T,A)\in \U\H$ for short.
\end{theorem}
\begin{proof}
    We will begin with $(1) \implies (2)$. We begin with existence of the stable and unstable direction. Fix some $c,C>0$, where $c$ is small and $C$ is large, and $\lambda > 1$ so that the assumption holds. Now we remark that $A$ is continuous and $\Omega$ is compact and hence $A$ achieves a maximum with respect to any norm equipped onto $SL(2,\RR)$. Hence, we may write
    \[Q:= \max_{\omega\in\Omega} ||A(\omega)||\]
    Furthermore, we may assume that $|n| > \log(C^{-1})/\log(\lambda)$, so that we can safely assume $||A^n(\omega)||>1$. Indeed, if $|n|> \log(C^{-1})/\log(\lambda)$ we find
    \[\frac{\log(C^{-1})}{\log(\lambda)} = \log_\lambda(C^{-1}) \implies \lambda^{\log_\lambda(C^{-1})} = C^{-1}\]
    We are now in the position to apply Proposition 3.2.1. Thus, we can let $S(A) \in \RR\PP^1$ be the most contracted direction for $A$ and $U(A)$ the most expanded direction of $A$. Now for each $\omega\in \Omega$ we can let $\Lambda^s_n(\omega):= S(A^n(\omega))$. Let $v_n(\omega) \in \Lambda^s_n(\omega)$, then we have
    \begin{align*}
        ||A^n(\omega)v_{n+1}(\omega)|| &= ||A(T^n\omega)^{-1}A^{n+1}(\omega)v_{n+1}(\omega)||\\
        &\leq ||A(T^n\omega)|| ||A^{n+1}(\omega)v_{n+1}(\omega)||\\
        &\leq Q ||A^{n+1}(\omega)||^{-1} ||v_{n+1}(\omega)||\\
        &=Q ||A^{n+1}(\omega)||^{-1}
    \end{align*}
    Where the first equality is clear, first inequality uses operator norm definition, second inequality uses definition of $Q$ and Proposition 3.2.1, and the final equality uses Proposition 3.2.1. The inequality essentially bounds $R$ in Proposition 3.2.1. Whence $V = \Lambda^s_{n+1}$ and $S = \Lambda^s_{n}$ we are in position to apply Proposition 3.2.1 again.
    \[d(\Lambda^s_n(\omega), \Lambda^s_{n+1}(\omega))\leq \frac{\pi}{2}Q ||A^{n+1}(\omega)||^{-1} ||A^n(\omega)||^{-1}\]
    Applying again
    \begin{align*}
        ||A^n(\omega)|| & ||A(T^n\omega)^{-1} A^{n+1}(\omega)||\\
        &\leq ||A(T^n\omega)^{-1}||  ||A^{n+1}(\omega)||\\
        &\leq Q ||A^{n+1}(\omega)||\\
    \end{align*}
    This implies
    \[||A^{n+1}(\omega)||\geq Q^{-1}||A^n(\omega)||   \implies ||A^{n+1}(\omega)||^{-1}\leq Q||A^n(\omega)||^{-1}\]
    From this we have
    \[d(\Lambda^s_n(\omega), \Lambda^s_{n+1}(\omega))\leq \frac{\pi}{2}Q^2 ||A^n(\omega)||^{-2} \leq C\lambda^{-2n}\]
    This shows that $\{\Lambda^s_n(\omega)\}_1^\infty$ is Cauchy sequence in $\RR\PP^1$ for each $\omega\in \Omega$. Therefore for each $\omega\in \Omega$ we can define the pointwise limit, so define
    \[\Lambda^s(\omega) := \lim_{n\rightarrow \infty} \Lambda^s_n(\omega)\]
    We remark, without proof, that as a map $\Lambda^s_n: \Omega \rightarrow \RR\PP^1$ is continuous. Hence, if we can show uniform convergence we have $\Lambda^s$ is continuous. Indeed, consider
    \begin{align*}
        d(\Lambda^s_n(\omega), \Lambda^s(\omega)) &\leq \sum_{m=n}^\infty d(\Lambda^s_m(\omega), \Lambda^s_{m+1}(\omega))\\
        &\leq C\sum_{m=n}^\infty \lambda^{-2m}\\
        &\leq C\lambda^{-2n}
    \end{align*}
    Thus, we have uniform convergence.\\
    Now we show invariance. Consider
    \[||A^n(T\omega)A(\omega)v_{n+1}(\omega)|| = ||A^{n+1}(\omega) v_{n+1}(\omega)|| = ||A^{n+1}(\omega)||^{-1} \leq C\lambda^{-{n+1}} \leq C\lambda^{-n}\]
    Hence, we are in then position to apply Proposition 3.2.1 again. Therefore we have
    \[d(A(\omega)\cdot \Lambda^s_{n+1}(\omega), \Lambda^s_n(T\omega)) \leq C\lambda^{-n}\]
    Taking limits we have
    \[A(\omega)\cdot \Lambda^s(\omega) = \Lambda^s(T\omega)\]
    To show the unstable we simply run the argument above, but define
    \[\Lambda^u(\omega) := \lim_{n\rightarrow -\infty} \Lambda^s_n(\omega)\]
    The details are essentially the same. \\
    We now want to show exponential growth and to do so we need to first show that $\Lambda^s(\omega) \neq \Lambda^u(\omega)$ for each $\omega\in \Omega$. To do so we show:
    \[\forall \omega \in \Omega, \exists n,k\in \ZZ \text{ so that } A^n(T^k\omega) \text{ shrink (expands) vectors in } \Lambda^s(T^k\omega) \hspace{2mm}(\Lambda^u(T^k\omega))\]
    Let us begin by fixing an $\omega$ defining the following set
    \[Q := \{r>1 : \text{All $n\in\ZZ$ except for finitely many satisfy }: ||A^n(T^k\omega)||\geq Cr^{|n|} \text{ for all } k\in \ZZ\]
    Let $R = (\sup M)^{3/4}$. First, note that the right hand side is nonempty by exponential growth condition. Furthermore, we clearly have
    \[M \leq Q < \infty \implies \sup M < \infty\]
    Now by $R < \sup M$ we can let $C' = \min\{C, R^{-N}\}$, note only depending on $\omega$, where $N$ is the maximal number $n$ so that there exists $k$
    \[||A^n(T^k\omega)||< CR^{|n|}\]
    is satisfied. With this we have
    \[||A^n(T^k(\omega))||\geq C'R^{|n|} \text{ for all } n,k\in \ZZ\]
    Thus, running the same argument in showing $\Lambda^s(\omega)$ existed, but with these new exponential bound we find
    \[d(\Lambda^s(T^k\omega), \Lambda^s_{n}(T^k\omega)) \leq C'R^{-2n}\]
    On the other hand, we know that $R^{3/2}>\sup M$, which implies there exists subsequence $\{n_j\}, \{k_j\}$ so that
    \[||A^{n_j}(T^{k_j}\omega)||\leq CR^{\frac{3}{2}|n_j|}\]
    Where $|n_j| \rightarrow\infty$ and WLOG we can assume $n_j \rightarrow \infty$. Let us define
    \[\phi_j := d(\Lambda^s(T^{k_j}\omega), \Lambda^s_{n_j}(T^{k_j}\omega)) \implies \phi_j \leq C'R^{-2|n_j|}\]
    Now consider
    \begin{align*}
        ||A^{n_j}(T^{k_j}\omega) v_s(T^{k_j}\omega)||^2 &= ||A^{n_j}(T^{k_j}\omega)||^{-2}\cos^2(\phi_j) + ||A^{n_j}(T^{k_j}\omega) ||^2\sin^2(\phi_j)\\
        &\leq C'(R^{-2n_j}  +R^{3n_j}(R^{-2n_j})^{2})
    \end{align*}
    Where we use Proposition 3.2.1, $\sin(x) \leq x$, and $\cos(x)\leq 1$. Notice that as $j\rightarrow \infty$ we have that for $j$ large 
    \[||A^{n_j}(T^{k_j}\omega) v_s(T^{k_j}\omega)|| < 1\]
    We want to show expanding for the unstable direction, so observe
    \[||A^{-n_j}(T^{k_j+n_j}\omega)|| = ||A^{n_j}(T^{k_j + n_j}\omega)^{-1}|| = ||A^{n_j}(T^{k_j}\omega)|| \leq CR^{\frac{3}{2}|n_j|}\]
    Then by a similar argument we have for large $j$
    \[||A^{n_j}(T^{k_j}\omega)v_u(T^{k_j}\omega)|| > 1\]
    Therefore $\Lambda^s(T^{k_j}\omega) \neq \Lambda^u(T^{k_j}\omega)$. By invariance we can go back to $\Lambda^s(\omega) \neq \Lambda^u(\omega)$. Furthermore, both are continuous functions and hence $\Lambda^s(\omega) - \Lambda^u(\omega)$ is a continuous function on a compact set and therefor achieves a maximum. Thus, there is some $\gamma>0$ so that $d(\Lambda^s(\omega), \Lambda^u(\omega))\geq \gamma$ for all $\omega$.\\
    We are now ready to show exponential decay and to do so we will define
    \[\theta_n(\omega):= d(\Lambda^s(T^n\omega), \Lambda^s_{-n}(T^n\omega))\]
    By the convergence to the unstable direction there must exist some $N\in \NN$ so that $\theta_n \geq \gamma/2$ for all $n\geq N$ and any $\omega$. Therefore we can compute the following
    \begin{align*}
        ||A^{-n}(T^n\omega)v_s(T^n\omega)||^2 &= ||A^{-n}(T^n\omega)||^2\sin^2(\theta_n) + ||A^{-n}(T^n\omega)||^{-2}\cos^2(\theta_n)\\
        &\geq C\lambda^{2n}\sin^2(\gamma/2)\\
        &\geq C\lambda^{2n}
    \end{align*}
    This implies
    \[||A^{-n}(T^n\omega)v_s(T^n\omega)||\geq C\lambda^n\]
    Notice that this bound essentially states that the stable direction in backwards time expands exponentially. Of course this implies exponential decay in the forward direction. Formally, this is done via invariance and the cocycle identity that we applied earlier above to get
    \[||A^{n}(\omega)v_s(\omega)||\leq C\lambda^{-n}\]
    The same can be done for the unstable direction. Completing $(1) \implies (2)$.
    \\ \vspace{2mm} \\
    $(2) \implies (3):$ Let $\omega\in \Omega$ and $v\in \SS^1$, then by the invariant exponential splitting there are constants $c_s,c_u$ so that we can write
    \[v = c_sv_s(\omega) + c_uv_u(\omega)\]
    By the exponential growth of in forward and backwards time $v$ cannot enjoy a bounded orbit.
    \\ \vspace{2mm} \\
    $(3) \implies (1):$ Let us define the following sets
    \[O_{n,\delta}:=\{(\omega, v)\in \Omega\times \SS^1: ||A^n(\omega)v||> 1 +\delta\}\]
    We claim that this set is open (\textcolor{red}{Will type proof of openness later}). By assumption no bounded orbit exists and hence $\{O_{n,\delta}\}_{n\in \ZZ}$ is an open cover of $\Omega\times \SS^1$. Since $\Omega, \SS^1$ are compact then the product is compact and hence there exists a finite subcover. Thus, we can pick $\epsilon>0$ so that there is an $N\in \NN$ such that for any $(\omega,v)$ there exists some $n\leq N$ such that
    \[||A^n(\omega)v|| > 1+\epsilon\]
    To show exponential growth now we need the following two lemmas.
    \begin{lemma}
        For $\omega\in \Omega$ and all $v\in \SS^1$ there exists a sequence $n_k\in \ZZ$ such that
        \begin{enumerate}
            \item $n_k$ is strictly monotone.
            \item $n_0 = 0$.
            \item $|n_k - n_{k-1}|\leq N$ for all $k\in \NN$.
            \item $||A^{n_k}(\omega)v||> (1+\epsilon)^k$ for every $k\in \NN$.
        \end{enumerate}
    \end{lemma}
    \begin{lemma}
        For every $\omega\in \Omega$ there exists $v\in \SS^1$ and a strictly increasing sequence that satisfies $(2) - (4)$ of the above.
    \end{lemma}
    \noindent To conclude we can let $\omega\in \Omega$ and $v\in \SS^1$ as in Lemma 4.1.2. Then by $(3), (4)$ of Lemma 4.1.1 gives us
    \[||A^{n_k}(\omega)||\geq ||A^{n_k}(\omega)v|| > (1+\epsilon)^{k}\geq (1+\epsilon)^{n_k/N}\]
    Where the last inequality follows through $(3)$ and a nice application of the triangle inequality
    \[|n_k|\leq |n_k-n_{k-1}| + |n_{k-1}| \leq \cdots \leq \sum_{j=1}^k |n_j - n_{j-1}| \leq kN\]
    We now abuse the uniform boundedness of $|n_k - n_{k-1}|$ to achieve an exponential lower bound. To do so recall the notation $Q = \max_{\omega \in \Omega}||A(\omega)||$ and fix any $0\leq r\leq N$. Observe
    \[1< ||A^{n_k + r}(\omega)|| \leq ||A^{n_k}(\omega)|| ||A(\omega)||^r \leq ||A^{n_k}(\omega)|| Q^r\]
    Using this in the first inequality we have
    \begin{align*}
        ||A^{n_k + r}(\omega)||&\geq ||A^{n_k}(\omega)||Q^{-r}\\
        &\geq (1+\epsilon)^k Q^{-r}\\ 
        &\geq \frac{(1+\epsilon)^{k + \frac{r}{N}}}{Q^N(1+\epsilon)}\\
        &\geq \frac{(1+\epsilon)^{\frac{1}{N}(n_k+r)}}{Q^N(1+\epsilon)}
    \end{align*}
    Thus, when we let 
    \begin{align*}
        \lambda &= (1+\epsilon)^{\frac{1}{N}} & C = \frac{1}{Q^N(1+\epsilon)}
    \end{align*}
    We have exponential growth bounds. To extend to the negative bounds we use the cocycle identity again
    \[||A^{-n}(\omega)|| = ||A^n(T^{-n}\omega)^{-1}|| = ||A^n(T^{-n}\omega)|| \geq C\lambda^{|n|}\]
    \\ \vspace{2mm} \\
    $(b) \implies (d):$ Due to the complexity of this direction we omit the proof.
    \\ \vspace{2mm} \\
    $(d) \implies (a):$ By the conjugacy to a hyperbolic diagonal cocycle it is a direct computation to show exponential growth of $(T,A)$.
\end{proof}

\subsubsection{Exercises}
\begin{enumerate}
    \item If $\Omega$ is a singleton and $T$ is the identity map and $A: \Omega \rightarrow SL_2(\RR)$. Show 
    \[(T,A)\in \uh \iff |Tr(A)|>2\]

    \item If $\Omega$ has $n$ elements and $T$ bijective and $A: \Omega \rightarrow SL_2(\RR)$. Then when is $(T,A)\in \uh$? 

    \item Show that if $f_n \rightarrow f$ converges uniformly and $f_n$ is continuous then $f$ is continuous.
\end{enumerate}

\newpage
\subsection{Non-Uniformly Hyperbolic}
\begin{definition}
    The \textbf{Lyapunov Exponent} is defined as
    \[L(A) := \lim_{n\rightarrow\infty} \frac{1}{n}\int_\Omega \log||A^n(\omega)||d\mu\]
\end{definition}
It turns out we have equivalent definitions of the Lyapunov exponent. Though we need to show that this limit even exists. This is where the hard work of learning Kingman's Ergodic theorem comes into play. This will be left as an exercise, but you can show that the terms inside the integral are subadditive. Then by applying Kindman's you can show that the infimum of the term of above is the same as the definition given above.

\begin{proposition}
    We have
    \begin{align*}
        L(A) &= \lim_{n\rightarrow\infty} \frac{1}{n}\int_\Omega \log||A^n(\omega)||d\mu\\
        &= \inf_{n\rightarrow\infty} \frac{1}{n}\int_\Omega \log||A^n(\omega)||d\mu\\
        &= \lim_{n\rightarrow\infty} \frac{1}{n}\log||A^n(\omega)|| \text{ for a.e. } \omega\in \Omega
    \end{align*}
\end{proposition}

Furthermore, the Lyapunov exponent takes values in the interval $[0,\infty)$.
\begin{theorem}
    (Johnson's Theorem) $\sigma(H_\omega) = \{E\in \RR: A_E\notin \uh\}$
\end{theorem}
\begin{proof}
    We will show that if $E\in \RR$ such that $A_E\in \U\H$, then $E\in \rho(H_\omega)$. Notice that this equivalently shows the $\subset$ containment. By Theorem 4.1.1 we know that there must exists exponential invariant directions. Thus, let us choose $\psi^s, \psi^u$ in $\Lambda^s, \Lambda^u$ respectively that satisfy the eigenvalue equation $H_\omega \psi^\bullet = E\psi^\bullet$ for $\bullet = \psi^s,\psi^u$. We can note that by definition we have
    \[\begin{pmatrix}
        \psi_0^\bullet\\
        \psi_{-1}^\bullet
    \end{pmatrix}\in \Lambda^\bullet(0)\]
    By normalizing we have
    \[\det\begin{pmatrix}
        \psi_0^s & \psi_0^u\\
        \psi_{-1}^s & \psi_{-1}^u
    \end{pmatrix} = 1\]
    By exercise 1 in section 3.4.1 we have 
    \[\det\begin{pmatrix}
        \psi_n^s & \psi_n^u\\
        \psi_{n-1}^s & \psi_{n-1}^u
        \end{pmatrix} = 
        \det \Big[A^n_E(0)\begin{pmatrix}
        \psi_0^s & \psi_0^u\\
        \psi_{-1}^s & \psi_{-1}^u
    \end{pmatrix}\Big] = 1\]
    Now let us define Green's function, which will play a key role in show $H_\omega - E$ is invertible.
    \begin{align*}
        G: \ZZ^2 &\rightarrow \RR\\
        (p,q) &\mapsto \begin{cases}
            \psi^u_p \cdot \psi^s_q & p\leq q\\
            \psi^u_q \cdot \psi^s_p & p>q
        \end{cases}
    \end{align*}
    Note that $G(p,q) = G(q,p)$. Now we need a Lemma to give an upper estimate of $G$.
    \begin{lemma}
        There exists uniform constants $C>0, \lambda>1$ in $(p,q)$ such that
        \[|G(p,q)| \leq \frac{C}{\gamma}\lambda^{-|p-q|}\]
        We remark that $\gamma$ is actually the same uniform bound of the distance between $\Lambda^s,\Lambda^u$ found in Theorem 4.1.1.
    \end{lemma}
    \noindent With this lemma let us define an operator 
    \begin{align*}
        S: \ell^2(\ZZ) &\rightarrow \ell^2(\ZZ)\\
        \psi_n &\mapsto \sum_{p\in \ZZ} G(n,p)\psi_p
    \end{align*}
    We want to show that $S$ is actually the inverse of $H_\omega - E$, but to do so we need to do a calculation involving $G$. We will fix $p$ and consider $G(p,\cdot)$ as a function in the second coordinate. Let us consider
    \begin{align*}
        [(H_\omega - E)G(p,\cdot)](n) &= G(p, n+1) + G(p, n-1) + [V_\omega(n) - E]G(p,n)\\
        &= \begin{cases}
            \psi^u_p \cdot \psi^s_{n+1} + \psi^u_p \cdot \psi^s_{n-1} + [V_\omega(n) - E]\psi^u_p \cdot \psi^s_n & n\geq p+1\\
            \psi^u_{n+1} \cdot \psi^s_p + \psi^u_{n-1} \cdot \psi^s_p + [V_\omega(n) - E]\psi^u_n \cdot \psi^s_p & n\leq p-1\\
            \psi^u_p \cdot \psi^s_{p+1} + \psi^u_{p-1} \cdot \psi^s_p + [V_\omega(n) - E]\psi^u_p \cdot \psi^s_p & n=p
        \end{cases} \\
        &= \begin{cases}
            \psi^u_p[\psi^s_{n+1} + \psi^s_{n-1} + [V_\omega(n) - E]\psi^s_n] & n\geq p+1\\
            \psi^s_p [\psi^u_{n+1}+ \psi^u_{n-1} + [V_\omega(n) - E]\psi^u_n] & n\leq p-1\\
            \psi^u_p \cdot \psi^s_{p+1} +  \psi^s_p[\psi^u_{p-1}+ [V_\omega(n) - E]\psi^u_p] & n=p
        \end{cases}\\
        &= \begin{cases}
            0 & n\neq p\\
            \psi^u_p \cdot \psi^s_{p+1} - \psi^s_p \cdot \psi^u_{p+1} &n=p
        \end{cases}\\
        &= \begin{cases}
            0 & n\neq p\\
            1 & n=p
        \end{cases}\\
        &= \delta_{pn}
    \end{align*}
    The first equality is definition of the $H_\omega - E$, the second is just using definition of $G$, the third is simplifying, the fourth uses two facts. Fact one is solutions of eigenvalue equation and the second is essentially exercise 1 in section 3.4.1. The last equality equality is by noticing the that the equation was the determinant. With this we can show that $S$ is indeed the inverse, so let $\phi\in \ell^2(\ZZ)$
    \begin{align*}
        [(H_\omega - E)S(\phi)](n) &= [S\phi](n+1) + [S\phi](n-1) + [V_\omega(n) - E][S\phi](n)\\
        &= \sum_{p\in \ZZ} [G(p, n+1) + G(p,n-1) + [V_\omega(n) - E] G(p,n)]\phi_p\\
        &= \sum_{p\in \ZZ} \delta_{pn} \phi_p\\
        &= \phi_n
    \end{align*}
    It remains to show that $S$ is bounded. Indeed, again let $\phi \in \ell^2(\ZZ)$ 
    \begin{align*}
        ||S\phi||^2_2 &= \sum_{n\in \ZZ} |[S\phi](n)|^2\\
        &= \sum_{n\in \ZZ} |\sum_{p\in \ZZ} G(n,p)\psi_p|^2\\
        &= \sum_{n\in \ZZ} |\sum_{p\in \ZZ} G(n,p)^{\frac{1}{2}}G(n,p)^{\frac{1}{2}}\psi_p|^2\\
        &\leq \sum_{n\in \ZZ} (\sum_{p\in \ZZ} |G(n,p)|^{\frac{1}{2}}|G(n,p)|^{\frac{1}{2}}|\psi_p|)^2\\
        &\leq \sum_{n\in \ZZ} (\sum_{p\in \ZZ} |G(n,p)|) (\sum_{p\in \ZZ}|G(n,p)||\psi_p|^2)\\
        &\leq \frac{C}{\gamma}\sum_{n\in \ZZ}\sum_{p\in \ZZ}|G(n,p)||\psi_p|^2\\
        &= \frac{C}{\gamma}\sum_{p\in \ZZ} \sum_{n\in \ZZ} |G(n,p)||\psi_p|^2\\
        &= \frac{C}{\gamma}\sum_{p\in \ZZ} |\psi_p|^2 \sum_{n\in \ZZ} |G(n,p)|\\
        &\leq (\frac{C}{\gamma})^2 \sum_{p\in \ZZ} |\psi_p|^2\\
        &=(\frac{C}{\gamma})^2 ||\psi||_2^2 < \infty
    \end{align*}
    Where we use the Lemma, Cauchy-Schwartz, and Fubini Theorem (We did not cover this but it is the formal reason we can swap the summations). This completes one containment. We remark that the reverse containment is much more complicated and hence we omit this part.
\end{proof}

\begin{definition}
    We denote \textbf{Non-uniformly hyperbolic} as $\nuh$. We say $(T,A)\in\nuh$ if $(T,A)\notin \uh$ and $L(A)>0$.
\end{definition}

A remark is that if $(T,A) \in \uh$, then $L(A)>0$. While the definition above is essentially asking about the converse. Of course, from the definition above it should be clear that the converse fails to hold. It turns out that to even see an example of such it takes a lot of work. This brings us to the final section and the main goal of the reading. An example of $\nuh$.

\subsubsection{Exercises}
\begin{enumerate}
    \item Use Kingman's theorem to show that $L(A)$ is defined.
\end{enumerate}

\newpage
\subsection{Polar Decomposition}
Unfortunately, we will not have time for this section. For those interested this can be read in Linear Algebra Done Right by Sheldon Axler. For a more general setting of polar decomposition you can read Reed and Simons book on functional analysis.

\newpage
\subsection{Doubling Map Example}
Before beginning we remark that all of this is work done by Zhenghe Zhang. We use his paper as a direct resource. Since we have an easily accessed resource I do not rewrite the proofs, but rather give ideas on what is going on in the proofs.

Let us define the set we are in. Consider the family of Schrodinger operators defined as
\begin{align*}
    H_{\lambda, v, x}: \ell^2(\ZZ) &\rightarrow \ell^2(\ZZ)\\
    \phi(n) &\mapsto \phi(n+1) + \phi(n-1) + \lambda v(T^nx) \phi(n)
\end{align*}
Where $\lambda \in \RR$ is called the \textbf{coupling constant}, $v: \RR/\ZZ \rightarrow \RR$ is a bounded measurable function called the \textbf{potential}. Furthermore, $T$ is doubling map on $\RR/\ZZ$. The goal is to study the eigenvalue equation $H\phi = E\phi$ for some \text{energies} $E\in \RR$. The study of the equation leads us to the relation to cocycle maps. For each $E$ we can define
\begin{align*}
    A^{E - \lambda v(x)}: \RR/\ZZ &\rightarrow SL_2(\RR)\\
    x &\mapsto \begin{pmatrix}
    E - \lambda v(x) & -1\\
    1 & 0
\end{pmatrix}
\end{align*}
This defines a cocycle map
\begin{align*}
    (T,A^{E - \lambda v(x)}): \RR/\ZZ \times \RR^2 &\rightarrow \RR/\ZZ \times \RR^2\\
    (\omega, \Vec{v}) &\mapsto (T\omega, A^{E - \lambda v(x)}(\omega) \Vec{v})
\end{align*}
Note that we are working on $\ell^2(\NN)$, so we only consider the right half-line. Therefore cocyle iterations are of the form
\[A^{E - \lambda v(x)}_n = \begin{cases}
    A(T^{n-1}\omega) \cdots A(T\omega) A(\omega) & \text{ when } n\geq 1\\
    I & \text{ when } n=0
\end{cases}\]
To see the relation of solutions to $H\phi = E\phi$ we remark the following
\[H\phi = E\phi \iff A^{E - \lambda v(x)}_n \begin{pmatrix}
    u_0 \\
    0
\end{pmatrix} = 
\begin{pmatrix}
    u_n\\
    u_{n-1}
\end{pmatrix} \text{ for all } n\in \NN\]

Before moving to the polar decomposition we remark that we can assume WLOG that $v(0) = 0$ and that $v(x) \xrightarrow{x\rightarrow 1_-} 1$. and $v'(x) >0$ as we want a monotone potential. Now letting $t = E/\lambda$, where $t\in \I = [-1, 2]$, we apply polar decomposition. Therefore the set up we truly work in is defined now 
\begin{align*}
    g: \RR/\ZZ \times \I &\rightarrow \RR\\
    (x,t) & \mapsto [t-v(x)]^2 + 1
\end{align*}
Where we remark that $c < g(x,t) < C$. 

\[A(x; t, \lambda) = \Lambda(2x) \cdot O(x) = \begin{pmatrix}
    \lambda \sqrt{g(2x,t)} & 0\\
    0 & (\lambda \sqrt{g(2x,t)})^{-1}
\end{pmatrix} R_{\theta(x,t)}\]
Where
\[R_{\theta(x,t)} = \begin{pmatrix}
    \cos\theta(x,t) & -\sin \theta(x,t)\\
    \sin \theta(x,t) & \cos\theta(x,t)
\end{pmatrix} \text{ with } \theta(x,t) = \cot^{-1}(t-v(x))\]

\begin{theorem}
    (Zhenghe, 2016) Given a potential $v\in C^1(0,1)$, $||v||_{C_1}$, and $\inf_{n\geq 1} > c$. Considering the Schrodinger cocycle (polar), then there exists a $C_0(v) > 0$ so that for all $\lambda > 0$ we have
    \[L(t; \lambda) > \log \lambda - C_0 \text{  for all } t\in \I\]
    Note that we consider the polar version, but by being a $SO_2(\RR)$ conjugacy away, then we have the result for the initial case too.
\end{theorem}


A key observation to make to prove the theorem is the following inequality
\[L(t; \lambda) \geq \lim_{n\rightarrow \infty} \frac{1}{n} \int_{\RR/\ZZ} \log||A_n(x)\Vec{e}_1|| dx \]
This observation tells us that it suffices to consider the orbit of $\Vec{e}_1$ and show that the limit is bounded below by the desired result. With this in mind we define two maps for $n\geq 1$.
\begin{align*}
    \phi_n: \RR/\ZZ &\rightarrow \RR & \theta_n: \RR/\ZZ &\rightarrow \RR\\
    x&\mapsto \cot^{-1}[\lambda^2g(T^nx)\cot\theta_{n-1}(x)] & x &\mapsto \phi_n(x) + \theta(T^nx)
\end{align*}
Where we let $\theta_0 = \theta$. The question that should be asked now is what are these maps? The maps help us track the orbit of $\Vec{e}_1$. First, notice that $\phi_1(\cdot)$ is the angle of $A(x;t,\lambda) \Vec{e}_1$, then $\theta_1$ take the angle $\phi_1$ and rotates it via $\theta(Tx)$, which is equivalent to the angle of the vector $R_{(Tx, t)} A(x;t,\lambda) \Vec{e}_1$. After seeing this it becomes easy to see that $\phi_n$ represents the angle after being hit via the hyperbolic matrix, while $\theta_n$ represents the angle after the rotation matrix. Where both are on the $n$-th step.

Now that we have defined the maps above it turns out the next key observation is to be able estimate how the derivatives of the maps behave. 

\begin{lemma}
    For all $t\in \I$ and for all $x\in \RR/\ZZ$ so that $\theta_n$ is differentiable we have
    \[\frac{\partial\theta_n}{\partial x} > c2^n\]
\end{lemma}

This in turn implies that $\theta_n$ is a piecewise monotone function. We of course have piecewise, since this function clearly has discontinuities due to the doubling map. In fact, we can count said discontinuities, which is actually the next result needed.

\begin{lemma}
    For each $t\in \I$ and $n\geq 0$, then $\theta_n$ is discontinuous at points, which are a subset of
    \[\D_n = \{\frac{j}{2^n}: 0\leq j \leq 2^n - 1\}\]
    Furthermore, if we define 
    \[\C_n = \{x\in \RR/\ZZ: \theta_n(x) \in \frac{\pi}{2} + \pi \ZZ\}\]
    then for all $t\in \I$
    \[Card(\C_n) \leq 2^{n+1} - 1\]
\end{lemma}

Both lemmas are proved via induction. The core idea of the proof for the first lemma is to consider two cases, where the first is when $x\in \RR/\ZZ$ satisfies the inequality
\[|g(T^nx) \cot\theta_{n-1}(x)| < C\lambda^{-3/2}\]
Then we consider the second case, which is when it fails to hold the inequality. The first case is assumed almost immediately in the inductive step. Besides these cases most of the proof is a direct calculation to estimate the derivative. For the second lemma, the first part is completely clear from the discontinuities of $\cot(x)$. As for the second the inductive step essentially computes an upper bound of the Lebesgue measure of the image on $\theta_n$. The key is to notice the relation
\[\phi_n(x) \in \pi\ZZ + \frac{\pi}{2} \iff \theta_{n-1} \in \pi\ZZ + \frac{\pi}{2}\]
From this we split $\RR/\ZZ$ into intervals, where each interval under $T^n$ becomes $\RR/\ZZ$. For an example, consider $I = [0,1/2)$, then $T(I) = \RR/\ZZ$ Then we approximate the Lebesgue measure of the image of these intervals, which makes it clear how to count the cardinality of $\C_n$.

\begin{corollary}
    Let $||\cdot||_{\RR\PP^1}$ denote the distance to closest element in $\pi \ZZ$. Define
    \[\B_n(\delta) = \{x\in \RR/\ZZ: ||\theta_n(x) - \frac{\pi}{2}||_{\RR\PP^1} < \delta \}\]
    Then $Leb(\B_n(\delta)) < C\delta$
\end{corollary}

This corollary follows immediately from the two lemmas, by notice that

With the theorem at hand we can now ask about the construction of a $\N\U\H$ cocycle. We know we have a family of Schrodinger Operators, so we let $\Sigma$ be the almost sure spectrum, which means that for a.e. $E\in \Sigma$ then $E\in \sigma(H_{\lambda, v,x})$ for any $(\lambda, v, x)$. By Johnson's theorem we know 
\[A^{E - \lambda v(x)} \in \U\H \iff E \notin \Sigma\] Therefore, picking an $E\in \Sigma$, which we note that $\Sigma \neq \varnothing$, then $(T,A^{E - \lambda v(x)})$ has positive Lyaponuv exponents, implying $(T,A^{E - \lambda v(x)})\in \N\U\H$.

\subsubsection{Exercises}
\begin{enumerate}
    \item How many connected components are there in $B_n(\delta)$ and why?

    \item Given the $\lambda>1$ and a hyperbolic matrix
    \[A = \begin{pmatrix}
        \lambda & 0\\
        0 &\lambda^{-1}
    \end{pmatrix}\]
    Then notice $A\in SL(2,\RR)$. Thus, we can induce $\tilde{A}\in PSL(2,\RR)$. What does the graph of $\Tilde{A}$ look like? Can you draw an explicit picture?
\end{enumerate}

\newpage
\section{References}
\begin{enumerate}
    \item One-Dimensional Ergodic Shr\"{o}dinger Operators by David Damanik and Jake Fillman
    \item Ergodic Theory by Einsiedler and Ward.
    \item Real Analysis by Folland 
    \item Uniform Positivity of the Lyapunov Exponent for Monotone Potentials Generated by the Doubling Map by Zhenghe Zhang
    \item Uniform Hyperbolicity and its Relation with Spectral Analysis of 1D Discrete Shr\"{o}dinger Operators by Zhenghe Zhang
\end{enumerate}

\end{document}


\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathabx}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{pslatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{epsf}
\usepackage{tikz}
\usepackage{framed}
\usepackage{hyperref}
\geometry{top=1in, bottom=1in, left=1cm, right=1cm}
%\pagestyle{empty}

\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Tomas Sbardelotto Dos Santos}
\rfoot{Page \thepage}


\def\Z{\mathbb Z}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\title{}
\date{}

\begin{document}
\begin{enumerate}
\item Suppose $\left(X, d_X\right),\left(Y, d_Y\right)$ are metric spaces and $f: X
\rightarrow Y$ is continuous. Let $A \subset X$.
\begin{enumerate}
\item Show that $f(\bar{A}) \subset \overline{f(A)}$.
\item Show that the subset can be proper.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
    
    Suppose \(y \in \overline{A}\). By definition of the closure, for every \( \epsilon > 0 \), there exists \( x \in A \) such that \( d_X(x, y) < \epsilon \). Consider a sequence \(\{x_n\}\) in \(A\) converging to \(y\). By the continuity of \(f\), the sequence \(\{f(x_n)\}\) converges to \(f(y)\) in \(Y\). Since each \(f(x_n) \in f(A)\), and \(\{f(x_n)\}\) converges to \(f(y)\), it follows that \(f(y) \in \overline{f(A)}\). Hence, every element of \(f(\overline{A})\) is in \(\overline{f(A)}\), proving that \(f(\overline{A}) \subset \overline{f(A)}\).

Suppose \(f\) is not injective, meaning there exist distinct elements \(x_1, x_2 \in X\) such that \(f(x_1) = f(x_2)\). Consider the sets \(A = \{x_1\}\) and \(\overline{A} = \{x_1\}\), as singletons are closed. Then \(f(A) = \{f(x_1)\}\) and \(f(\overline{A}) = \{f(x_1)\} = f(A)\). However, if \(x_2\) is not a limit point of \(A\) but maps to the same value, \(\overline{f(A)}\) could potentially include other limit points of sequences from \(f(A)\) that do not necessarily come from points in \(\overline{A}\).
    
\end{mdframed}

\item Suppose $f: X \rightarrow Y$ is continuous for metric spaces $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$. Show that if $X$ is connected, then $f(X)$ is
connected.
\begin{mdframed}
    \textbf{Solution:}\\

    Suppose $f$ is continuous, $X$ is connected, and $f(X)$ is not connected.\\

    Then there are $U,V \subset Y$ which are nonempty, disjoint open sets and 
    \[
    f(X) = f(X) \cap U\cup V
    \]
    By the properties of preimages of unions we have
    \[
    X = f^{-1}(f(X) \cap U\cup V) = X \cap f^{-1}(U) \cup f^{-1}(V)
    \]
    
    Since $f$ is continuous, the preimages of $U,V$ are also open, nonempty, and disjoint which means that $X$ is disconnected. A contradiction. Thus if $X$ is connected, $f(X)$ is connected.

    
\end{mdframed}

\item Prove the following version of the intermediate value theorem. Let $(X, d)$
be a connected metric space and $f: X \rightarrow \mathbb{R}$ a continuous
function. Suppose that there exist $x_0, x_1 \in X$ and $y \in \mathbb{R}$ such
that $f\left(x_0\right)<y<f\left(x_1\right)$. Then prove that there exists a $z \in
X$ such that $f(z)=y$. \\{\it Hint: See Exercise 2.}
\begin{mdframed}
    \textbf{Solution:}\\
    Assume for contradiction that no such \(z\) exists where \(f(z) = y\). Consider the sets:
\[
A = \{x \in X : f(x) < y\} \quad \text{and} \quad B = \{x \in X : f(x) > y\}
\]
Since \(f\) is continuous and \(f(x_0) < y\) while \(f(x_1) > y\), \(A\) and \(B\) are non-empty. Moreover, both \(A\) and \(B\) are open in \(X\). To see why, consider any \(x \in A\). Since \(f\) is continuous, there exists an \(\epsilon > 0\) such that for all \(x' \in X\) within \(\epsilon\)-distance from \(x\), \(f(x') < y\), implying \(x' \in A\). A similar argument holds for \(B\).

Given that \(A\) and \(B\) are both open, non-empty, and \(X = A \cup B\) (since there is no \(x \in X\) with \(f(x) = y\)), \(X\) is expressed as a union of two disjoint non-empty open sets. This contradicts the assumption that \(X\) is connected.

Therefore, the assumption that there is no \(z \in X\) such that \(f(z) = y\) must be false. Hence, there exists some \(z \in X\) such that \(f(z) = y\), which completes the proof.

    
\end{mdframed}

\item A continuous function $f: X \rightarrow Y$ for metric spaces $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$ is said to be proper if for every compact set
$K \subset Y$, the set $f^{-1}(K)$ is compact. Suppose a continuous $f:(0,1) \rightarrow(0,1)$ is proper and $\left\{x_n\right\}$ is a sequence in $(0,1)$ that
converges to 0 . Show that $\left\{f\left(x_n\right)\right\}$ has no subsequence
that converges in $(0,1)$.
\begin{mdframed}
    \textbf{Solution:}\\
    Assume for contradiction that there exists a subsequence \( \{f(x_{n_k})\} \) converging to some \( y \in (0,1) \). Let \( K = \{y\} \), which is a compact set in \( (0,1) \) because it is closed and bounded in the metric space \( (0,1) \).

Since \( f \) is proper, the preimage \( f^{-1}(K) \) must also be compact. Thus, the set \( f^{-1}(\{y\}) \) is compact in \( (0,1) \). Being compact in \( (0,1) \), it must be closed and bounded, and hence it must contain its limit points.

However, since \( \{x_{n_k}\} \) is a subsequence of \( \{x_n\} \) and \( \{x_n\} \) converges to \(0\), which is outside of \( (0,1) \), the sequence \( \{x_{n_k}\} \) cannot have limit points in \( (0,1) \). This contradicts the fact that \( f^{-1}(\{y\}) \) is compact, as it would imply \( f^{-1}(\{y\}) \) should contain the limit point \( 0 \), which it cannot since \( 0 \notin (0,1) \).

Therefore, the assumption that there exists a converging subsequence \( \{f(x_{n_k})\} \) must be incorrect. Hence, the sequence \( \{f(x_n)\} \) cannot have any convergent subsequences within \( (0,1) \).

    
\end{mdframed}

\item Take the metric space of continuous functions $C([0,1], \mathbb{R})$. Let $k:
[0,1] \times[0,1] \rightarrow \mathbb{R}$ be a continuous function. Given $f \in
C([0,1], \mathbb{R})$ define
$$
\varphi_f(x):=\int_0^1 k(x, y) f(y) d y .
$$
\begin{enumerate}
\item Show that $T(f):=\varphi_f$ defines a function $T: C([0,1], \mathbb{R}) \rightarrow C([0,1], \mathbb{R})$.
\item Show that $T$ is continuous.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
The map \( \varphi_f \) is continuous by the continuity of \( k \) and \( f \), and the linearity and boundedness of integration. For each \( x, x' \in [0,1] \):
\[
|\varphi_f(x) - \varphi_f(x')| \leq \int_0^1 |k(x, y) - k(x', y)| |f(y)| dy.
\]
Since \( k \) is uniformly continuous on a compact set and \( f \) is bounded, this integral shows \( \varphi_f \) is continuous, and thus, \( T(f) = \varphi_f \) defines a function from \( C([0,1], \mathbb{R}) \) to itself.

Assume \( \{f_n\} \) in \( C([0,1], \mathbb{R}) \) converges uniformly to \( f \). For every \( x \in [0,1] \):
\[
\begin{aligned}
|\varphi_{f_n}(x) - \varphi_f(x)| &= \left|\int_0^1 k(x, y) (f_n(y) - f(y)) dy\right| \\
&\leq \int_0^1 |k(x, y)| |f_n(y) - f(y)| dy \\
&\leq \sup_{y \in [0,1]} |f_n(y) - f(y)| \int_0^1 |k(x, y)| dy.
\end{aligned}
\]
Given \( k \) is bounded, \( T(f_n) \rightarrow T(f) \) uniformly, proving \( T \) is continuous.


    
\end{mdframed}

\item Let $(X, d)$ be a compact metric space, let $C(X, \mathbb{R})$ be the set of
real-valued continuous functions. Define
$$
d(f, g):=\|f-g\|_u:=\sup _{x \in X}|f(x)-g(x)| .
$$

\begin{enumerate}
\item Show that d makes $C(X, \mathbb{R})$ into a metric space.
\item Show that for any $x \in X$, the evaluation function $E_x: C(X, \mathbb{R}) \
rightarrow \mathbb{R}$ defined by $E_x(f):=f(x)$ is a continuous function.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
By definition, \( |f(x) - g(x)| \geq 0 \) for any \( x \in X \), so \( \sup_{x \in X} |f(x) - g(x)| \geq 0 \). If \( f = g \), then \( |f(x) - g(x)| = 0 \) for all \( x \), so \( d(f, g) = 0 \). Conversely, if \( d(f, g) = 0 \), then \( \sup_{x \in X} |f(x) - g(x)| = 0 \), implying \( |f(x) - g(x)| = 0 \) for all \( x \) and thus \( f = g \).

 \( d(f, g) = \sup_{x \in X} |f(x) - g(x)| = \sup_{x \in X} |g(x) - f(x)| = d(g, f) \).

 For any \( x \in X \),
\[
|f(x) - h(x)| \leq |f(x) - g(x)| + |g(x) - h(x)|,
\]
so taking supremums,
\[
d(f, h) = \sup_{x \in X} |f(x) - h(x)| \leq \sup_{x \in X} (|f(x) - g(x)| + |g(x) - h(x)|) \leq d(f, g) + d(g, h).
\]

Thus, \( d \) is a metric on \( C(X, \mathbb{R}) \).


Consider the evaluation function \( E_x: C(X, \mathbb{R}) \rightarrow \mathbb{R} \) defined by \( E_x(f) = f(x) \). To show \( E_x \) is continuous, consider a sequence \( \{f_n\} \) in \( C(X, \mathbb{R}) \) converging to \( f \) under \( d \), i.e., \( \|f_n - f\|_u \rightarrow 0 \).

By definition of \( d \), for any \( \epsilon > 0 \), there exists \( N \) such that for all \( n \geq N \), \( \sup_{x \in X} |f_n(x) - f(x)| < \epsilon \). Thus, \( |f_n(x) - f(x)| < \epsilon \) for all \( x \) and particularly for the fixed \( x \), showing \( E_x(f_n) \rightarrow E_x(f) \) and hence \( E_x \) is continuous.

    
\end{mdframed}

\item Let $(X, d)$ be a metric space, $S \subset X$, and $p \in X$. Prove that $p$
is a cluster point of $S$ if and only if $p \in \overline{S \backslash\{p\}}$.
\begin{mdframed}
    \textbf{Solution:}\\

\textbf{$\implies$} Suppose \(p\) is a cluster point of \(S\). By definition, for every \(\epsilon > 0\), there exists some \(x \in S\) with \(x \neq p\) such that \(d(p, x) < \epsilon\). This implies that each neighborhood of \(p\) contains points from \(S\) other than \(p\) itself, namely from \(S \setminus \{p\}\). Hence, \(p\) is an accumulation point of \(S \setminus \{p\}\) and thus \(p \in \overline{S \setminus \{p\}}\).

\textbf{$\impliedby$} Now assume \(p \in \overline{S \setminus \{p\}}\). This means that every neighborhood of \(p\) contains at least one point from \(S \setminus \{p\}\). Specifically, for every \(\epsilon > 0\), there exists \(x \in S \setminus \{p\}\) such that \(d(p, x) < \epsilon\). Since these \(x\) are distinct from \(p\) and belong to \(S\), \(p\) is a cluster point of \(S\).

Thus, \(p\) being a cluster point of \(S\) is equivalent to \(p \in \overline{S \setminus \{p\}}\).

    
\end{mdframed}

\item Let $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$ be metric spaces, $S \subset X, p \in X$ a cluster point of $S$, and let $f: S \rightarrow Y$ be a
function. Prove that $f: S \rightarrow Y$ is continuous at $p$ if and only if
$$
\lim _{x \rightarrow p} f(x)=f(p) .
$$
\begin{mdframed}
    \textbf{Solution:}\\


\textbf{$\implies$} Assume that \(f\) is continuous at \(p\). By definition, this means for every \(\epsilon > 0\), there exists a \(\delta > 0\) such that for all \(x \in S\) with \(d_X(x, p) < \delta\), we have \(d_Y(f(x), f(p)) < \epsilon\). Since \(p\) is a cluster point of \(S\), for every \(\delta > 0\), there exists \(x \in S\) such that \(x \neq p\) and \(d_X(x, p) < \delta\). Therefore, \(f(x)\) gets arbitrarily close to \(f(p)\) as \(x\) approaches \(p\) from within \(S\). This shows that \(\lim_{x \to p} f(x) = f(p)\).

\textbf{$\impliedby$} Now assume that \(\lim_{x \to p} f(x) = f(p)\). This means that for every \(\epsilon > 0\), there exists a \(\delta > 0\) such that for all \(x \in S\) with \(d_X(x, p) < \delta\), \(d_Y(f(x), f(p)) < \epsilon\). This definition directly matches the definition of continuity at \(p\). Thus, \(f\) is continuous at \(p\).
    
    
\end{mdframed}

\end{enumerate}




\end{document}

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}



\doublespacing

%% Commands that may be useful

\newcommand{\dudx}{\frac{\partial u}{\partial x}}
\newcommand{\dudy}{\frac{\partial u}{\partial y}}


%% EQUATIONS
\def\bernoulli{\dydx + f(x)y = g(x)y^n}
\def\exact{\int A(x) dx + B(x) dy =0}
\def\dydx{\frac{dy}{dx}}
\def\d2ydx2{\frac{d^2y}{dx^2}}
\def\dudx2{\frac{d^2u}{dx^2}}
\def\dudx{\frac{du}{dx}}
\def\intab{\int_{a}^{b}}
\def\hline{\hrule\bigskip}
\def\sumni{\sum_{n=1}^\infty}
\def\fourier2{\frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{L})} + b_n \sin{(\frac{2\pi nx}{L})}\right]}
\def\superint{\int_{-\infty}^{\infty}}
\def\zuperint{\int_{0}^{\infty}}
\def\lap{\mathscr{L}}
\def\lapinv{\mathscr{L}^{-1}}
\def\itint#1#2{\int_{#1}^{#2}}
\def\integratingfac{e^{-\itint{}{x}Pdx'}}
\def\quadform#1#2#3#4{\frac{-#1 \pm \sqrt{#2-#3}}{#4}}


\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}



\title{Book}
\author{Bipin Desai }
\date{}

\begin{document}

\maketitle
\pagebreak




\section{Table of Contents}
\begin{enumerate}
    \item \textbf{Basics in Mathematical Methods}
    \begin{enumerate}[label=1.\arabic*]
    \item Numbers: Real and Complex
    \item Differentiation
    \item The Chain Rule
    \item Taylor's Series
    \item Infinite Series Expansion
    \item Exponential Function
    \item Partial Derivatives
    \item Some Basic Functions
    \item Coordinate Systems
    \item Functions of More Than One Variable
    \item Vectors
    \item Integration
    \item Integration by Parts
    \item Multiple Integrals
    \item Some Important Integrals
    \item Basic Physics Equations
    
\end{enumerate}

\item \textbf{Dirac's Delta Function}
    \begin{enumerate}[label=2.{\arabic*}]
        \item Matrices and Determinants
        \item Algebraic Equations
        \item Matrix Algebra
        \item Hermitian And Unitary Matrices
        \item Determinants
        \item Properties of Determinants
        \item Coordinate Transformations
        \item Diagonalization of Matrices
        \item Eigenvalues and Eigenfunctions
        \item Matrix Inversion
        \item Functions of Matrices
        \item Matrix Commutation Relation
        \item Dirac's Bra and Ket Vectors
\end{enumerate}


\item \textbf{Ordinary Differential Equations}
    \begin{enumerate}[label=3.{\arabic*}]
        \item Meaning of Differential Equations
        \item Power Series Solutions
        \item Equations with Constant Coefficients
        \item Simply Solvable Equations
        \item Separable Equations
        \item Integrating Factors
        \item Integration by Parts
        \item Special Equations
        \item Linear Differential Equations
        \item Harmonic Oscillators
        \item Eigenvalue Equations
        \item Boundary Value Problems
    \end{enumerate}


\item \textbf{Other?}
    \begin{enumerate}[label=4.{\arabic*}]
    \item Differentiation
    \item Integration
    \item Taylor's Series
    \item Infinite Series Expansion
    \item Basic Differential Equations and Power Series Solutions
    \item Ordinary Linear Differential Equations: First Order
        \begin{enumerate}
            \item Simply Solvable
            \item Separable Equations
            \item Integrating Factors
            \item Homogeneous Equations
            \item Special Equations
        \end{enumerate}
    \item Ordinary Differential Equations: Second Order
        \begin{enumerate}
            \item Homogeneous Equations
            \item Equations with Constant Coefficients
        \end{enumerate}
    \item Sturm-Louisville Equations, Eigenvalue Problems, Orthogonal Functions
    \item Complex Variables
    \item Fourier Transforms
    \item Laplace Transforms
    \item Solving Differential Equations Through Integral Transforms
    \item Green's Function
    \item Solutions of Differential Equations Through Green's Function
    \item integral Equations
    \item Partial Differential Equations
        \begin{enumerate}
            \item Group Theory
            \item Lie Groups
            \item Infinitesimal Transformations
            \item Calculus of Variations Euler-Lagrange Equations
        \end{enumerate}
    \item Probability and Statistics
    \item Elements of Complex Dynamical Systems
        \begin{enumerate}
            \item Logistic Map
            \item Stability and Bifurcation
            \item Limit Cycles
            \item Basic Non-linear Equations
            \item Chaos Theory
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
\pagebreak










%% Beginning Math


\section{Differential Equations}
\subsection{Differential Equations 2}

\bigskip
M-W
Check p. 2, 3, 4, 5, 6 on separable... Bernoulli and Clairaut


$\int A(x) dx + B(x) dy =0$ are separable 
$\int A(x) dx + B(x) dy =c$


P.2



\textbf{\emph{Consider:}}
\begin{center}

\begin{equation}\label{exact}
A(x)dx + B(x)dy = 0
\end{equation}

    
\end{center}

This Equation is exact if: 
    \begin{center}
        $\frac{\partial A}{\partial y} = \frac{\partial B}{\partial x}$
    \end{center}




\textbf{\emph{Proof:}}


For an equation to be exact we must have:


\begin{center}

$du =  \dudx dx + \dudy dy$

\end{center}


But $du$ can be written as:

\begin{center}

    $du = \dudx + \dudy  dy $
    
    \vspace{0.5 cm}
    
    $ \implies  A =  \dudx \text{ and }  B =  \dudy $ 
    
    \vspace{0.5 cm}

    $\implies$ $\frac{\partial A}{\partial y} = \frac{\partial^{2} u}{\partial y \partial x} = \frac{\partial B}{\partial x}$

\end{center}
\vspace{0.25 cm}


\begin{equation}
    \frac{dy}{dx} = a \implies y = ax + b
\end{equation}

\begin{equation}
    \frac{dy}{dx} = ax + b \implies y = \frac{ax^2}{2} + bx + c
\end{equation}

\begin{equation}
    \frac{d^2y}{dx^2} = a \implies \frac{d}{dx}(\frac{dy}{dx}) = a 
    \implies let u = \frac{dy}{dx} \implies \frac{du}{dx} = a \implies u = ax + b \implies y = \frac{ax^2}{2} + bx + c
\end{equation}

Every Equation of the form \ref{exact} can be made exact through inequality from P.32 M-M. \newline
A linear equation can be written in the form:
\begin{equation}\label{linear1}
    \frac{dy}{dx} + f(x)y = g(x)
\end{equation}

\begin{center}
  $  \frac{dy}{dx} = au \implies \frac{du}{u} = u dx \implies ln (u) = ax + b $
  
  \vspace{0.25 cm}
  
 $ \implies u = e^{ax + b}$ $\implies u = ce^{ax}$
  
   $ \frac{d^2u}{dx^2} = a^2u \implies \frac{d^2u}{dx^2} - a^2u = 0 $ \\
  \vspace{0.25 cm} 
  
   $\implies (\frac{d}{dx} - a)(\frac{d}{dx} + a )u = 0 = (\frac{d}{dx} + a)(\frac{d}{dx} - a )u = 0 $ 
  \vspace{0.25 cm}  
   \\ 
   $ \implies (\frac{d}{dx} + a )u = 0 $ and $ (\frac{d}{dx} - a)u = 0  $
   
    \begin{equation}\label{realaux}
      u =  c_1e^{-ax} + c_2e^{ax}
   \end{equation}
   
\end{center}


\vspace{0.25 cm} One can do the above writing as: 
    
    \begin{equation}\label{doper}
        D = \frac{d}{dx} \implies (D^2 - a^2) u = (D-a)(D+a)u = 0 
    \end{equation}
    
\vspace{0.25 cm}(discuss auxiliary equation) where $a$ becomes the exponent of the exponential:
\begin{center}
    

 $\frac{d^2x}{dx^2} = -a^2u \implies (D^2 - a^2) u = (D-ia)(D+ia)u = 0 \implies$
 
 \end{center}

\begin{equation}\label{compaux}
        u = c_1e^{-iax} + c_2e^{iax} 
\end{equation}


\begin{center}
    \begin{equation}\label{auxequation}
        a\frac{d^2u}{dx^2} + b\frac{du}{dx} + c = 0 
    \end{equation}
    \newline
    $\implies aD^2 + bD + c = 0 $ 
    $\implies  D = \frac{-b\pm\sqrt{b^2-4ac}}{2a} = (a_1,a_2) $
    
    \begin{equation}\label{auxrealsol}
        u = c_1e^{-a_1x}+c_2e^{a_2x}
    \end{equation}
    
\end{center}

Differential equations with constant coefficients should be linear:

\begin{equation}
    a_n\frac{d^nu}{dx^n} + a_{n-1}\frac{d^{n-1}u}{dx^{n-1}} + \cdots = 0 \implies a_nD^n + a_{n-1}D^{n-1} + \cdots = 0
\end{equation}

Find the roots.

\begin{equation}
    \frac{dy}{y} + (\frac{1}{x} - \frac{x}{y})dx= 0
\end{equation}

Is not exact...

\begin{gather*}
    \frac{dy}{dx} + (\frac{y}{x} - x) = 0 
    \\ \vspace{0.2 cm}
    \frac{dy}{dx} + (\frac{1}{x})y = x
    \\ \vspace{0.2 cm}
    f(x) = \frac{1}{x} \textbf{ and } g(x) = x
    \\
    F = \int \frac{1}{x} = ln(x)
    \\
    y = e^{-ln(x)}\int e^{ln(x)}x + c
\end{gather*}

\subsection{Homogeneous Solution Power Series}

\bigskip

\begin{center}
    $\frac{du}{dx} = au \implies u = Ce^{ax}$
    \\
    $\frac{du}{dx} - au = f(x)$
\end{center}

    Let $u = e^{ax}w(x)$
\begin{gather}
    \implies \frac{du}{dx} = ae^{ax}w + e^{ax}\frac{dw}{dx} = au + e^{ax}\frac{dw}{dx}
\end{gather}
    
    From the above equation:

\begin{gather}
    e^{ax}\frac{dw}{dx} = f(x) \implies \frac{dw}{dx} = e^{-ax}f(x) \implies w(x) = \int e^{-ax'}dx'f(x') + C
\end{gather}

Hence Solution:

\begin{gather}\label{solutionintegratingfactor}
    u(x) = e^{ax}[\int e^{-ax'}dx'f(x') + C]
\end{gather}

\begin{gather}
    \frac{du}{dx} = f(x)u \implies \frac{du}{u} = \int dxf(x) + b \implies u = Ce^{\int dxf(x)}
\end{gather}


These are linear equations with constant coefficients: homogeneous solution plus particular solution

\begin{equation}\label{nonhomo}
      a_n\frac{d^nu}{dx^n} + a_{n-1}\frac{d^{n-1}u}{dx^{n-1}} + \cdots = f(x) \implies a_nD^n + a_{n-1}D^{n-1} + \cdots = f(x)
\end{equation}


\begin{gather}\label{linear2}
    \dydx + P(x)y - Q(x)
\end{gather}

i.e.
\begin{gather}
    dy + P(x)ydx = Q(x)dx
\end{gather}

Integrating factors can turn equations into exact equations:
\newline Let $\lambda(x)$ be the integrating factor.

\begin{gather}
    \lambda(x)[dy + P(x)ydx] = \lambda(x) Q(x)dx
    \\
    \implies \lambda(x)dy + \lambda(x)[P(x)y-Q(x)]dx = 0
    \\
    \implies A(x,y) = \lambda(x)[P(x)y-Q(x)]; B(x,y) = \lambda(x)
\end{gather}

The criterion for this to be exact is:

\begin{gather}
    \frac{\partial A}{\partial y} = \lambda(x)P(x) = \frac{\partial B}{\partial x} = \frac{d\lambda(x)}{dx} 
    \\
    \implies \frac{d\lambda(x)}{dx} = \lambda(x)P(x)
\end{gather}

This will determine the expression for $\lambda(x)$. Therefore, $\lambda(x) = e^{\int P(x) dx}$. (Note that there will not be a constant factor multiplying since it factors out of the equation and it immaterial.)
\\
\begin{center}
  Let $ w(x) = y\lambda(x) $ since $d\lambda(x)P(x)dx$
\end{center}

We have:

\begin{center}
    $dw = \lambda(x)dy + \lambda(x)dx$
\end{center}

The RHS above is the LHS from \ref{linear2}.

Hence from \ref{linear2} we have: 
\begin{center}
    $dw = \lambda(x)Q(x)dx = [e^{\int P(x')dx'}]Qdx$
\end{center}

Therefore:
\begin{gather}
    y = [e^{-\int dxP(x)}]{[\int e^{dx'P(x')}]Qdx + C}
\end{gather}





%%Interesting Physics Equations




\pagebreak
\subsection{Some Interesting and Simple Physics Equations:}





%% HEAT FLOW

\begin{mdframed}
\textbf{P.34 Heat Flow}
\begin{gather}\label{heatflow}
    R = - kA\frac{dT}{dx}
\end{gather}

\emph{\textbf{Solution:}}

\begin{gather*}
    T_1-T = \frac{Rx}{kA}
\end{gather*}

If the heat source at the center of a sphere:

\begin{gather*}
    A=4\pi r^2
    \\
    \implies T = \frac{1}{4\pi k} \frac{R}{r} + C
\end{gather*}

\textbf{P.36 Homogeneous Gas Reactions}
\begin{gather}
    \frac{dx}{dt} = k(a-x)(b-x)
\end{gather}

\emph{\textbf{Solution:}}

\begin{gather}
    k = \frac{1}{t(a-b)} ln \frac{b(a-x)}{a(b-x)}
\end{gather}

\textbf{P.38 Clausius-Clapeyron Equation}

\begin{gather}
    \frac{dP}{dT} = \frac{lP}{RT^2}
\end{gather}

\textbf{\emph{Solution:}}

\begin{gather}
    \frac{dP}{dT} = Ce^{-\frac{l}{RT}}
\end{gather}
\end{mdframed}








%%Helholz Equation

\begin{mdframed}
\textbf{P.39 Helmholz Equation}
\begin{gather}
    L\frac{dI}{dT} + RI = E \text{  } ; (I = 0 \text{ at } t=0)
\end{gather}
\emph{\textbf{Solution:}}
\begin{gather}
    I=\frac{E}{R}(1-e^{-\frac{Rt}{L}})
\end{gather}

\end{mdframed}



%%Radioactive Decay

\begin{mdframed}

\textbf{P.33, 43 Radioactive Decay}
\\
P.33:
\begin{gather}
    \frac{dN}{dt} = -\lambda N
\end{gather}
\emph{\textbf{Solution:}}
\begin{gather}
    N = N_0 e^{-\lambda t}
\end{gather}
P.43:
\begin{gather}
\frac{dA}{dt} = -\lambda_A A \text{ and } \frac{dB}{dt} = -\lambda_B B + \lambda_A A
\end{gather}
\emph{\textbf{Solution:}}
\begin{center}
    Therefore, $A = A_0e^{-\lambda_At}$ ; $\frac{dB}{dt}=-\lambda_B B + \lambda_A A_0 e^{-\lambda_A t}$
    I.E. $\frac{dB}{dt}+\lambda_B B=\lambda_A A_0 e^{-\lambda_A t}$
\end{center}
\begin{gather}
    \implies B = e^{-\lambda_B t}[\int \lambda_A A_0 e^{(\lambda_B t-\lambda_A)t}dt + C]
\end{gather}
If $B(0)=0$, then:
\begin{gather}
    B = \frac{\lambda_A}{\lambda_A-\lambda_B}A_0[e^{-\lambda_A t} -e^{-\lambda_B t}]
\end{gather}
\end{mdframed}





%%Equations reducible to linear form
\begin{mdframed}
\textbf{P.44 Equations Reducible to Linear Form:}
\begin{equation}\label{Bernoulli}
    \dydx + f(x)y = g(x)y^n
\end{equation}
\textbf{\emph{Solution:}}
\begin{align}\bernoullisol
   \text{Let }  y = u^{1/(1-n)}\\
    \frac{dy}{dx} = \frac{1}{(1-n)}u^{n/(1-n)}\frac{du}{dx}
\end{align}
Therefore:
\begin{align}
    \frac{1}{(1-n)}u^{n/(1-n)}\frac{du}{dx} + f(x)u^{1/(1-n)} = g(x)u^{n/(1-n)}
    \\ 
    \frac{du}{dx} + (1-n)f(x)u = (1-n)g(x)
\end{align}
Note that we could have also proceeded by multiplying both sides of \ref{Bernoulli} by $y^{-n}$. \\We would then get:
\begin{align}
    y^{-n}\dydx + y^{1-n}f(x) = g(x)
\end{align}
Then going forward with the substitutions etc. Or start with $y^{1-n}=u$
\end{mdframed}





%%HOMOGENEOUS EQUATION

\begin{mdframed}
\textbf{P.45 Homogeneous Equation}
A Homogeneous equation is given by:
\begin{align*}
    A(x)dx + B(x)dy = 0 \text{ }\eqref{exact}
\end{align*}
Where $A$ and $B$ are homogeneous functions of the same degree. \\ I.E. :
\begin{align}
    A(tx,ty) = t^{\alpha}A(x,y) \text{ and } B(tx,ty) = t^{\alpha}B(x,y)  
\end{align}
Then Let $y = vx$
\begin{gather*}
    \implies A(x,y)=A(x,vx)=A(1\cdot x, v\cdot x)\\
    =x^{\alpha} A(1,v) \text{ ; } B(x,y)=x^{\aplha} B(1,v) x^{\alpha} A(1,v)dx + x^{\aplha} B(1,v)(xdv,vdx) = 0\\
    \implies (\frac{A(1,v)}{B(1,v)}+v)dx+xdy=0\\
    \implies (\frac{A(1,v)}{B(1,v)}+v)d+x\frac{dy}{dx}=0\\
    \implies (-f(v)+v)+x\frac{dv}{dx}=0
\end{gather*}
Where $f(v)=-\frac{A(1,v)}{B(1,v)}$\\Therefore:
\begin{gather*}
    \frac{dv}{(f(v)-v)}=\frac{dx}{x} \implies \text{ Separable.}
\end{gather*}
    Note: one could say $v=\frac{y}{x}$ is a dimensionless variable.
\end{mdframed}    








%%Clairaut's Equation

\begin{mdframed}
\textbf{P.47 Clairaut's Equation}
\begin{equation}\label{clairautequation}
    y=x\dydx + {(\dydx)}^2
\end{equation}
\textbf{\emph{Solution:}}\\
Differentiate both sides:
\begin{gather*}
    \implies \dydx = \dydx + x\frac{d^2y}{dx^2} + 2(\dydx)\frac{d^2y}{dx^2}
\end{gather*}
Therefore:
\begin{gather*}
    (x + 2(\dydx))\d2ydx2 = 0
\end{gather*}
Two Solutions:
\begin{gather*}
    \d2ydx2 = 0 \implies y=cx+c_1\\
    \implies \text{ This is a first order equation and can have only one arbitrary constant.}
\end{gather*}
Putting back in the original equation gives:
\begin{gather*}
    cx+c_1=x(c)+(c)^2 \implies c_1=c^2
\end{gather*}
To satisfy the equation due to the vanishing of the first factor in \ref{clairautequation}:
\begin{gather*}
    x+2(\dydx) = 0 \implies \dydx = -\frac{x}{2}\\
    \implies y=-\frac{x^2}{4} + a
\end{gather*}
However, substituting $\dydx = -\frac{x}{2}$ into the original equation gives:
\begin{gather*}
    y = x()-\frac{x}{2} + {(-\frac{x}{2})}^2 = --\frac{x^2}{4}\\
    \implies a=0
\end{gather*}                                   
\end{mdframed}





%%Singular Solutions

\begin{mdframed}
\textbf{P.47 Singular Solutions}
\\  The above solution $y= -\frac{x^2}{4}$ is not derivable from the original solution by fixing the constant $c$. It is called a singular solution. Equations of the form $y=x\dydx + f(\dydx)$ are called Clairaut's equations.
Their solution is:
\begin{equation}\label{clairautsolution}
    y=cx+f(c)
\end{equation}
The general solutions is:
\begin{equation}\label{clairautgeneral}
    y=x(y')+f(y')
\end{equation}
To obtain the solution, differentiate both sides:
\begin{gather*}
    y'=y'+xy''+\frac{df(y')}{dx}\\
    \text{  where:    } \frac{df(y')}{dx}=\frac{df(y')}{dy'}\frac{dy'}{dx}=\frac{df(y')}{dy'}y''=f'(y')y''
\end{gather*}
Therefore,
\begin{gather*}
    y''(f'(y')+x) = 0 \implies \text{  One solution is:  } y''=0 \implies y=ax+b
\end{gather*}
Substituting into equation \ref{clairautequation} we get:
\begin{gather*}
    (ax+b)=x(a)+f(a) \implies b=f(a)
\end{gather*}
Therefore, $y=ax+f(a)$ is the general solution. However, the other solution is $(f'(y')+x) =0$ which when reversed gives $y'=\phi(x)$. Putting this back in the original equation \ref{clairautequation} we get:
\begin{gather*}
    y=x\phi(x)+f(\phi(x))
\end{gather*}
I.E. $y=x\phi(x)+g(x)$ which is another solution which has no arbitrary constants and hence can not be obtained by adjusting the constant $a$ in the general solution. It is the envelope formed by the family of general solutions obtained by varying the constant $a$. (See P.63 Morris and Brown)\\
It has been called a singular solution though this could be misleading as pointed out by Tenenbaum and Pollard (p.757).
\begin{gather*}
    y=xy' + ln(y')\\
    f(y')=ln(y')\\
\end{gather*}
\emph{\textbf{Solution:}}
\begin{gather*}
    y=cx+ln(c)\\
    x=-f'(y')=-\frac{1}{y'} \implies y'=-\frac{1}{x}
\end{gather*}
Putting back in \ref{clairautequation}:
\begin{gather*}
    y=-1+ln(-\frac{1}{x}) \text{  is the other solution.}
\end{gather*}

\end{mdframed}



%%Homogeneous Solution + Complementary Function





%% Double check this section...
%% MORE NEEDS TO BE WRITTEN (PAGE STARTING WITH BROWN)....)





\begin{mdframed}
\textbf{P.53 Homogeneous Solution + Complementary Function + Dynamical Systems}\\
Linear Harmonic Oscillator\\
P.51 Damped Oscillation\\
P.52 Forced Oscillation\\
P.58 Cable Hanging\\
P.258 Normal Modes Golstein\\
P.547 Longitudinal Waves in Elastic Rod Golstein (Transition to continuous system)\\
P.165 Normal Modes Boas\\
Ch.25 N Oscillator Bipin\\
Calculus of Variations (See M-M and M-W) Variational Principal\\
P.267 Sturm-Louiville\\
M-W\\
P.2 Separable\\
switching of eigenstates ch.13 p.254 Bipin (appropriate for Ch. on matrices)\\
neutrino oscillation ch.15 p.294 Bipin (appropriate for ch. on matrices)\\
infinites in physic Bipin (Casimir effect, Bernoulli's no., renormalization)
Bipin\\
Ch.25 Lagrangian non-rel\\
ch.41 Lagrangian rel. p.890\\
ch.37 Casimir effect sec. 37.14\\
ch.38 Phonons sec 38.3

\end{mdframed}


\begin{mdframed}
\subsection{Ordinary Differential Equations (Including linear, non-linear)}
\begin{enumerate}
    \item Separable ($\frac{d^3y}{dx^3}=$ Third Order)
    \item Power Series
    \item Integrating Factor
    \item Bernoulli Equation ($\bernoulli$)
\end{enumerate}
\end{mdframed}

\begin{mdframed}
\textbf{Intercepts in 3-D}\\
\begin{underline} Hildebrand \end{underline}\\
\begin{underline}
A-W
\end{underline}
Ordinary = only one variable? ODE\\
Partial = more than one variable. PE\\
Partial = Elliptic, Hyperbolic, Parabolic
\begin{underline}P. 543\end{underline}\\
B.C. Cauchy, Dirichelet, Neuamann, Separable. 
\begin{gather*}
    \dydx + P(x)y = f(x) 
\end{gather*}

\end{mdframed}




%% BASIC MATH P. 208
%% LINEAR DIFFERENTIAL EQUATIONS WITH CONSTANT COEFFICIENTS

\begin{mdframed}
\textbf{231}\\
\textbf{Linear Differential Equation with Constant Coefficients}\\
\begin{gather*}
    \frac{d^2u}{dx^2} = 0\\
    u = c_2x+c_2\\
    u(0) = c_2 \text{  and  } u'(0) = c_2\\
\end{gather*}
\begin{gather*}
    \frac{d^2u}{dx^2} = f(x)\\
    f(x) = ax^n \text{ , } f(x) = \theta(x)
\end{gather*}
\begin{gather*}
    \frac{du}{dx} = \int_{0}^{x} dx' f(x') +c_1\\
    u=\int_{0}^{x}dx''\int_{0}{x''}f(x') + c_1x + c_2
\end{gather*}
\begin{gather*}
    \frac{d^2u}{dx^2} = Au\\
    u = ce^{\alpha x} \text{ , } u' = \aplha e^{\alpha x} = \alpha u
\end{gather*}
\underline{$A>0$} 
\begin{gather*}
    u'' = \alpha u' = \alpha^2u\\
    \alpha^2=A \therefore \alpha = \pm \sqrt{A}\\
    u = ce^{+\sqrt{A}x}+De^{-\sqrt{A}x}\\
\end{gather*}
\begin{gather*}
    \frac{d^2u}{dx^2} + B\dudx = Au\\
    u = e^{\aplha x}\\
    \aplha^2 + B\aplha = A\\
    \alpha^2+B\alpha - A = 0\\
    \alpha = \frac{-B \pm \sqrt{B^2+4A}}{2}\\
    \dudx = f(x)u \text{ , } u = ce^{\int_{0}^{x}f(x)dx'}
\end{gather*}
\end{mdframed}
%% NOTE NOT A CONSTANT COEFFICIENT
\begin{mdframed}
\begin{gather*}
    \dudx + P(x)u = Q(x)
\end{gather*}
Note: Not a constant coefficent!
\begin{gather*}
    u(x) = \beta (x) e^{\alpha x}\\
    u' = \beta e^\alpha \alpha' + \beta e^\alpha\\
    \beta e^\alpha \alpha' + \beta'e^\alpha + P\beta e^\alpha = Q\\
    \alpha' = -P \text{ , } \beta'e^\alpha = Q\\
    \alpha(x) = - \int_{0}^{x}dx'P(x')\\
    \beta' = Qe^{-\alpha} \therefore \beta = \int_{0}^{x}dx''Q(x'')e^{-\alpha(x'')} + c_2\\
    u(x) = \beta(x) e^{\alpha (x)} = [\int_{0}^{x}dx''Q(x'')e^{+\int_{0}^{x''}dx'P(x')}+c_2]e^{-\int_{0}^{x''}dx'P(x')}
\end{gather*}
Check $P(x) = 0$ i.e. $u'=Q$
\begin{gather*}
    u(x) = \int_{0}^{x}dx''Q(x'') +c_2\\
    u' = Q
\end{gather*}
\end{mdframed}

%%Linear Differential equations with constant coeff???????


\begin{mdframed}
Linear Differential Equation with Constant Coefficient
\begin{gather*}
    a_n \frac{d^ny}{dx^n} + \cdots + a_0y = 0
\end{gather*}
$y=e^{nx}$ is a solution...
\begin{gather*}
    \dydx = 0\\
    a_nm^n + a_{n-1}m^{n-1}+ \cdots + a_0 = 0\\
    m_1=m_2 \text{ ,  } \frac{d}{dx} e^{\lambda x} = xe^{\lambda x}\\
    (D-\lambda)[\dydx - \lambda y]=0\\
    (\frac{d^2}{dx^2}-2\lambda\frac{d}{dx}+\lambda^2)xe^{\lambda x}\\
    (\frac{d}{dx}-\lambda)e^{\lambda x} = 0\\
    e^{\lambda_1x} \text{ , } e^{\lambda_2 x}\\
    \dydx = \lambda y = 0\\
    \frac{df}{dx} = 0 \text{ ,  } f= c\\
    (\frac{d^2}{dx^2}-2\lambda\frac{d}{dx}+\lambda^2)f(x)e^{\lambda x}= 0
\end{gather*}

Possible Solutions:

\begin{gather*}
    \frac{df^2}{dx^2} = 0 \text{ , } f = ax + b\\
    \begin{cases}
    a = 0 , f = b\\
    b = 0 , f = ax
    \end{cases}
\end{gather*}
Example:
\begin{gather*}
    \frac{d^2y}{dx^2} + 3\dydx + 2y = e^{-x}
\end{gather*}
\begin{gather*}
    m^2 + 3m + 2 = 0\\
    m = -1, -2\\
    y_c = c_1e^{-x} + c_2e^{-2x}
\end{gather*}
\begin{gather*}
    y_p = Ae^{-x}\\
   \to y_p'=-Ae^{-x}\\
   \to y_p'' = Ae^{-x}\\
   Ae^{-x} - 3Ae^{-x} + 2Ae^{-x} = e^{-x}\\
   0 \neq e^{-x} \\
   \cancel{y_p = Ae^{-x}}
\end{gather*}
\begin{gather*}
    y_p = Axe^{-x}\\
   \to y_p'= Ae^{-x} - Axe^{-x}\\
    \to y_p'' = Axe^{-x} - 2Ae^{-x}\\
    (Axe^{-x} - 2Ae^{-x}) + 3(Ae^{-x} - Axe^{-x}) + 2(Axe^{-x}) = e^{-x}\\
    Ax - 2A + 3A - 3Ax + 2Ax = 1\\
    A = 1\\
    \boxed{y_p = xe^{-x}}
\end{gather*}
\end{mdframed}





%% PARTIAL DIFFERENTIAL EQUATIONS



\begin{mdframed}
\textbf{Partial Differential Equations}
\begin{gather*}
    u(x,y) = c\\
    du = \frac{\partial u}{\partial x} dx + \dudy dy\\
    A(x,y) = \frac{\partial u}{\partial x} \;\; , \;\; B(x,y) = \frac{\partial u}{\partial y}\\
    \frac{\partial A}{\partial y} = \frac{\partial^2 u}{\partial y \partial x} = \frac{\partial B}{\partial x}
\end{gather*}
\begin{gather*}
    \frac{\partial A}{\partial y} = 1 \;\;\;\;\;\; \frac{\partial B}{\partial x} = 1\\
    \begin{cases}\frac{\partial u}{\partial x} = x + y \;\;\;\;\;\; u = \frac{1}{2}x^2 + xy + C(y) \;\;\; \Circled{1}\\
    \frac{\partial u}{\partial y} = x \;\;\;\;\;\;\;\;\;\;\;\;\; u = xy + D(x) \;\;\; \Circled{2}
    \end{cases}\\
    \Circled{1} \quad \frac{\partial u}{\partial y} = x + \frac{\partial C}{\partial y} = x \quad \therefore \quad \frac{\partial C(y)}{\partial y} = 0 \to C(y) = c \\
    \Circled{2} \quad \frac{\partial u}{\partial x} = y + \frac{\partial D}{\partial x} = x + y \quad \therefore \quad \frac{\partial D(x)}{\partial x} = x \to D(x) = \frac{1}{2}x^2 + c
\end{gather*}
Solution:
\begin{gather*}
    \frac{1}{2}xy^2+xy
\end{gather*}

\end{mdframed}



%%ASK ABOUT THE PARTIAL DIFFERENTIAL EQUATIONS



\begin{mdframed}
\textbf{Sturm-Louiville}\\
\underline{Changtao - 1}\\
If $A$ is Hermitian $A = A^\dag $
\begin{gather*}
    \bra{u_n}A\ket{u_m}^* = \bra{u_m}A^\dag \ket{u_n} = \bra{u_m}A\ket{u_n}\\
    \bra{u_m}A\ket{u_n} = \bra{u_n}A\ket{u_n}
\end{gather*}
Consider if $u_n$'s are complex and $A$ is real:
\begin{gather*}
    A=\dv{x}\left[p\dv{x}\right]-q
\end{gather*}
$p$ , $q$ are real functions;
\begin{gather*}
    \text{LHS} = \bra{u_m}A\ket{u_n} = \int_{a}^{b} dxu_m^*[\dv{x}[p\dv{u_n}{x}]-qu_n] = \int_{a}^{b} dxu_m^*\dv{x}[p\dv{u_n}{x}]- \int_{a}^{b}u_m qu_n^*\\
    \text{RHS} = [\int_{a}^{b} dxu_n^*[\dv{x}[p\dv{u_m}{x}]-qu_m]]^* = \int_{a}^{b} dxu_n^*\dv{x}[p\dv{u_m}{x}]- \int_{a}^{b}u_n qu_m^*
\end{gather*}
The second terms in RHS and LHS are the same, so need to consider only the first term.\\
The first term in the LHS:
\begin{gather*}
    \int_{a}^{b} dxu_m^* \left[\dv{x}\left[p\dv{u_n}{x}\right]\right] = u_m^* \left(p\dv{u_n}{x}\right)\eval_{a}^{b} - \int_{a}^{b}dx \left(\dv{x} u_m^*\right)\left(p\dv{u_n}{x}\right)
\end{gather*}
Let $a$ and $b$ be such that: LHS = $\left[u_m^*\left(p\dv{u_n}{x}\right)\right]\eval_{a}^{b} = 0 $
\begin{gather*}
    =-\int_{a}^{b}dx\dv{x}\left[p\dv{u_m^*}{x}\right]\dv{u_n}{x}\\
\end{gather*}


RHS:
\begin{gather*}
   RHS  = \left[u_n\left(p\dv{u_m^*}{x}\right)\right]\eval_{a}^{b} - \int_{a}^{b}dx\dv{u_m^*}{x} \left[p\dv{u_n}{x}\right]
\end{gather*}
LHS = RHS if
\begin{gather*}
    P(b)=P(a)=0 \quad \textbf{or} \quad u_n(b) = u_n(a) = 0 \quad \textbf{or} \;\;\;\;\; \dv{u_n}{x}(b) = \dv{u_n}{x}(a) = 0
\end{gather*}
\end{mdframed}









\begin{mdframed}
\textbf{II}
\begin{gather*}
    -\dv[2]{u}{x} = \lambda u \quad \lambda > 0 \quad (a,b)
\end{gather*}
An operator $L$ is Hermitian:
\begin{gather*}
    \int_{a}^{b}dx\left[u^*(x)Lv(x)\right] = \int_{a}^{b}dx\left[v^*(x)Lu(x)\right]\\
    \bra{u}L\ket{v}=\bra{v}L\ket{u}^*
\end{gather*}
Note: $u$ and $v$ need not be vector functions.
\\ If $L$ is Hermitian, then the eigenfunctions are orthogonal, and the eigenvalues are real.
\begin{gather*}
    Lu_i(x) = \lambda_iu_i(x) \quad \text{and} \quad Lu_j(x) = \lambda_ju_j(x)\\
    u_j^*Lu_i(x) = \lambda_iu_j^*u_i \quad \text{and} \quad u_i^*Lu_j(x) = \lambda_ju_i^*u_j\\
    \int_{a}^{b}dx\left[u_j^*Lu_i - (u_i^*Lu_j)^*\right] = (\lambda_i - \lambda_j)\int_{a}^{b}dxu_iu_j*
\end{gather*}
If $L$ is Hermitian, the LHS = 0
\begin{gather*}
    \therefore (\lambda_i - \lambda_j)\int_{a}^{b}dxu_iu_j* = 0
\end{gather*}
\begin{gather*}
    \Circled{1} \;\; \text{For} \;\; i\neq j \quad \lambda_i \neq \lambda_j\\
    \therefore \int_{a}^{b}dxu_iu_i^* = 0\\
    \therefore u_i \; , \; u_j \;\; \text{orthogonal}
\end{gather*}
\begin{gather*}
    \Circled{2} \;\; \text{For} \;\; i = j\\
    \int_{a}^{b}dxu_iu_j^* = \int_{a}^{b}dx|u_i|^2 > 0\\
    \therefore \lambda_i = \lambda_i^*\\
    \therefore \lambda_i = \text{real}
\end{gather*}
Check if $-\dv[2]{}{x}$ is Hermitian.
\begin{gather*}
    \int_{a}^{b}dx \left[u^*(\dv[2]{v}{x})\right] \stackrel{?}{=} \intab dx\left[v^*(\dv[2]{u}{x})\right]\\
    \stackrel{i.e.}{\to} \intab dx u^{*} {\dv[2]{v}{x} \stackrel{?}{=}dxv\dv[2]{u^*}{x}}\\
    LHS = u^*\dv{v}{x}\eval_{a}^{b} - \intab dx \dv[]{u^*}{x}\dv{v}{x} = -\dv{u^*}{x}v\eval_{a}^{b} + \intab dx \dv[2]{u^*}{x}v\\
    \therefore LHS = RHS\\
    \bra{u}L\ket{v}=\bra{v}L\ket{u}^*\\
    \bra{i}L\ket{j}=\bra{j}L\ket{i}^*\\
    L_{ij}=L_{ji}^*\\
    L_{11}=L_{11}^*\\
    \text{etc.}
\end{gather*}
\begin{gather*}
    \mqty(L_{11} & L_{12} & L_{13} \\ L_{21} & L_{22} & L_{23} \\ L_{31} & L_{32} & L_{33})
\end{gather*}
Disposal elements real $L_{21} = L_{12}^*$
\begin{gather*}
    \mqty(L_{11} & L_{12}^* & L_{13} \\ L_{21} & L_{12}^* & L_{13}^* \\ L_{31} & L_{23}^* & L_{33})
\end{gather*}


%%START?

\textbf{\underline{START}}\\
General Equation Sturm-Louiville:
\begin{gather*}
    \dv[]{}{x}\left[p(x)\dv[]{u(x)}{x}\right]-q(x)u(x)+\lambda\rho(x)u(x) = 0\\
    L = \dv[]{}{x}\left[p\dv[]{u}{x}\right] - q = pu'' + p'u' - q\\
\end{gather*}

$Lu=-\lambda\rho u$ $L$ is Hermitian

\begin{gather*}
    \intab dx u_i^*(x)u_j(x)\rho(x) = 0, \quad  i \neq j
\end{gather*}
\end{mdframed}


%%SPECIAL FUNCTIONS

\begin{mdframed}
\textbf{Special Functions}\\
$\to$ See other notes.\\

Any "well-behaved" function can be written as a complete set of orthonormal functions. these orthonormal functions are solutions to eigenvalue equations e.g. are mostly the "special" functions.

\doublespacing
\begin{gather*}
    f(x) = \sum_n a_n u_n (x) \quad (a,b)\\
    \intab dxu_n(x)u_m^*(x) = \delta_{nm}\\
    \therefore \;\; a_m = \intab dx f(x) u_m^*(x)\\
    f(x) = \sum_n a_n u_n = \sum_n \left[\intab dx' f(x') u_n^* (x')\right]u_n(x) = \intab dx'f(x')\sum_n u_n^* (x') u_n(x)\\
    \int_0^L dx \sin^2\left(\frac{n\pi x}{L}\right) = \int_0^L dx \frac{1 - \cos\frac{2n\pi x}{L}}{2} = \frac{1}{2} L\\
    \sum_n u_n^*(x')u_n (x) = \delta(x'-x) \qquad \delta (x'-x) = 0\\ \qquad x' \neq x \qquad \text{and} \qquad \intab dx' f(x') \delta (x'-x) = f(x)
\end{gather*}


\textbf{Fourier Series}

\begin{gather*}
    f(x) = \sum_n a_n \sqrt{\frac{2}{L}} \sin\left(\frac{n \pi x}{L}\right) \qquad (0,L)\\
    \text{or}\\
    f(x) = \sum_n b_n \frac{2}{L}\sin\left(\frac{2n\pi x}{L}\right) + \sum_n a_n \frac{2}{L} \cos \left(\frac{2n\pi x}{L}\right)\\
    a_n = \frac{2}{L} \int_0^L dx f(x) \sin\left(\frac{n \pi x}{L}\right)\\
    \text{etc.}
\end{gather*}

One can write:\\


\textbf{Sine Series}
\begin{gather*}
    f(x) = \sum_n \sin\left(\frac{n \pi x}{L}\right)\\
    a_n = \frac{2}{L} \int_0^L dx f(x) \sin\left(\frac{n \pi x}{L}\right)
\end{gather*}


\textbf{Fourier Series}
\begin{gather*}
    f(x) = \frac{1}{2}a_0 + \sum_n a_n \cos\left(\frac{2n \pi x}{L}\right) + \sum_n b_n \sin\left(\frac{2n \pi x}{L}\right)\\
    a_n = \frac{2}{L}\int_0^L dx f(x) \cos\left(\frac{2n \pi x}{L}\right)\\
    b_n = \frac{2}{L}\int_0^L dx f(x) \sin\left(\frac{2n \pi x}{L}\right)\\
\end{gather*}

\end{mdframed}





%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK



\begin{mdframed}
\begin{gather*}
    L(x) = \dv[2]{}{x}
\end{gather*}
Is it Hermitian?
\begin{gather*}
    \intab dx u_m(x) \dv[2]{u_n}{x} - \intab dx u_n \dv[2]{u_m}{x}\\
    =\intab dx \dv{}{x} \left[u_m \dv[]{u_n}{x} - u_n \dv[]{u_m}{x}\right] = \left(u_m \dv[]{u_n}{x} - u_n \dv[]{u_m}{x}\right) \eval_a^b
\end{gather*}

\hrule
\bigskip
\textbf{\Circled{2}} Eigenvalues + Orthogonality\\
\begin{gather*}
    A\ket{\alpha_n} = a_n\ket{\alpha_n}\\
    \bra{\alpha_n}A\ket{\alpha_n} = a_n\\
    \bra{\alpha_n}A\ket{\alpha_n}^* = \bra{\alpha_n}A^\dag \ket{\alpha_n} = \bra{\alpha_n}A\ket{\alpha_n}\\
    a_n = a_n^*
\end{gather*}

Eigenvalues are real.\\
\bigskip


\textbf{Orthogonality}
\begin{gather*}
    A\ket{\alpha_m} = a_n\ket{\alpha_m}\\
    \text{then}\\
    \bra{\alpha_m}\ket{\alpha_n} = 0
\end{gather*}
\begin{align*}
    &\Circled{1} \bra{\alpha_m}A\ket{\alpha_n} = a_n \bra{\alpha_m}\ket{\alpha_n}\\
    &\Circled{2} \bra{\alpha_n}A\ket{\alpha_m} = a_m \bra{\alpha_n}\ket{\alpha_m} \to \bra{\alpha_m}A^\dag \ket{\alpha_n} = a_m \bra{\alpha_m}\ket{\alpha_n}\\
    &\Circled{3} \bra{\alpha_m}A\ket{\alpha_n} = a_m \bra{\alpha_m}\ket{\alpha_n}
\end{align*}

But
\begin{gather*}
    A^\dag = A \qquad \bra{\alpha_m}A\ket{\alpha_n} = a_m \bra{\alpha_m}\ket{\alpha_n}\\
    0 = (a_n - a_m) \bra{\alpha_m}\ket{\alpha_n}\\
    \text{If } a_n \neq a_m , \quad
    \bra{\alpha_m}\ket{\alpha_n} = 0\\
    \implies \text{Orthogonal.}
\end{gather*}

\hrule
\bigskip


$L(x) = \dv[]{}{x}(1-x^2)P_l'$ is Hermitian.

\begin{gather*}
    \int_{-1}^1dx\left[P_n\dv[]{}{x}\left[(1-x^2)P_l'\right] - P_l\dv[]{}{x}\left[(1-x^2)P_n'\right]\right] = (\lambda_n - \lambda_l) \int_{-1}^1 dx P_nP_l\\
    LHS = \int_{-1}^1 dx \dv[]{}{x}\left[ (1-x^2)(P_l'P_n-P_n'P_l)\right] = \left[ (1-x^2)(P_l'P_n-P_n'P_l)\right] \eval_{-1}^{1}\\
    \int_{-1}^{1} dx P_l(x)P_{l'}(x) = 0 \qquad l \neq l'\\
    \int_{-1}^{1} dx P_l(x)P_{l'}(x) = \frac{2}{2l+1}\delta_{ll'}
\end{gather*}
\hrule
\bigskip

\textbf{Associate Legendre Polynomial}

\begin{gather*}
    (1-x^2)y'' - 2xy' + \left[l(l+1) - \frac{m^2}{1-x^2}\right] y = 0\\
    y = P_l^m(x) = associate \;\; L.P.\\
    P_l^m(x) = (1-x^2)^{m/2}\dv[m]{}{x}P_l(x)
\end{gather*}

\hrule
\bigskip

\textbf{Sturm-Liouville Equation}

\begin{gather*}
    \dv[]{}{x}\left[p(x)\dv[]{u}{x}\right] - q(x)u(x) + \lambda\rho(x)u(x) = 0\\
    P_0(x) = 1 \quad P_1(x) = x \quad P_2(x) = \frac{3x^2 - 1}{2}\\
    (1-x^2)P_l'' - 2xP_l + l(l+1)P_l = 0\\
    P_1(x) = \frac{1}{2^l l!}\dv[']{}{x}(x^2 - 1)'
\end{gather*}

Rodriques' Formula\\
Generate Function
\begin{gather*}
    \frac{1}{\sqrt{1-2xh+h^2}} = P_0(x) + P_1(x)h + \cdots
\end{gather*}

One can write this equation:

\begin{gather*}
    \dv[]{}{x}\left[(1-x^2)P_l'\right] = -l(l+1)P_l
\end{gather*}

Eigenvalue Equation

\begin{gather*}
    L(x) P_l = \lambda_lP_l \quad \lambda_l = -l(l+1)\\
    L(x)P_l = \dv[]{}{x}\left[(1-x^2)P_l'\right]
\end{gather*}


\hline

Hermitian Operator
\begin{gather*}
    \bra{\alpha_m}A\ket{\alpha_n}^* = \bra{\alpha_m}A^\dag\ket{\alpha_n}
\end{gather*}

$A^\dag$ is the Hermitian conjugate of $A$.\\
If $A$ is Hermitian $A^\dag = A$.

\begin{gather*}
    \bra{\alpha_m}A\ket{\alpha_n}^* = \bra{\alpha_m}A\ket{\alpha_n}
\end{gather*}

If the quantities are real $\bra{\alpha_m}A\ket{\alpha_n}^* = \bra{\alpha_m}A\ket{\alpha_n}$ are interchangeable.
\bigskip





\hline

\begin{gather*}
    a_5 = -\frac{k^2}{5 \cdot 4}a_3 = \left(\frac{-k^2}{5 \cdot 4}\right)\left(\frac{-k^2}{3 \cdot 2}\right)a_0 = \frac{k^4}{5!}a_0 \quad v = 3
\end{gather*}
\begin{gather*}
    u(x) = a_1x +a_3x^3 + a_5x^5 + \cdots = a_1x - \frac{k^2}{3!}a_1^3 + \cdots = \frac{a_1}{k}\left[kx - \frac{(kx)^3}{3!} + \cdots \right] = \frac{a_1}{k} \sin{kx} = a_1' \sin{kx}
\end{gather*}
\begin{gather*}
    a_x = - \frac{k^2}{v(v-1)}a_{v-2} = \left[\frac{-k^2}{v(v-1)}\right]\left[\frac{-k^2}{(v-2)(v-3)}\right]a_{v-4} = \left[\frac{-k^2}{v(v-1)}\right]\left[\frac{-k^2}{(v-2)(v-3)}\right] \cdots \left[\frac{-k^2}{2 \cdot 1}\right]a_0
\end{gather*}                       

$y(x), \;\; y(-x) \implies$ are both odd or even solutions.\bigskip





\hline
\textbf{Legendre Polynomials}
\begin{gather*}
    (1-x^2)\dv[2]{y}{x} - 2x\dv[]{y}{x} + l(l+1)y = 0
\end{gather*}
\begin{gather*}
    y = a_0 + a_1x + a_2x + \cdots + a_vx^v + a_{v+1}x^{v+1} + a_{v+2}x^{v+2} + \cdots
\end{gather*}
\begin{gather*}
    \dv[]{y}{x} = a_1 + 2a_2 x + \cdots + va_vx^{v-1} + (v+1)a_{v+1}x^{v} + (v+2)a_{v+2}x^{v+1} + \cdots
\end{gather*}
\begin{gather*}
    \dv[2]{y}{x} = 2a_2 + \cdots + v(v-1)a_vx^{v-2} + (v+1)va_{v+1}x^{v-1} + (v+2)(v+1)a_{v+2}x^v + \cdots
\end{gather*}
\begin{gather*}
    \dv[2]{y}{x} - x^2\dv[2]{y}{x} - 2x\dv[]{y}{x} + l(l+1)y = 0
\end{gather*}
Coefficient of $x^v$\\
\begin{gather*}
    (v+2)(v+1)a_{v+2} - v(v-1)a_v - 2va_v + l(l+1)a_v = 0\\
    (v+2)(v+1)a_{v+2} - \left[(v^2 - v +2v) - l(l+1) \right]a_v = 0\\
    (v+2)(v+1)a_{v+2} - \left[v(v+1) - l(l+1)\right] = 0
\end{gather*}
\begin{gather*}
    a_{v+2} = \frac{v(v+1) - l(l+1)}{(v+2)(v+1)}a_v\\
    \text{As}\;\; v \to \infty \quad a_{v+2} \to a_v
\end{gather*}

Very large $v$ (finite) $+ (a_vx^v + a_{v+1} + \cdots) = a_v\left[x^v + x^{v+1} + \cdots \right]$ diverage at $x = 1$,
$v = l$ series terminate.


\begin{gather*}
    y(x) = \begin{cases}
    a_0 + a_2x^2 + \cdots + a_lx^l \quad (l = \text{ even})\\
    a_1x + a_3x^3 + \cdots + a_lx^l \quad (l = \text{ odd})
    \end{cases}
\end{gather*}
\begin{gather*}
    l = 0 \quad y(x) = a_0 \quad a_2 = 0\\
    l = 2 \quad a_2 = \frac{-2 \cdot 3}{2 \cdot 1}a_0 = -3a_0 \quad y = a_0 - 3x^2a_0 \to \frac{3x^2-1}{2}
\end{gather*}

Individual Equation
\begin{gather*}
    y(x) = x^s(a_0 + a_1 x + \cdots )
\end{gather*}

Lowest Power
\begin{gather*}
    -s(s-1)x^{s-2}a_0 \quad (s=0, \;s=1)\\
    P_l(x), \; \text{under that} \; P_l(x) = 1
\end{gather*}

\begin{gather*}
    L=\dv[]{}{x}\left[P(x)\dv[]{}{x}\right] - q(x) \text{ Is Hermitian}
\end{gather*}
\begin{gather*}
    \intab dx u_m^*(x)u_n(x)\rho(x) = 0 \qquad m \neq n
\end{gather*}
$\rho(x) =$ weight function/ density function
\bigskip
\hline

\textbf{Bessel Function}

\begin{gather*}
    x^2y'' + xy' + (x^2 - p^2)y = 0
\end{gather*}

Individual
\begin{gather*}
    x^2 \cdot s(s-1)x^{s-2} + x \cdot sx^{s-1} + x^2 \cdot x^s - p^2x^s\\
    x^s\left[s(s-1) + s - p^2\right] = x^s(s^2 - p^2)\\
    s = p, \;\; s = -p
\end{gather*}



\end{mdframed}




%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK






%%EIGENVALUE EQUATIONS AND GREEN'S FUNCTION



\begin{mdframed}
\large{\textbf{\underline{Eigenvalue Equations and Green's Function}}}\\
\Circled{1}
\begin{gather*}
    \dv[2]{x}{u} = \lambda u
\end{gather*}
(i)
\begin{gather*}
    \lambda > 0 \quad d = R^2 \quad u = Ae^{kx} + be^{kx} \quad (R = \text{real})
\end{gather*}
\begin{align*}
    u(0) = u(a) \quad &\implies \quad A+B = Ae^{Ru} + Be^{-Ru}\\
    u'(0) = u'(a) \quad &\implies \quad R(A-B) = R(Ae^{Ru} + Be^{-Ru})
\end{align*}
\begin{gather*}
    \text{implies} \quad e^{Ra} = 1 \qquad \text{\underline{only} one possibility:} \quad R=0
\end{gather*}
(ii)
\begin{gather*}
   d<0 \quad d=-R^2 \quad u = A\sin Rx + B\cos Rx
\end{gather*}
\begin{align*}
    u(0) = u(a) \quad &\implies \quad B = A\sin Ra + B\cos Ra\\
    u'(0) = u'(a) \quad &\implies \quad RA = R\left[A\sin Ra - B\cos Ra\right]
\end{align*}
\begin{gather*}
    \text{Solutions are:} \begin{cases}
    A = 0; \quad \cos Ra = 1, \quad \sin Ra = 0\\
    B = 0; \quad \sin Ra = 0, \quad \cos Ra =1 
    \end{cases}\\
    Ra = 2u\pi
\end{gather*}
\begin{gather*}
    \therefore \quad u = A\sin \left(\frac{2u\pi x}{a}\right) + B\cos \left(\frac{2u\pi x}{a}\right)
\end{gather*}



\Circled{2}
\begin{gather*}
    \dv[]{u}{x} + Pu = Q
\end{gather*}
\begin{align*}
    &\text{For} \quad Q = 0 \quad u=ce^{-\itint{}{x}Pdx'}
\end{align*}
\begin{align*}
    &\text{For} \quad Q\neq 0 \quad\\
    &\text{let} \quad u = \alpha(x)e^{-\itint{}{x}Pdx'}\\
    &\text{then} \quad \left[\alpha' e^{-\itint{}{x}Pdx'}+\alpha (-P)e^{-\itint{}{x}Pdx'}\right] + P\alpha e^{-\itint{}{x}Pdx'} = Q
\end{align*}
\begin{gather*}
    \alpha' \integratingfac = Q
\end{gather*}
\begin{gather*}
    \therefore \quad \alpha = \int dx' Q(x')e^{-\itint{}{x'}Pdx''} + C
\end{gather*}
\begin{gather*}
    \therefore \quad u = \left[\int dx' Q(x) e^{-\itint{}{x'}Pdx'' + C}\right]e^{-\itint{0}{x}Pdx'}
\end{gather*}


\Circled{3}
\begin{gather*}
    A\dv[2]{u}{x} + B\dv[]{u}{x} + C = 0 \qquad \text{A, B, C are constants}
\end{gather*}
\begin{gather*}
    u = e^{\beta x} \quad \beta = \text{constant}
\end{gather*}
\begin{gather*}
    A\beta^2 + B\beta + C = 0 \quad \text{from the above equation}
\end{gather*}
\begin{gather*}
    \beta = \frac{-B \pm \sqrt{B^2-4AC}}{2A}
\end{gather*}
\begin{gather*}
    \therefore \quad u = c_1e^{\beta_1x} + c_2e^{\beta_2x}
\end{gather*}


\Circled{4}
\begin{gather*}
    L = -\dv[2]{}{x}
\end{gather*}
\begin{gather*}
    \bra{u}L\ket{v}=-\itint{a}{b}dxu^*\dv[2]{v}{x} = -\left[u^*\dv[]{v}{x}\eval_a^b = \itint{a}{b}dx \dv[]{u^*}{x}\dv[]{v}{x}\right] = 0\\
    \text{either hom b.c or periodic b.c.}
\end{gather*}
\begin{gather*}
    =+\itint{a}{b}dx\dv[]{u^*}{x}\dv[]{v}{x}
\end{gather*}
\begin{gather*}
    \dv[]{u^*}{x}v\eval_a^b - \itint{a}{b}dx \dv[2]{u^*}{x}v = -\itint{a}{b}dx \dv[2]{u^*}{x}v = 0
\end{gather*}
\begin{gather*}
    \bra{v}L\ket{u}
\end{gather*}
\Circled{5}
Similarly for B
\begin{gather*}
    L = \dv[]{}{x}[P\dv[]{}{x}] - q
\end{gather*}
\Circled{6}
\begin{gather}
    f(x) = x \quad, \quad (0,L)
\end{gather}
(i)
\begin{gather*}
    \text{Fourier Series} \quad x = \sum_n C_n \sqrt{\frac{2}{L}}\sin\frac{n\pi x}{L} = \sum_n C_n U_n
\end{gather*}
\begin{gather*}
    \therefore \quad C_n = \itint{0}{L}dx xU_n = \itint{0}{L}dx x \sqrt{\frac{2}{L}}\sin\frac{n\pi x}{L}
\end{gather*}
\begin{gather*}
    \itint{0}{L} dx x \sin\frac{n\pi x}{L} = \frac{L^2}{n\pi}(-1)^{n+1} \quad \text{then obtain $C_n$}
\end{gather*}

(ii) Fourier Series
\begin{gather*}
    x = \frac{a_0}{2}+ \sumni \left[a_n \cos\frac{2n\pi x}{l} + b_n \sin\frac{2n\pi x}{L}\right]\\
    \text{use orthogonality in (i)}
\end{gather*}

\hline
\Circled{1}\\
For 
\begin{gather*}
    g(x,x') = \frac{(x-L)x'}{L} \theta(x-x') + \frac{x(x'-L)}{L}\theta(x-x')
\end{gather*}
show that
\begin{gather*}
    \dv[2]{y}{x} = \delta(x-x')
\end{gather*}
\underline{Proof}
\begin{gather*}
    \dv[]{y}{x} = \frac{x'}{L}\theta(x-x') + \frac{(x-L)x'}{L}\delta(x-x') + (\frac{x'-L}{L})\theta(x-x') + \frac{x(x'-L)}{L}(-)\delta(x'-x)
\end{gather*}
\begin{gather*}
    \text{since} \quad \delta(x-x') = \delta(x'-x)
\end{gather*}
\begin{gather*}
    \therefore \quad \dv[]{y}{x} = \frac{x'}{L} \theta(x-x') + \frac{(x-L)x'}{L}\theta(x-x') + \left[\frac{(x-L)x'}{L}-\frac{x(x'-L)}{L}\right]\delta(x-x') = 0
\end{gather*}

\begin{gather*}
    \dv[2]{y}{x} =\frac{x'}{L}\delta(x-x') + (\frac{x'-L}{L}(-)\delta(x-x')
\end{gather*}
\begin{gather*}
    =\left[\frac{x'}{L}-\frac{(x'-L)}{L}\right]\delta(x-x') = \delta(x-x')
\end{gather*}

\Circled{2}
\begin{gather*}
    \dv[2]{u}{x}=1 \qquad u=\frac{x^2}{2}-\frac{Lx}{2} \qquad \text{for} \quad u(0) = 0 = u(L)
\end{gather*}
Show the value result obtains with the Green's theorem.\\
In this case, $f(x) = 1$ and $g(x,x')$ is already given above.
\begin{gather*}
    u(x) = \itint{0}{L} dx'g(x,x')f(x') = \itint{0}{L}dx'g(x,x')
\end{gather*}
\begin{align*}
    =&\itint{0}{x}(\frac{x-L}{L})x' \quad +&\itint{x}{L}dx'\frac{x(x'-L)}{L}\\
    &x'<x &x'>x
\end{align*}
Integration gives back the correct answer.\\


\Circled{3}
\begin{gather*}
    \dv[2]{g}{x} - R^2g =\delta(x-x') \quad 0<x<\infty\\
    g(0) = 0 = g(\infty)
\end{gather*}
\begin{gather*}
    \dv[2]{g}{x}-R^2g = 0 \quad x \neq x' \quad \implies \quad g= c_1e^{Rx} + c_2e^{-Rx}
\end{gather*}
\begin{align*}
    \therefore \quad g(x,x') &= A\sinh Rx \quad x<x'\\
    &=Be^{-Rx} \quad x>x'
\end{align*}
Symmetry (or equality or continuity) at $x=x'$ gives 
\begin{align*}
    g(x,x') &= c\sinh(Rx) e^{-Rx'} \quad x<x'
    &=ce^{-Rx}\sinh(Rx') \quad x>x'
\end{align*}
\begin{gather*}
    \itint{x-\epsilon}{x-\epsilon}dx'\dv[2]{y}{x} - 0 = 1 \quad \implies \quad c = -\frac{1}{R}
\end{gather*}
\begin{align*}
    \therefore \quad g(x,x') &= -\frac{\sinh(Rx)e^{-Rx'}}{R} \quad x<x'
    &=-\frac{e^{-Rx}\sinh(Rx')}{R} \quad x>x'
\end{align*}


\Circled{4}
\begin{gather*}
    F(R) = \frac{1}{2\pi}\superint f(x)e^{-iRx}
\end{gather*}
\begin{gather*}
    f(x) = e^{-ax^2} \quad F(R) = \frac{e^{-R^2/4a}}{2\sqrt{\pi a}}
\end{gather*}
\begin{gather*}
    f(x) = \begin{cases} e^{-ax^2} \quad &x>0\\ 0 \quad &x<0 \end{cases}
   \quad  F(R) = \frac{1}{2\pi(1+iR)}
\end{gather*}
\begin{gather*}
    f(x) = e^{-a\abs{x}} \quad F(R) = \frac{a}{\pi(a^2 + R^2}
\end{gather*} 

\Circled{5}\\
(i)
\begin{gather*}
    f(z) = e^z + e^{x+iy} = e^x\cos y   + ie^x\sin y
\end{gather*}
\begin{gather*}
    u = e^x\cos y \quad , \quad v = e^x\sin y
\end{gather*}
\begin{align*}
    &\pdv{u}{x} = e^x\cos y \qquad &\pdv{v}{y} = e^x\cos y\\
    &\pdv{u}{y} = -e^x\sin y \qquad &\pdv{v}{x} = e^x\sin y\\
    &\text{C-R conditions satisfied}&
\end{align*}
(ii)
\begin{gather*}
    f(z) = e^{iz} = e^{ix-y} = e^{-y}\cos x + ie^{-y}\sin x\\
    f(z) \text{ can be shown to be analytic}
\end{gather*}
(iii)
\begin{gather*}
    f(z) = \abs{z}^2 = x^2+y^2 \qquad u = x^2 + y^2, \quad v = 0
\end{gather*}
\begin{gather*}
    \therefore \quad \pdv{u}{x} \neq \pdv{v}{y} \quad \text{etc. C-R conditions \underline{not} satisfied.}
\end{gather*}

\bigskip\hline
\Circled{1}\\
(i) Show
\begin{gather*}
    \lim_{\beta\to\infty} \sqrt{\frac{\beta}{\pi}}e^{-\beta x^2} = \delta(x)
\end{gather*}
\begin{gather*}
    \sqrt{\frac{\beta}{\pi}}\eval_{-\infty}^{\infty} dxe^{-\beta x^2} \quad \text{let} \quad \beta x^2 = y^2 \quad \sqrt{\beta}x = y \quad \therefore \quad dx = \frac{dy}{\sqrt{\beta}}
\end{gather*}
\begin{gather*}
   \sqrt{\frac{\beta}{\pi}}\eval_{-\infty}^{\infty} dye^{-y^2} =\frac{1}{\sqrt{\pi}}\cdot \left[\sqrt{\pi}\right] = 1 \leftarrow \text{from integration table or otherwise}
\end{gather*}
(ii) Show
\begin{gather*}
    \lim_{\epsilon\to 0} \frac{1}{\pi}\frac{\epsilon}{\epsilon^2+x^2} = \delta(x)
\end{gather*}
\begin{gather*}
    \frac{\epsilon}{\pi}\superint \frac{dx}{\epsilon^2 + x^2} = \frac{\epsilon}{\pi}\frac{1}{\epsilon}\tan^{-1}\eval_{-\infty}^\infty = \frac{1}{\pi}\left[\frac{\pi}{2}-\frac{\pi}{2}\right]=1
\end{gather*}
\Circled{2} Numerical\\
\Circled{3}
\begin{gather*}
    \pdv[2]{\phi}{x} - \frac{1}{v^2}\pdv[2]{\phi}{t} = 0 \quad  0 <x<a, \quad t>0
\end{gather*}
\begin{gather*}
    \phi(0,t) = 0 = \phi(a,t) \quad ;\quad \pdv{\phi}{t}(x,0) = 0\\
    \text{and} \quad \phi(x,0) = f(x)\\
    \phi(x,t) = \Bar{X}(x)T(t)
\end{gather*}
\begin{gather*}
   \therefore \quad \frac{\Bar{X''}}{\Bar{X}} - \frac{1}{v^2}\frac{T''}{T} = 0
\end{gather*}
\begin{gather*}
    \textbf{\underline{b.c.}} \quad \therefore \quad \Bar{X_n} = A_n\sin(\frac{n\pi x}{a}) , \quad T_n = B_n \cos(\frac{n\pi vt}{a})\\
    \phi(x,t) = \sum_n C_n \sin(\frac{n\pi x}{a})\cos(\frac{n\pi vt}{a})\\
    f(x) = \sum_n C_n \sin(\frac{ n \pi x}{a})\\
    \text{$C_n$ can be obtained as usual}
\end{gather*}
\bigskip
\Circled{4}\\
Show
\begin{gather*}
    \phi(x \pm vt) \quad \text{satisfy} \quad \pdv[2]{\phi}{x} - \frac{1}{v^2}\pdv[2]{\phi}{t} = 0
\end{gather*}
Consider:
\begin{gather*}
    \phi(x+vt)\\
    \text{let} \quad y = x + vt
\end{gather*}
\begin{align*}
    \therefore \quad &\pdv{\phi}{x} = \pdv{\phi}{y}\pdv{y}{x} = \pdv{\phi}{y} \quad \text{similarly} \quad \pdv[2]{\phi}{x} = \pdv[2]{\phi}{y}\\
    &\pdv{\phi}{t} = \pdv{\phi}{y}\pdv{y}{t} = \pdv{\phi}{y}v \quad \text{similarly} \quad \pdv[2]{\phi}{t} = v^2\pdv[2]{\phi}{y}
\end{align*}
\begin{gather*}
    \therefore \quad \pdv[2]{\phi}{x} - \frac{1}{v^2} = 0 \quad \text{for} \quad \phi = \phi(x+vt) , \quad \phi = \phi(x-vt)
\end{gather*}
\bigskip
\Circled{5}
The solution is 
\begin{gather*}
    \phi(x,y,t) = \sum_{nm} C_{nm} \sin (\frac{n\pi x}{a}) \sin(\frac{m\pi y}{b}) \cdot \sin(v\pi \sqrt{\frac{n^2}{a^2} + \frac{m^2}{b^2}}t)
\end{gather*}
\begin{gather*}
    \pdv{\phi}{t} = xy \quad \text{at} \quad t=0 \quad \text{gives}
\end{gather*}
\begin{gather*}
    xy = \sum_{nm} C_{nm}\left[v\pi\sqrt{\frac{n^2}{a^2} + \frac{m^2}{b^2}}\right]\sin(\frac{n\pi x}{a})\sin(\frac{m\pi x}{b})
\end{gather*}
use double orthogonality
\begin{gather*}
    f(x,y) = \sum_{nm} C_{nm} U_n(x) W_m(x)
\end{gather*}
then
\begin{gather*}
    X_{nm} ~ \itint{0}{a}dx\itint{a}{b}dy f(x,y)U_n^*(x)W_m^*(x)
\end{gather*}
\bigskip
\Circled{6}
\begin{gather*}
    \phi(x,y) = X(x)Y(y)\\
    X~\sin(\frac{n\pi x}{a}) \quad y ~\sinh(\frac{n\pi y}{a})
\end{gather*}
\begin{gather*}
    \therefore \quad \phi = \sum_n X_n \sin(\frac{n\pi x}{a})\sinh(\frac{n\pi y}{a})
\end{gather*}
\begin{gather*}
    \text{from the b.c.} \quad \pdv{\phi}{y} = x \quad \text{at} \quad y =0, \quad \text{determine $C_n$}
\end{gather*}

\bigskip

\Circled{1}
\begin{gather*}
    f(z) = \frac{b}{z-z_0} = \frac{b[(x-x_0) + i(y-y_0)]}{(x-x_0)^2+(y-y_0)^2}
\end{gather*}
From this obtain $\pdv{u}{x}$ etc. and show C-R condition is satisfied.\\
\Circled{2}
\begin{gather*}
    \itint{0}{2\pi} \frac{d\theta}{a+b\cos\theta}\\ 
    a + b\cos\theta = a + \frac{b}{2}(z+\frac{1}{z}) = \frac{bz^2+2az+b}{2z}
\end{gather*}
Poles are given by $bz^2+2az+b = 0$
\begin{align*}
    (A) \quad a>b \qquad &z=\frac{-a\pm i\sqrt{a^2-b^2}}{b}\\
    (B) \quad b>a \qquad &z= \frac{-a\pm i\sqrt{b^2-a^2}}{b}
\end{align*}
For case (B) the poles are \underline{on the contour} so the integral doesn't exist.\\
\Circled{3}
\begin{gather*}
    \itint{0}{2\pi}\frac{d\theta}{5+4\cos\theta} \quad \text{use case (A) above}
\end{gather*}
\Circled{4}
\begin{gather*}
    \oint_c \frac{dz}{z^2+2z+2}\\
    z^2+2z+2 = 0 \quad \implies \quad z = -1\pm i\\
    z_1 = -1+i \quad z_2 = -1-i %%GRAPHS
\end{gather*}
The poles are located at a distance $\sqrt{2}$ from the origin.\\
\Circled{1} \quad $c_1$ radius $=1$ center $z=0$\\
the poles lie outside $\oint_{c_1} = 0$\\
\Circled{2} \quad $c_2$ radius $=2$ center $z=0$\\
the poles lie inside. Use Cauchy's theorem.\\
\Circled{3} \quad $c_3$ radius $=1$ center $z=1$\\
the poles lie outside $\oint_{c_3}=0$
\hline
\begin{gather*}
    \dv[2]{u}{x}=\lambda u
\end{gather*}
\begin{gather*}
    u = ce^{\alpha x} \qquad \alpha^2 = \lambda \quad \therefore
\end{gather*}
\Circled{1}
\begin{gather*}
    \lambda = 0 \qquad \alpha = \pm \sqrt{\lambda} \quad \lambda = k^2
\end{gather*}
\begin{gather*}
    u = c_1e^{kx} + c_2e^{-kx}
\end{gather*}
Consider a finite interval $(0,a)$\\
Homogneneous b.c.
\begin{gather*}
    u(0) = 0 ,\quad u(a) = 0
\end{gather*}
\begin{gather*}
    \therefore \quad 0 = c_1+c_2, \quad 0 = c_1e^{ka} + c_2e^{-ka}
    \implies c_1 = 0 = c_2\\
    \text{No solution!}
\end{gather*}
\Circled{2}
\begin{gather*}
    \lambda <0 \qquad \lambda = -R^2 \quad \alpha = \pm ik
\end{gather*}
\begin{gather*}
    u = c_1e^{iRx}c_2e^{-iRx}\\
    u = A\sin Rx + B\cos Rx
\end{gather*}
\underline{hom b.c.}
\begin{gather*}
    u(0) = 0 , \quad u(a) = 0
\end{gather*}
\begin{gather*}
    B\cos R(0) = 0 \quad\therefore \quad B=0 \quad \therefore \quad u = A\sin Rx\\
    u(a) = 0 \quad \implies \quad A\sin Ra = 0 \quad \text{if} \quad A\neq 0 \quad Ra = 0,\pi,\cdots,n\pi
\end{gather*}
\begin{gather*}
    R = n\frac{\pi}{4}, \quad \lambda = -\frac{n^2\pi^2}{a^2} \quad \text{discrete values and infinite no.}
\end{gather*}
\begin{gather*}
    \dv[2]{u}{x} = -R^2u \quad R = \frac{n\pi}{a}, \quad u_n = A_n \sin(\frac{n\pi x}{a}), \quad n =1 ,2 \cdots
\end{gather*}
\begin{gather*}
    \text{Eigenvalue equations:} %%Add in the graphs
\end{gather*}
\underline{Periodic b.c.}
\begin{gather*}
    u(0) = u(a), \quad u'(0)' = u'(a)
\end{gather*}
\begin{gather*}
    u(x) = A\sin Rx + B\cos Ra, \quad u'(x) = AR\cos Rx - BR \sin Rx\\
    B = A\sin Ra + B\cos Ra\\
    AR = AR\cos Ra - BR \sin Ra\\
    Ra = 2n\pi \quad n = 0,1,\cdots
\end{gather*}
\begin{align*}
    A = 0, \;\; B\neq 0 \qquad &u(x) = A\sin(\frac{2n\pi x}{a})\\
    A\neq 0, \;\; B = 0 \qquad &u(x) = B\cos(\frac{2n\pi x}{a})
\end{align*}
\begin{gather*}
    \begin{rcases}
    \dv[2]{u_n}{x} &=\lambda_nu_n\\
    \dv[2]{u_m}{x} &=\lambda_mu_m
    \end{rcases} \to \dv[2]{u_m^*}{x} = \lambda_m^*u_m^*
\end{gather*}
\begin{gather*}
    u_m^*\dv[2]{u_m}{x}-u_m\dv[2]{u_m^*}{x} = (\lambda_m-\lambda_m^*)u_mu_m^*
\end{gather*}
\begin{gather*}
    \eval_0^a dx \dv[]{}{x}[u_m^*\dv[]{u_m}{x}-u_m\dv[]{u_m^*}{x}]=(\lambda_m-\lambda_m^*)\itint{0}{a}dxu_mu_m^*
\end{gather*}
\begin{gather*}
    \left[u_m^*\dv[]{u_m}{x}-u_m\dv[]{u_m^*}{x}\right]_0^a = (\lambda_m-\lambda_m^*)\itint{0}{a}dv\abs{u_m(x)}^2
\end{gather*}
\begin{gather*}
    \text{LHS} = 0 \quad \therefore \quad \lambda_m = \lambda_m^*\\
    \therefore \quad \text{eigenvalues} = \text{real}
\end{gather*}
\begin{gather*}
    \lambda_n \neq \lambda_m \qquad\\ [u_m^*\dv[2]{u_n}{x}-u_n\dv[2]{u_m^*}{x}]_0^a = (\lambda_m-\lambda_n \itint{0}{a}dxu_m^*u_n\\
    \text{LHS} = 0 \quad \therefore \quad \itint{0}{a} dx u_m^*u_n = 0 \quad \therefore \quad \text{orthogonal}
\end{gather*}




\hline
\begin{gather*}
\dv[]{}{x}\left[P(x)\dv[]{u}{x}\right]-q(x)u(x)+\lambda \rho u(x) = 0
\end{gather*}
\begin{gather*}
    u(x) = -\lambda \rho u(x)
\end{gather*}
For q is the Hermitian provided
\begin{gather*}
    \bra{u_m}\lap\ket{u_n} = \bra{u_n}\lap\ket{u_m} \quad \text{or} \quad \bra{u_m}\lap\ket{u_n}^* = \bra{u_n}\lap^\dag\ket{u_m} = \bra{u_n}\lap\ket{u_m}
\end{gather*}
\begin{gather*}
    u_np(x)\dv[]{u_m}{x}\eval_a^b = 0
\end{gather*}
\begin{gather*}
    \lap u_n = -\lambda_n\rho u_n
\end{gather*}
$\lambda_n$ are real and discrete
\begin{gather*}
    \itint{a}{b}dxu_m^* \rho u_n = \delta_{mn}
\end{gather*}

\begin{gather*}
    P = 1 \quad q = 0 \quad \rho = 1
\end{gather*}
\begin{gather*}
    \lambda = k^2>0 \qquad \dv[2]{u}{x}+k^2u(x) = 0 \qquad 0<x<L
\end{gather*}
\begin{gather*}
    u(0) = 0 = u(L)
\end{gather*}
\begin{gather*}
    u = e^{\pm ikx} = a\cos kx + b\sin kx
\end{gather*}
\begin{gather*}
    u(0) = 0 \quad a = 0 \quad u = b\sin kx \quad u(L) = 0 \quad b\sin kL = 0 \quad kL = n\pi 
\end{gather*}

\end{mdframed}





%%This is another section

\begin{mdframed}
\begin{gather*}
    \zuperint dx e^{-\alpha x} = \frac{1}{\alpha}\\
    \zuperint dx x e^{-\alpha x} = \frac{x e^{-\alpha x}}{-\alpha} \eval_0^\infty \frac{1}{\alpha}\zuperint dx e^{-\alpha x} = \frac{1}{\alpha^2}
\end{gather*}
\begin{gather*}
    \zuperint dx x^2 e^{-\alpha x} = \frac{2}{\alpha^2}, \quad \zuperint dx x^3e^{-\alpha x} = \frac{3!}{\alpha^4}
\end{gather*}
\begin{gather*}
    \alpha = 1 \quad \zuperint dx x^n e^{-x} = n!
\end{gather*}
\begin{gather*}
    \boxed{\zuperint dx x^{p-1}e^{-\alpha x} = r(p) = (p-1)! \quad \underline{p>0}}
\end{gather*}
\begin{gather*}
    r(p+1) = \zuperint dx x^p e^{-x} = x^pe^{-x} + p\zuperint dx x^{p-1}e^{-x} = pr(p)
\end{gather*}
\begin{gather*}
    r(\frac{1}{2}) = \zuperint dx \frac{1}{\sqrt{x}}e^{-x} \quad u = \sqrt{x} \quad du = \frac{1}{2\sqrt{x}}dx\\
\end{gather*}
\begin{gather*}
    = 2\zuperint du e^{-u^2}
\end{gather*}
\begin{gather*}
    [r(\frac{1}{2})]^2 = 4[\zuperint du e^{-u^2}][\zuperint dv e^{-v^2}]
\end{gather*}
\begin{gather*}
    =4\zuperint du \zuperint dv e^{-(u^2+v^2)}\\
    u = r\cos\theta \qquad v = r\sin\theta
\end{gather*}
\begin{gather*}
    \frac{1}{r(z)}=\frac{1}{2\pi}\oint t^{-z}e^tdt  
\end{gather*}
\begin{gather*}
    r(p)r(z) = \zuperint dx x^{p-1}e^{-x}\zuperint dy y^{z-1}e^{-y}
\end{gather*}
\begin{gather*}
    x + y = u
\end{gather*}
\begin{gather*}
    y = u-x
\end{gather*}
\begin{gather*}
    \zuperint du \itint{0}{4}dx x^{p-1}(u-x)^{z-1}e^{-u}
\end{gather*}
\begin{gather*}
    x = ut \qquad = \zuperint du \itint{0}{1} dt yu^{p-1}t^{p-1}u^{z-1}(1-t)^{z-1}e^{-u}
\end{gather*}
\begin{gather*}
    \zuperint du u^{p+z-1}e^{-u}\itint{0}{1}dt t^{p-1}(1-t)^{z-1}
\end{gather*}
\begin{gather*}
    =r(p+z) \beta(p,z)
\end{gather*}

\bigskip
\hline
\begin{gather*}
    \itint{0}{1}dt t^{\alpha -1}(1-t)^{-\alpha}
\end{gather*}
\begin{gather*}
    t = \frac{v}{v+1} \qquad dt = \frac{(v+1)-v}{(v+1)^2} = \frac{1}{(v+1)^2}dv
\end{gather*}
\begin{gather*}
    \zuperint \frac{dv}{(v+1)^2}(\frac{v}{v+1})^{\alpha - 1}(\frac{1}{1+v})^{-\alpha}
\end{gather*}
\begin{gather*}
    = \zuperint \frac{dv}{(v+1)^2} \qquad v+1 = \zuperint dv \frac{v^{\alpha -1}}{v+1} = \frac{\pi}{\sin\pi \alpha}
\end{gather*}
\begin{gather*}
    r(\alpha)r(1-\alpha) = \frac{\pi}{\sin\pi\alpha} \qquad \alpha = \frac{1}{2}[r(\frac{1}{2})]^2 = \pi
\end{gather*}
\begin{gather*}
    \zuperint dx e^{-x^2} \qquad x^2 = y \quad 2xdx = dy \quad dx = \frac{dy}{2\sqrt{y}}
\end{gather*}

\hline
\begin{gather*}
    (1-x^2)\dv[2]{y}{x}-2x\dv[]{y}{x}+l(lH)y = 0
\end{gather*}
\begin{gather*}
     y = [a_0+a_1x+]
\end{gather*}
\end{mdframed}





















\begin{mdframed}


\underline{\textbf{Eigenvalue Equation:}}\\
$Lu(x) = \lambda u(x)$\\
Where $\lambda$ is a parameter. The solution of this equation consisten with B.C's will allow only certain values for $\lambda$ called \underline{eigenvalues} and correspondingly certain $u(x)$ called \underline{eigenfunctions}.\\
\underline{Ex.}\\
\begin{gather}
    \text{Let} \quad L = -\dv[2]{}{x} \quad \text{and} \quad \lambda = k^2 \quad \text{where k is real} \quad \therefore \quad \lambda \leq 0
\end{gather}
\begin{gather*}
    -\dv[2]{u(x)}{x} = k^2u(x)
\end{gather*}
\begin{gather*}
    u = a\sin kx + b\cos kx \quad \text{or} \quad = Ae^{ikx}+B{-ikx}
\end{gather*}
\underline{B.C.}
\begin{gather*}
    u(0) = 0 = u(L) \quad \text{periodic boundary}
\end{gather*}
\begin{align*}
    u(0) = 0 \quad &\implies \quad b = 0\\
    u(L) = 0 \quad &\implies \quad \sin RL = 0 \quad \therefore \quad RL = n\pi,\;\; R = \frac{n\pi}{L},\;\; n = 0,1,2,\cdots
\end{align*}
\begin{gather*}
    \therefore \quad u_n(x) = a\sin(\frac{n\pi x}{L}) \quad \text{is eigenfunction}
\end{gather*}
\begin{gather*}
    \lambda = R^2 = \frac{n^2\pi^2}{L^2} \quad (n = 0,1,2,\cdots) \quad \text{\underline{eigenvalues}}
\end{gather*}
\begin{align*}
    u_0(x) = &0 \quad \text{is trivial solution}\\
    u_1(x) = &a\sin(\frac{\pi x}{L})\\
    u_2(x) = &a\sin(\frac{2\pi x}{L})
\end{align*}
Since homogeneous problem, $a$ is undetermined.\\
Note $a$ can be complex if $\itint{0}{L}dxu^2(x) = 1$ $\abs{a}^2$ determined but phase of $a$ still arbitrary.\\
\bigskip
The operator L is called Hermitian if
\begin{gather*}
    \int u^*(x) Lv(x) = \int dx (v^*(x)Lu(x))^*
\end{gather*}
\begin{gather*}
    \text{Symbolic} \quad \implies \bra{u}L\ket{v} = \bra{v}L\ket{u}^*
\end{gather*}
Now let $L$ be hermitian and $u_i(x)$ and $u_j(x)$ be any two eigenfunctions with eigenvalues $\lambda_i$ and $\lambda_j$ 
\begin{gather*}
    \therefore \quad u_j^*Lu_i = \lambda_iu_i(x) \quad \text{and} \quad Lu_j(x) = \lambda_ju_j(x)
\end{gather*}
\begin{gather*}
    \therefore \quad u_j^*Lu_i = \lambda_iu_j^*u_i \quad \text{and} \quad h_j^*Lu_j = \lambda_ju_i^*u_j\\
    \text{or} \quad (u_i^*Ku_j)^* = \lambda_j^*u_iu_j^*
\end{gather*}
\begin{gather*}
    \therefore \quad \int dx \{u_j^*Lu_i - (u_i^*Lu_j)^*\} = (\lambda_i - \lambda_j^*)\int dx u_iu_j^*
\end{gather*}
Since L is Hermitian, $LHS = 0$
\begin{gather*}
    \therefore \quad (\lambda_i - \lambda_j^*) \intab dx u_i(x)u_j^*(x) = 0
\end{gather*}
The limits are appropriate for the problem.\\
If $i=j$ then $u_iu_i^*=\abs{u}^2$ $\therefore$ non negative.
\begin{gather*}
    \therefore \quad \lambda_i = \lambda_j^* \quad \therefore \lambda_i \quad \text{real (real eigenvalues for Hermitian L)}
\end{gather*}
\begin{gather*}
    \text{if} \quad i \neq j \quad \text{then} \quad \intab dx u_i(x)u_j^*(x) = 0
\end{gather*}
\begin{gather*}
    \therfore \quad u_i(x) \quad \text{are orthogonal}
\end{gather*}
\bigskip
\underline{Ex.}\\
The operator $L = -\dv[2]{}{x}$ is Hermitian\\
Let $u(x)$ ad $v(x)$ be any two functions such that 
\begin{gather}
    \begin{cases}
    u(0) = 0 = v(0)&\\
    u(L) = 0 = v(L)&
    \end{cases}
\end{gather}
we then prove that
\begin{gather*}
    -\itint{0}{L}dx u^*(x) = \dv[2]{u(x)}{x} = \itint{0}{L}dx[v^*(x) \dv[2]{u(x)}{x}]^* = - \itint{0}{L}\dv[2]{u^*}{x}v(x)
\end{gather*}
\begin{gather*}
    (-)LHS = \dv[]{v(x)}{x}\eval_0^L - \itint{0}{L}dx \dv[]{u^*(x)}{x}\dv[]{v(x)}{x} = -\itint{0}{L}dx\dv[]{u^*}{x}
\end{gather*}
\begin{gather*}
     = -\dv[]{u^*}{x}v\eval_0^L + \itint{0}{L}dx \dv[2]{u^*}{x}v = \itint{0}{L}dx \dv[2]{u^*}{x}v = RHS
\end{gather*}
Also
\begin{gather*}
    \lambda = k^2 = (\frac{n\pi}{L})^2 \quad \text{is real}
\end{gather*}
\begin{gather*}
    \itint{0}{L}dx \sin(\frac{n\pi x}{L}) \sin(\frac{m\pi x}{L}) = \frac{1}{2}\itint{0}{L}dx [\cos(n-m)\frac{\pi x}{L} - \cos (n+m)\frac{\pi x}{ L}]
\end{gather*}
\begin{align*}
    =0 \quad &\text{if} \quad n \neq m\\
    =\frac{1}{2}L \quad &\text{if} \quad n = m \neq 0
\end{align*}
\begin{gather*}
    \therefore \quad u_n(x) \quad \text{are orthogonal}
\end{gather*}
If
\begin{gather*}
    u_n(x) = \sqrt{\frac{2}{L}}\sin(\frac{n\pi x}{L})
\end{gather*}
then
\begin{gather*}
    \itint{0}{L}dxu^*(x)u_m(x) = \delta_{nm}
\end{gather*}
$u_n'$s are then called orthonormal.


\bigskip\hline
\underline{Sturm-Liouville Differential Equation:}
\begin{gather*}
    \dv[]{}{x}[p(x)\dv[]{u(x)}{x}] - q(x)u(x) + \lambda p(x)u(x) = 0
\end{gather*}
The operator
\begin{gather*}
    L = \dv[]{}{x}[p\dv[]{u}{x}]- q = p\dv[2]{u}{x}+p'\dv[]{}{x}-q \quad  \text{Is Hermitian}
\end{gather*}
\begin{gather*}
    \text{and} \quad \intab u_i^*(x)u_j(x)p(x) = 0 \quad \text{where} \quad u_i(x) \quad \text{ are eigenfunctions}
\end{gather*}
\underline{Eigenfunction Expansion}\\
If $f(x)$ satisfies same boundary conditions as the function $u(x)$ then one can write ( with $u_n's$ orthonormal eigenfunction)
\begin{gather*}
    f(x) = \sum_n C_nu_n(x)
\end{gather*}
\begin{gather*}
    C_m = \int dx' u_m^*(x')f(x')
\end{gather*}
\begin{gather*}
    \therefore \quad f(x) = \sum_n \int dx' u_m^*(x')f(x')u_n(x)
\end{gather*}
\begin{gather*}
    =\int dx'[\sum_n u_n^*(x')u_n(x)]f(x')
\end{gather*}
\begin{gather*}
    \sum_nu_n(x)u_n^*(x') = \delta(x-x') \quad \implies \quad \text{Completeness relation}
\end{gather*}
\bigskip
\underline{Inhomogeneous equation}
\begin{gather*}
    Lu(x) -\lambda u(x) = f(x)
\end{gather*}
Now
\begin{gather*}
    u(x) = \sum_n x_nu_n(x) \qquad L u(x) = \sum_n c_n\lambda_nu_n(x)
\end{gather*}
and let
\begin{gather*}
    f(x) = \sum_nd_nu_n(x)
\end{gather*}
We know $f(x) \; \therefore$ we know $d_n$ to determine $u(x) \; \therefore \; c_n$\\
Then
\begin{gather*}
    \sum_n c_n(\lambda_n - \lambda)u_n(x) = \sum_n d_nu_n(x)\\
    \therefore \quad c_n = \frac{d_n}{\lambda_n - \lambda} = \frac{\int dx' u_n^*(x')f(x')}{\lambda_n - \lambda}
\end{gather*}
\begin{gather*}
    \therefore \quad u(x) = \sum_n \int dx' \frac{u_n^*(x')f(x')}{\lambda_n - \lambda}u_n(x)\\
    =\int dx'[\sum_n\frac{u_n^*(x)u_n^*(x')}{\lambda_n--\lambda}]f(x')
\end{gather*}
\begin{gather*}
    \text{let} \quad G(x,x') = \sum_n \frac{u_n(x)u_n^*(x')}{\lambda_n - \lambda} \quad \text{called Green's function.}
\end{gather*}
\begin{gather*}
    \text{then} \quad u(x) = \int dx' G(x,x')f(x')
\end{gather*}
$G$ acts like a unit source.
\begin{gather*}
    Lu(x) -\lambda u(x) = \int dx' (L_x G - \lambda G)f(x')
\end{gather*}
\begin{align}
    \therefore \quad L_xH(x,x') - \lambda G(x,x') = \delta (x-x')
\end{align}
Equation satisfied by $G$ $\therefore$ $G$ satisfies same equation as $u$ except $f(x)$ replaced by $\delta(x-x')$. Also $G(x,x') = G(x',x)$


\bigskip
\begin{gather*}
    Lu = t \qquad LL^{-1} = L^{-1}L \qquad L^{-1}f(x) = \int dx'G(x,x')f(x')\\
    L^{-1}Lu = L^{-1}f \qquad u = L^{-1}f = \int dx'G(x,x')f(x')
\end{gather*}
\bigskip
\underline{Ex}\\
For the vibrating string problem
\begin{gather*}
    u_n(x) = \sqrt{\frac{2}{L}}\sin(\frac{n\pi x}{L})
\end{gather*}
\begin{gather*}
    \text{and} \quad G(x,x') = \frac{2}{L}\sum_n\frac{\sin(\frac{n\pi x}{L})\sin(\frac{n\pi x'}{L})}{(\frac{n\pi}{L})^2-k^2}
\end{gather*}
\underline{Another way}
\begin{gather*}
    \dv[2]{G(x,x')}{x}+k^2G(x,x') = \delta(x-x') \qquad G(0,x') = 0 = G(L,x')
\end{gather*}
Now integrating from $x = x' + \epsilon$ to $x=x'-\epsilon$, we find in the limit $\epsilon \to 0$
\begin{gather*}
    \dv[]{G}{x}\eval_{x'-\epsilon}^{x'+\epsilon} = 1
\end{gather*}
i.e.
\begin{gather*}
    \dv[]{G(x'+\epsilon,x')}{x} - \dv[]{G(x'-\epsilon,x')}{x} = 1 \quad \therefore \quad \text{a \underline{unit} jump in the derivation at} \quad x = x'
\end{gather*}
Integrating once again noting that $x'$ is fixed
\begin{gather*}
    \itint{0}{\epsilon'}d\epsilon \left[\dv[]{G(x'+\epsilon,x')}{x} - \dv[]{G(x'-\epsilon,x')}{x}\right] = 0 \quad \text{as} \quad \epsilon' \to 0
\end{gather*}
\begin{gather*}
    \therefore \quad G(x'+\epsilon, x') = G(x'-\epsilon, x') \quad \therefore \quad \text{G is continuous at} \quad x=x'
\end{gather*}
\begin{align*}
    \therefore \quad G(x,x') &= a\sin kx \quad 0<x<x'\\
     &= b\sin k(x-L) \quad x'<x<L
\end{align*}
\begin{align*}
    \text{continuity at} \quad x=x' \qquad &a\sin kx' = b\sink(x'-L)\\
    \text{jump in the derivative} \quad \qquad &bk\cos k(x'-L)-ak\cos kx' =1
\end{align*}
\begin{gather*}
    \therefore \quad a = \frac{b\sin k(x'-L)}{\sin kx'}
\end{gather*}
\begin{gather*}
    bk\left[\cos k(x'-L) - \frac{\sin k(x'-L)}{\sin kx'}\cos kx'\right] = 1
\end{gather*}
\begin{gather*}
    \therefore \quad b = \frac{\sin kx'}{k\sin kL}
\end{gather*}
\begin{gather*}
     a = \frac{\sin k(x'-L)}{k\sin kL}
\end{gather*}
\begin{gather*}
    \therefore \quad G(x,x') = \frac{\sin k(x'-L\sin kx}{k\sin kL}\theta(x'-x) + \frac{\sin kx'\sin k(x-L)}{k\sin kL}\theta(x-x')
\end{gather*}
\begin{gather*}
    \text{or} \quad = \frac{\sin k(x_>-L)\sin(kx_<)}{k\sin kL}
\end{gather*}
\begin{gather*}
    \therefore \quad \text{if} \quad \dv[2]{u(x)}{x}-k^2u(x) = f(x) \qquad 0<x<L \quad u(0) = 0 = u(L)
\end{gather*}
\begin{gather*}
    \text{Then} \quad u(x) = \itint{0}{R_L}dx'G(x,x')f(x')
\end{gather*}
\begin{gather*}
    =\itint{0}{x}dx' \frac{\sin kx' \sin k(x-L)}{k\sin kL}f(x') + \itint{x}{L}dx'\frac{\sin k(x'-L)\sin kx}{k\sin kL}f(x')
\end{gather*}
\underline{Further Examples}
\begin{gather*}
    \dv[2]{G(x,x')}{x} = \delta(x-x') \qquad 0<x<1\\
    \text{as Green's function for} \quad \dv[2]{u}{x} = f(x)
\end{gather*}
(i) $G(0,x') = 0 = G(1,x')$
\begin{align*}
\begin{rcases}
     G &= ax + b \quad 0<x<x' \quad \to \quad ax\\
      &= cx + d \quad x'<x<1 \quad \to \quad c(x-1)
\end{rcases} \quad \text{thm. B.C.'s}
\end{align*}
continuity at $x=x'$, $ax' = c(x'-1)$\\
jump in der. at $x=x'$, $c-a =1$
\begin{gather*}
    \therefore \quad c = a+1 \quad a = x' -1 \quad c = x'\\
    G(x,x') = x(x'-1)\theta(x-x') + x'(x-1)\theta(x-x')
\end{gather*}
(ii) $G(0,x') = 0 = G'(1,x')$
\begin{align*}
    g & = ax \quad (0,x')\\
    & = b \quad (x',1)
\end{align*}
cont. $b = ax'$\\
jump in $-a=1 \quad \therefore \quad a = -1 \quad b = -x'$
\begin{gather*}
    G(x,x') = -x\theta(x'-x) - x'\theta(x-x')
\end{gather*}
(iii) $G(0,x') = 0 = G'(0,x')$
\begin{align*}
    G & = 0 \quad (0,x')\\
    &=ax+b \quad (x',L)
\end{align*}
Cont. $ax'+b = 0$\\
der. $a=1 \quad b = -x'$
\begin{gather*}
    G = (x-x')\theta(x-x')
\end{gather*}
\begin{align*}
    \mqty(&G' = \theta(x-x') + (x-x')\delta(x-x') = \theta(x-x')\\
    &G'' = \delta(x-x'))
\end{align*}
\bigskip
\underline{General Problem}\\
Consider:
\begin{gather*}
    Lu(x) = f(x) \quad a<x<b\\
    \therefore LG(x,x') = \delta(x-x')
\end{gather*}
where
\begin{gather*}
    L = \frac{d}{dx} ( p \frac{d}{dx} ) - q
\end{gather*}
and where the following unmixed boundary condition is satisfied
\begin{gather*}
     u(a) = 0 \quad \text{or} \quad u'(a) = 0 \quad \text{and} \quad u(b) = 0 \quad \text{or} \quad u'(b) = 0
\end{gather*}
Now
\begin{gather*}
    \dv[]{}{x}\left[p(x)\dv[]{G(x,x')}{x}\right] - q(x)G(x,x') = \delta(x-x')
\end{gather*}
Integrating from $x'-\epsilon$ to $x'+\epsilon$
\begin{gather*}
    p(x)\dv[]{G(x,x')}{x}\eval_{x'-\epsilon}^{x'+\epsilon} = 1
\end{gather*}
\begin{gather*}
    \therefore \quad \dv[]{G(x'+\epsilon,x')}{x} - \dv[]{G(x'-\epsilon,x')}{x} = \frac{1}{p(x')}
\end{gather*}
$\therefore$ The jump in the derivative is $\frac{1}{p(x')}$ at $x = x'$\\
Now let 
\begin{gather*}
    G(x,x') \begin{cases}
=&v_1(x) \quad a<x<x'\\
=&v_2(x) \quad x'<x<b
\end{cases}
\end{gather*}
To satisfy continuity of $G$ at $x=x'$
\begin{gather*}
    G(x,x') \begin{cases}
=&v_1(x)v_2(x') \quad a<x<x'\\
=&v_2(x)v_2(x') \quad x'<x<b
\end{cases}
\end{gather*}
The jump in the derivative at $x=x'$ is
\begin{gather*}
    v_2'(x)v_1(x') - v_1'(x)v_2(x')
\end{gather*}
In order to get correct $G(x,x')$
\begin{gather*}
    \text{let} \quad v_2'(x)v_1(x') - v_1'(x)v_2(x') = \frac{W(v_1,v_2)}{p(x')} \quad \text{at} \quad x = x'
\end{gather*}
The
\begin{align*}
    G(x,x') & = \frac{v_1(x)v_2(x')}{W} \quad a<x<x'\\
    & = \frac{v_2(x)v_1(x')}{W} \quad x'<x<b
\end{align*}
is the correct $G$.\\
i.e
\begin{gather*}
    G(x,x') = \frac{v_1(x)v_2(x')\theta(x'-x)+v_2(x)v_1(x')\theta(x-x')}{W(v_1,v_2)}
\end{gather*}
The quantity $W(v_1,v_2)$ is called the Wronskian of the differential equation $Lv=0$, and is a \underline{constant} (i.e. independent of $x'$)\\
\bigskip
\underline{Proof:}
\begin{gather*}
    \dv[]{}{x}[p\dv[]{v_1}{x}]-qv_1 = 0\\
    \therefore \quad [pv_1']'-qv_1 = 0\\
    \text{also} \quad [pv_2']'-qv_2 = 0\\
    \therefore \quad v_1[pv_2']'-v_2[pv_1']' = 0
\end{gather*}
or
\begin{gather*}
    \dv[]{}{x}[p(v_1v_2'-v_2v_1')] = 0\\
    \therefore \quad p[v_1v_2'-v_2v_1'] = const. = W(v_1,v_2) \quad QED.
\end{gather*}
\hline
\begin{gather*}
    f(x) = \sum_nc_nu_n(x)\\
    c_n = \int dx' p(x')f(x')u_m^*(x')\\
    f(x) = \int dx' f(x')\sum_np(x')u_n(x)u_n^*(x')\\
    \therefore \quad \sum_np(x')u_n(x)u_n^*(x') = \delta(x-x')
\end{gather*}
and
\begin{gather*}
    G(x,x') = \frac{\sum_n u_n(x)u_n^*(x')}{\lambda_n - \lambda}
\end{gather*}
\bigskip


\underline{Infinite Intervals}\\
(i)
\begin{gather*}
    \dv[2]{u}{x} + ku^2 = 0 \qquad -\infty < x< \infty\\
    u(x) = Ae^{ikx}+Be^{-ikx}
\end{gather*}
where continuous values of $k$ are allowed $(-\infty < k < \infty)$\\
If the B.C. is that $u \sim e^{ikx}$ then the normalized eigenfunction is given by
\begin{gather*}
    u_k(x) = \frac{1}{\sqrt{2\pi}}e^{ikx} \quad \text{'outgoing' wave (mult. by $e^{-i\omega t})$}
\end{gather*}
hence
\begin{gather*}
    \superint dx u_{k'}^*(x)u_k(x) = \frac{1}{2\pi}\superint dx e^{i(k-k')x} = \delta(k-k')
\end{gather*}
\underline{Green's Function}
\begin{gather*}
    \dv[2]{G(x,x')}{x}+k^2G(x,x') = \delta(x-x')
\end{gather*}
\begin{gather*}
    G_k(x,x') = -\frac{1}{2\pi}\superint dk' \frac{e^{ik'(x-x')}}{(k')^2-(k^2+i\epsilon)}
\end{gather*}
the $i\epsilon$ prescription is needed to ensure outgoing b-wave boundary condition (i.e. $G \sim e^{ikx}$ as $x \to +\infty$)\bigskip\\
Poles at $(k')^2 = k^2 +i\epsilon$
\begin{align*}
    &k_1 = +\sqrt{k^2 + i\epsilon} = k(1+\frac{i\epsilon}{2k^2} = k + i\epsilon'\\
    &k_2 = -\sqrt{k^2 + i\epsilon} = -k(1+\frac{i\epsilon}{2k^2} = -k - i\epsilon' \qquad (k>0) 
\end{align*}
%%GRAPH
\begin{gather*}
    G_k = -\frac{1}{2\pi}\superint \frac{dk'e^{ik'(x-x')}}{(k'-k_1)(k'-k_2)}
\end{gather*}
For $x-x' > 0$, take the upper contour
\begin{gather*}
    G_k(x,x') = -\frac{1}{2\pi}2\pi i \frac{e^{ik_1(x-x')}}{k_1-k_2} = -\frac{i}{2k}e^{ik(x-x')}
\end{gather*}
For $x-x'<0$, take the lower contour
\begin{gather*}
    G_k(x,x') = -\frac{1}{2\pi}2\pi i \frac{e^{ik_2(x-x')}}{k_2-k_1} = -\frac{i}{2k}e^{-ik(x-x')}
\end{gather*}
Note that this $G$ is cont. at $x=x'$ and has a jump in derivative $=1$ at $x=x'$, as it should.\\
\bigskip
If
\begin{gather*}
    \dv[2]{u(x)}{x} + k^2u(x) = f(x)\\
\end{gather*}
\begin{gather*}
    u(x) = \superint dx' G(x,x')f(x')
\end{gather*}
\begin{gather*}
     =-\frac{1}{2\pi}\superint dx' \superint dk' \frac{e^{ik'(x-x')}f(x')}{(k')^2-(k^2+i\epsilon)}
\end{gather*}
\begin{gather*}
    = -\frac{1}{2\pi}\superint dk'\frac{e^{ik'x}F(k')}{(k')^2-(k^2+i\epsilon)} \qquad \qquad F(k') = \frac{1}{\sqrt{2\pi}}\superint dx' e^{ik'x'}f(x')
\end{gather*}
\bigskip
(ii)
\begin{gather*}
    \dv[2]{u}{x} + k^2u = 0 \qquad 0<x<\infty\\
    u(0) = 0
\end{gather*}
Then
\begin{gather*}
    u(x) = A\sin kx \qquad -\infty <k < \infty      
\end{gather*}
Normalized
\begin{gather*}
    u_k(x) = \sqrt{\frac{2}{\pi}}\sin kx\\
    \zuperint dx u_{k'}^*(x)u_k(x) = \delta(k'-k)
\end{gather*}
\begin{gather*}
    L = -\frac{1}{w}\dv[]{}{x}\left[p\dv[]{}{x}\right]+ q
\end{gather*}
\begin{gather*}
    \bra{v}L\ket{u}-\bra{u}L\ket{v} = -p\left[v^*u'-u^*v'\right]_0^1
\end{gather*}
\begin{gather*}
    \left[vu'-u^*v'\right]_0^1 = \text{Wronskian}
\end{gather*}
\Circled{1} \quad Then if boundary condition \underline{un-mixed} at 0 and 1.
\begin{align*}
    &\alpha_0u(0) + \beta_0u'(0) = 0\\
    &\alpha_1u(1) + \beta_1u'(1) = 0
\end{align*}
\Circled{2} \quad Periodic B.C.
\begin{align*}
    &u(0) = u(1)\\
    &u'(0) = u'(1)
\end{align*}
\Circled{3}
\begin{align*}
    &a_1u(0) + b_1u'(0) + c_1u(1)+d_1u'(1) = 0\\
    &a_2u(0) + b_2u'(0) + c_2u(1)+d_2u'(1) = 0
\end{align*}
\hline
\begin{gather*}
    Lg + k^2g = \delta(x-x')
\end{gather*}
\begin{gather*}
    \text{let} \quad g = g_0 + h\\
    \text{such that} \quad Lg_0 = \delta(x-x') \quad \text{then} \quad g_0 \quad \text{ is continuous}
\end{gather*}
\begin{gather*}
    (L+k^2)h = -k^2g_0
\end{gather*}
RHS is continous $\therefore$ h is continuous $\therefore$ g is continuous.
\begin{gather*}
    f(x) = \sum_n \alpha_n\sin u_n(x)\\
    \sum_n\int dx' f(x')u_n(x')u_n(x)
\end{gather*}
\bigskip
\underline{Eigenvalue Problem and Green's Functions}\\
First consider only function of one variable $x$\\
Let $u(x)$ be a function. A \underline{linear} differential operator $L$.
\begin{gather*}
    L = A + B\dv[]{}{x} + c\dv[2]{}{x} + \cdots
\end{gather*}
such that
\begin{gather*}
    Lu(x) = Au + B \dv[]{u}{x} + c\dv[2]{u}{x} + \cdots
\end{gather*}
This $L$ is called linear because $u$ appears to first degree (no $u^2$ terms.)\\
Now
\begin{gather*}
    Lu(x) = 0
\end{gather*}
is called a homogeneous differential equation\\
and
\begin{gather*}
    Lu(x) = f(x)
\end{gather*}
is called an inhomogeneous differential equation.\\
If terms up to $\dv[n]{u}{x}$ are present (and no more), then $n$th order differential equation.\bigskip\\
\indent Consider 2nd order differential equation. The solution will contain two arbitrary numbers corresponding say to the value and the derivatives $u(0), \; u'(0)$ resp. The higher derivatives are determined in terms of these two:\\
\indent If $u(0)$, and $u'(0)$ are given in terms of certain conditions (called boundary conditions) the function $u(x)$ is completely determined (for the 2nd order case). Other type of B.C. may be $u(0)$, and $u(1)$ specified etc.\\
\indent If B.C.'s are homogeneous e.g. $u(0) = 0 = u(1)$ then we are dealing with a homogeneous problem.
\indent Then if $u_1(x)$ and $u_2(x)$ are two solutions then so are $c_1u_1(x)$ and $c_2u_2(x)$, and $c_1u_1(x) + c_2u_2(x)$.\\
\indent A problem is inhomogeneous if the differential equation is inhomogeneous but the B.C. \underline{may} be homogeneous \Circled{or} if the differential equation is homogeneous but the B.C.'s are \underline{not} homogeneous.\bigskip\\


\underline{\underline{Ex}}
\begin{gather*}
    \dv[2]{u}{x} = 0 \implies u(x) = ax + b
\end{gather*}
\begin{align*}
    &u(0) = 0 = u(1) \implies u(x) = 0\\
    &u(0) = 0 = u'(1) \implies u(x) = 0\\
    &u(0) = 0, \quad u(1) = 1 \implies u(x) = x 
\end{align*}





\end{mdframed}




\begin{mdframed}
\begin{center}
    \textbf{Expansion in Terms of Orthonormal Functions}
\end{center}
(i) If $f(x) = 0$ at $x = 0$ and at $x = L$ then one can write:
\begin{gather*}
    f(x) = \sum_n c_n\sin (\frac{n\pi x}{L}) \quad \text{Fourier (sine) series}
\end{gather*}
\begin{gather*}
    c_m = \frac{2}{L}\itint{0}{L}dx'\sin(\frac{n\pi x'}{L})f(x')
\end{gather*}
\indent \underline{In general} if $\phi_n(x)$ are orthonormal functions in the interval $(a,b)$ then one can express a function $f(x)$ satisfying appropriate b.c.'s as
\begin{gather*}
    f(x) = \sum_n c_n \phi_n(x)
\end{gather*}
The conditions under which this is possible is understood as follows:
\begin{gather*}
    \text{Let} \quad S_n(x) = \sum_{n=1}^m c_n\phi_n(x) \qquad c_n\intab dx f(x)\phi_n(x)
\end{gather*}
Then the above expansion is valid if $S_n(x)$ converges to $f(x)$ as $m\to \infty$ i.e. if
\begin{gather*}
    \lim_{m\to\infty} \intab dx \left[f(x) - S_m(x)\right]^2 = 0
\end{gather*}
\begin{gather*}
    LHS = \intab dx \left[f^2(x) - 2f(x)S_m(x)+S^2(x)\right]
\end{gather*}
\begin{gather*}
    =\intab dx \left[f^2(x) - 2\sum_n c_n \intab f(x) \phi_n(x) + S_m^2(x)\right] = \intab dx \left[f^2 - 2\sum_n c_n^2 + \sum_n c_n^2\right]
\end{gather*}
\begin{gather*}
    \therefore \quad \intab dx f^2(x) = \sum_nc_n^2 \quad \text{Parseval's Theorem}
\end{gather*}
(ii) \underline{General Theorem for Fourier Series}\bigskip\\
If $f(x)$ satisfies the condition : a) $f(x+2\pi) = f(x)$ for all values of $x$ and b) if $f(x)$ is sectionally continuous in the interval $(-\pi,\pi)$. Then the Fourier series
\begin{gather*}
    \frac{1}{2}a_0 + \sumni(a_n\cos nx + b_n \sin nx)
\end{gather*}
where
\begin{gather*}
    a_n = \frac{1}{\pi}\itint{-\pi}{\pi}f(x) \cos nx dx
\end{gather*}
\begin{gather*}
    b_n = \frac{1}{\pi} \itint{-\pi}{\pi}f(x) \sin nx dx
\end{gather*}
converges to the value 
\begin{gather*}
    \frac{1}{2}\left[f(x+0) + f(x-0)\right]
\end{gather*}
at every point where $f(x)$ has a right and left hand derivative.
(iii) \underline{Infinite interval}\bigskip\\
\indent Let $f(x)$ be sectionally continuous in ever finite interval and let $\superint dx\abs{f(x)}$ converge.\\
\indent Then at every point $x (-\infty < x< \infty)$ where $f(x)$ has right and left hand derivative
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dx e^{-ikx}F(k)
\end{gather*}
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\pi}}\superint dx e^{ikx}f(x)
\end{gather*}
\indent For a physical problem of a sphere with external change and $\phi(x)$ with Dirichlet B.C. (i.e. $\phi$ given on surface) one gets 
\begin{gather*}
    \phi(\Bar{x}) = \frac{1}{4\pi}\oint_s da' \phi(x')\pdv{G}{n'} 
\end{gather*}
\underline{Orthogonal Functions}\bigskip\\
\indent An arbitrary continuous function in the range $(a,b)$ van be expanded in terms of a complete set of orthonormal functions
In a given interval $(a,b)$
\begin{gather*}
    f(x) = \sum_{n = 0}^\infty a_nu_n(x) \quad - \quad \text{Mathematical statement}
\end{gather*}
where
\begin{gather*}
    \intab dx' u_n^*(x')u_m(x') = \delta_{nm}
\end{gather*}
This statement is similar to writing a vector in terms of its components.\\
The $u_n$'s form a complete set.
\begin{gather*}
    a_n = \intab dx' f(x')u_n^*(x')
\end{gather*}
\begin{gather*}
    f(x) = \sum_{n = 0}^\infty \left[\intab dx' f(x') u_n^*(x')\right]u_n(x)
\end{gather*}
\begin{gather*}
    =\intab dx' f(x') \left[\sum_{n=0}^\infty u_n^*(x')u_n(x)\right]
\end{gather*}
\begin{gather*}
    \therefore \quad \sum_{n=0}^\infty u_n^*(x)u_n(x) = \delta(x'-x) \quad \text{completeness}
\end{gather*}
An example of orthonormal function is 
\begin{gather*}
    \sqrt{\frac{2}{a}}\sin (\frac{2\pi m x}{a}), \quad \sqrt{\frac{2}{a}}\cos (\frac{2\pi m x}{a})
\end{gather*}
in the interval $(-a/2,a/2)$
\begin{gather*}
    \sqrt{\frac{1}{L}}\sin (\frac{\pi m x}{L}), \quad \sqrt{\frac{1}{L}}\cos (\frac{\pi m x}{L}) \quad (-L,L)
\end{gather*}
\begin{gather*}
    f(x) = \sum_{m = 0}^\infty a_m u_m(x)
\end{gather*}
\begin{gather*}
    a_m = \itint{-a/2}{a/2} dx' f(x') \sqrt{\frac{2}{a}}\mqty(&\cos(\frac{2\pi m x}{a}) \\ &\sin(\frac{2\pi m x}{a}))
\end{gather*}
Let
\begin{align*}
    \sqrt{\frac{2}{a}}a_m &= A_m \text{ for cosine function with $m\neq 0$} \quad A_m = \frac{2}{a}\itint{-a/2}{a/2}x'f(x')\cos\frac{2\pi mx}{a}\\
    & = \frac{1}{2}A_0 \quad "\qquad" \quad \text{with $m = 0$}
\end{align*}
\begin{align*}
    \sqrt{\frac{2}{a}}a_m &= B_m \text{ for sine function wiht $m \neq 0$} \quad B_m = \frac{2}{a}\itint{-a/2}{a/2}x'f(x')\sin\frac{2\pi mx}{a}\\
    & = 0 \qquad \text{with $m = 0$}
\end{align*}
\begin{gather*}
    f(x) = \frac{1}{2}A_0 + \sum_{m=1}^\infty \left[A_m \cos(\frac{2\pi mx}{a}) + B_m \sin (\frac{2\pi mx}{a})\right]
\end{gather*}
Instead of sine and cosine we van start with $e^{i\frac{2\pi mx}{a}}$. Since they are not symmetric in $m$ they must sum with positive and negative $m$. The orthonormal functions are 
\begin{gather*}
    u_m(x) = \frac{1}{\sqrt{a}}e^{i\frac{2\pi mx}{a}}
\end{gather*}
and one can write 
\begin{gather*}
    f(x) = \sum_{m = -\infty}^\infty A_m u_m(x)
\end{gather*}
\begin{gather*}
    A_m = \itint{-a/2}{a/2}dx'f(x')u_m^*(x) = \frac{1}{\sqrt{a}}\itint{-a/2}{a/2}dx'f(x')e^{-i\frac{2\pi mx }{a}}
\end{gather*}
orthonormality condition
\begin{gather*}
    \itint{-a/2}{a/2}dx e^{-i\frac{2\pi m^* x}{a}}e^{i\frac{2\pi n x}{a}} = 0 \quad \text{if} \quad m \neq n
\end{gather*}
Let $a\to \infty$ $\frac{2\pi m}{a} = k$ then as m goes from $-\infty$ to $+\infty$ for any fixed $a$, n also goes from $-\infty$ to $+\infty$.\bigskip\\
Now the orthogonality relation becomes
\begin{gather*}
    \itint{-a/2}{a/2}dx e^{i\frac{2\pi x}{a}(n-m)} = \frac{1}{i(2\pi/a)(n-m)}\left(e^{i\pi(n-m)}-e^{-i\pi(n-m)}\right)
\end{gather*}
\begin{gather*}
    =\frac{a}{\pi(n-m)}\sin(n-m)\pi
\end{gather*}
\begin{gather*}
    \lim_{a\to \infty} \itint{-a/2}{a/2}dxe^{ikx(k-k')}\\
    \to \infty \quad \text{for} \quad n = m \quad \text{i.e.} \quad k=k'\\
    \to 0 \quad \text{for} \quad n \neq m
\end{gather*}
$\therefore$ like any $\delta$ function\\
\Circled{1}
\begin{gather*}
    \text{let} \quad \lim_{a\to\infty}\itint{-a/2}{a/2}dx e^{i(k-k')x} = \lambda \delta(k-k')
\end{gather*}
Integrate both sides over k and show $\lambda = 2\pi$
\begin{gather*}
    \sum_m \quad \to \quad \superint dm = \frac{a}{2\pi}\superint dk
\end{gather*}
\begin{gather*}
    A_m \quad \to \quad \sqrt{\frac{2\pi}{a}}A(k)
\end{gather*}
\begin{gather*}
    \therefore \quad f(x) = \frac{1}{\sqrt{a}}\frac{a}{2\pi}\superint dk \sqrt{\frac{2\pi}{a}}A(k) e^{-ikx} = \frac{1}{\sqrt{2\pi}}\superint dk A(k) e^{ikx}
\end{gather*}
\begin{gather*}
\sqrt{\frac{2\pi}{a}}A(k) = \frac{1}{\sqrt{a}}\superint dx' f(x') e^{ikx'}    
\end{gather*}
\begin{gather*}
    \therefore \quad A(k) = \frac{1}{\sqrt{2\pi}}\superint dx' f(x')e^{-ikx'}
\end{gather*}
If $f(x) = e^{ikx'}$ then $\frac{A(k)}{\sqrt{2\pi}} = \delta(k-k')$
\begin{gather*}
    \therefore \quad \sqrt{2\pi\delta(k-k')} = \frac{1}{\sqrt{2\pi}}\superint dx e^{ik'x}e^{-ikx}
\end{gather*}
\begin{gather*}
    \therefore \quad \frac{1}{2\pi}\superint dx e^{i(k-k')x} = \delta(k-k') \quad \text{orthogonality}
\end{gather*}
\begin{gather*}
    \frac{1}{2\pi} \superint dk e^{ik(x-x')} = \delta(x-x') \quad \text{completeness}
\end{gather*}
The functions $\frac{e^{ikx}}{\sqrt{2\pi}}$ are "orthogonal" functions.\bigskip\\
Read section 2-6 from page 35 to page 37.\\
\underline{Prob} Find the $G(\Vec{x},\Vec{x}')$ for an infinite plane\\
\underline{Prob} Do \#2-4
\hline

\noindent Q. What determines the interval of interest?\\
A. Boundary conditions. If the problem has b.c. at $x=a$ and $a=b$ then $(a,b)$ is the interval to choose.\bigskip\\

\noindent Q. How \underline{did} we know that $\sin(\frac{2m\pi x}{a}),\cos(\frac{2m\pi x}{a})$ form an orthogonal set?\\
A. From the general theorem on eigenvalue equation $\dv[2]{u_n}{x}=\lambda_nu_n$ with boundary condition such as $u(a) + u'(a) = 0 = \alpha u(b) + \beta u'(b)$. For an interval $(-L,L)$ the sine/cosine functions are $\sin(\frac{n\pi x}{L})\cos(\frac{n\pi x}{L})$ for interval $(0,L)$ \underline{only} $\sin(\frac{n\pi x}{L})$ is needed for orthonormal function. \bigskip\\

\noindent Q. Since, besides sines and cosines, there are many other orthogonal sets (e.g. Legendre polynomials etc.) which set should we use?\\
A. The one which makes the physics of the problem most transparent. Depends on which equation - Sturm Louiville Equation - comes closest to the equation you are solving.\bigskip\\

Differential Equation solving by using orthogonal functions:\\
\underline{Consider}
\begin{gather*}
    \dv[2]{f(x)}{x} = g(x) \quad \text{where $g(x)$ is given}
\end{gather*}
Express $f(x) = \sum_n a_nu_n (x)$ where $u_n$'s are appropriate for the b.c. With the same $u_n(x)$ write $g(x) = \sum_n b_nu_n(x)$. The $u_n$'s are say solutions of $\dv[2]{u_n}{x} = \lambda_nu_n$.
\begin{gather*}
    \text{then} \quad \sum_n a_n\lambda_nu_n(x) = \sum_nb_nu_n(x)\\
    \therefore \quad a_n = \frac{b_n}{\lambda_n} \quad \text{where $b_n$ and $\lambda_n$ are known}
\end{gather*}







\end{mdframed}












%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBB      OO        OO    OO        OO    KK        KK
%%BBBBBBB       OO        OO    OO        OO    KKKKKKKKKKK         PG. 732
%%BBBBBBBB      OO        OO    OO        OO    KKKKKKKKKKK
%%BB    BBB     OO        OO    OO        OO    KK        KK
%%BB    BBB     OO        OO    OO        OO    KK         KK
%%BBBBBBBBB     OOOOOOOOOOOO    OOOOOOOOOOOO    KK          KK
%%BBBBBBBB        OOOOOOOO        OOOOOOOO      KK          KK








\begin{mdframed}
\textbf{Fourier Series}\\
\textbf{Fourier Transforms}\\
\textbf{Laplace Transforms}\\
\begin{center}
    \textbf{231 B\\ Fourier Series\\ 06/17/2012}
\end{center}
\begin{gather*}
    f(x) = \frac{a_0}{2} + \sumni(a_n\cos nx + b_n \sin nx)
\end{gather*}
Almost any function, including discontinuous functions, can be represented by the above series in some sense the infinite number of terms of the Taylor expansion are already included. 
\begin{align*}
    &\text{(a) Valid only in finite interval.}\\
    &\text{(b) Periodic } f(x+2\pi) = f(x)
\end{align*}
Consider, for convenience, the period: ($-\pi$, $\pi$).\\
Using the relation:
\begin{align*}
    &\int_{-\pi}^\pi \mqty*(\sin nx \\ \cos nx) \mqty*(\sin mx \\ \cos mx) = 0 \quad n \neq m\\
    &\int_{-\pi}^\pi dx \sin^2 nx = \pi = \int_{-\pi}^\pi dx \cos^2 nx\\
    &\frac{a_0}{2} 2 \pi = \int_{-\pi}^\pi dx f(x)\\
    &a_p \pi = \int_{-\pi}^\pi dx f(x) \cos px\\
    &a_n = \frac{1}{\pi} \int_{-\pi}^\pi dx f(x) \cos nx \qquad 
    b_n = \frac{1}{\pi}\int_{-\pi}^\pi dx f(x) \sin nx
\end{align*}

The interval above is $-\pi$ to $\pi$. But other interval can be used. E.G.
\begin{gather*}
    f(x+T) = f(x)
\end{gather*}
\begin{align*}
    &f(x) = \frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{T})} + b_n \sin{(\frac{2\pi nx}{T})}\right] \qquad \text{Period} = T \quad \text{Interval} = (0,T)\\
    &\text{OR}\\
    &f(x) = \frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{L})} + b_n \sin{(\frac{2\pi nx}{L})}\right] \qquad \text{Period} = 2L \quad \text{Interval} = (-L,L)
\end{align*}

Example:
\begin{gather*}
    \text{Discontinuous Function, } f(x) = \begin{cases}
    -1 , \quad x<0\\
    +1 , \quad x>0
    \end{cases}
\end{gather*}
Let $g(x)$ be its Fourier Transform:
\begin{gather*}
    g(x) = \frac{a_0}{2} + \sumni [a_n\cos nx + b_n \sin nx]
\end{gather*}

and it can be shown that:
\begin{gather*}
    a_0 = 0 = a_n \qquad b_n = \begin{cases} \frac{4}{n\pi}, \quad n \;\; odd \\ 0, \quad otherwise\end{cases}
\end{gather*}
\begin{gather*}
    g(x) = \frac{4}{\pi}\sum_{n=odd}\frac{1}{n}\sin nx
\end{gather*}
\begin{gather*}
    g(x) \text{ Converges to } \begin{cases}
    +1, \quad 0<x<\pi\\
    -1, \quad -\pi<x<0\\
    0, \quad x=0
    \end{cases}
\end{gather*}
$\implies$ Fourier series converges to :
\begin{gather*}
    \frac{1}{2}[f(x_0+)+f(x_0-)]    
\end{gather*}
Definitions:\\
(a) Piece-wise continuous in $a\leq x \leq b$\\
\begin{center}
If the interval $(a,b)$ can be divided into a finite number of sub-intervals such that in each sub-interval is continuous and $f(x)$ has finite limits. For every $x_i$ i.e. $f(x_i+0)$ and $f(x_i-0)$ exists for all $i = 1, \; \cdots n$ and $f(a+0)$ and $f(b-0)$ exist. $f(x) = \frac{1}{x-a}$ in $(a,b)$, $f(x) = \frac{1}{x}$ in $(-L,L)$ ruled out, $f(x) = \begin{cases} +1, \quad x \geq 0 \\ -1, \quad x < 0 \end{cases}$ not ruled out.\\
\end{center}

\noindent(b) Piece-wise smooth in $a\leq x \leq b$\\
\begin{center}
If $f(x)$ is piece-wise continuous and $f'(x)$ is also piece-wise continuous\\
\end{center}

\noindent(c) Piece-wise very smooth in $a\leq x \leq b$\\
\begin{center}
If $f(x)$ is piece-wise cede smooth and $f''(x)$ is piece-wise continuous.\\
\end{center}

Theorem:
\begin{center}
    If $f(x)$ is piece-wise very smooth in the interval $(-L,L)$ then its Fourier Series converges to
    \begin{gather*}
        \frac{1}{2}[f(x-0)+f(x+0)] \quad for \quad -L<x<L\\
        \frac{1}{2}[f(-L+0)+f(L-0)] \quad for \quad x=-L \;\; or \;\; x=L
    \end{gather*}
    (Note: the Fourier series is $f(x) = \frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{L})} + b_n \sin{(\frac{2\pi nx}{L})}\right]$, generally argument = $\frac{2\pi nx}{\text{length of the interval}} = \frac{2n\pi x}{\Delta L}$ that was $f(x+\Delta L) = f(x)$)\\
    (Also note: there is a function $f(x)$ and there is 'its' Fourier series, the two need not to be identical.)\\
\end{center}



\bigskip
\hline
The Fourier series satisfies the Parseval's Theorem:
\begin{gather*}
    \frac{a_0^2}{2} \sumni (a_k^2 + b_k^2) = \frac{1}{L} \int_{-L}^L dx f^2(x)
\end{gather*}
\textbf{Proof:}\\
\noindent First consider:
\begin{gather*}
    s_m = \frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{L})} + b_n \sin{(\frac{2\pi nx}{L})}\right]
\end{gather*}
then assume,
\begin{gather*}
    \lim_{m\to \infty}\int_{-L}^L dx [f(x)-s_m(x)]^2 = 0
\end{gather*}
and consequently the Parseval's theorem.\\
Actually one need not assume $\lim_{m\to \infty}\int_{-L}^L dx [f(x)-s_m(x)]^2 = 0$, it can be proved for Fourier series.\\
Fourier series in complex form:
\begin{gather*}
   f(x) = \frac{a_0}{2} \sumni \left[ a_n \cos{(\frac{2n\pi x}{L})} + b_n \sin{(\frac{2\pi nx}{L})}\right] \qquad -L\leq x \leq L
\end{gather*}
\begin{gather*}
    \cos{\frac{n\pi x}{L}} = \frac{1}{2}\left(e^{i\frac{n\pi x}{L}} + e^{-i\frac{n\pi x}{L}} \right)
\end{gather*}
\begin{gather*}
    \sin{\frac{n\pi x}{L}} = \frac{1}{2i}\left(e^{i\frac{n\pi x}{L}} + e^{-i\frac{n\pi x}{L}} \right)
\end{gather*}
\begin{gather*}
    f(x) = \frac{a_0}{2} + \sumni \frac{1}{2}(a_n - i b_n)e^{i\frac{n\pi x}{L}} + \frac{1}{2}(a_n + i b_n)e^{-i\frac{n\pi x}{L}}
\end{gather*}
while,
\begin{gather*}
    f(x) = \sumni c_n e^{i\frac{n\pi x}{L}}
\end{gather*}
then,
\begin{gather*}
    c_0 = \frac{a_0}{2}, \quad c_n = \begin{cases}
    \frac{1}{2}(a_n - ib_n), \quad n>0\\
    \frac{1}{2}(a_n + ib_n), \quad n<0
    \end{cases}
\end{gather*}
For
\begin{gather*}
    n = p \quad \int_{-L}^L dx \cdot 1 = 2L
\end{gather*}
\begin{gather*}
    \int_{-L}^L dx e^{i(n-p)\frac{\pi x}{L}} = 2L\delta_{n,p}
\end{gather*}
\begin{gather*}
    c_n = \frac{1}{2L}\int_{-L}^L dx f(x) e^{-i\frac{n \pi x}{L}}
\end{gather*}
One can show that directly from $a_n$ and $b_n$.\\
\bigskip\hline

\textbf{Use of Fourier series in solution of problems.}\\
e.g.
\begin{gather*}
    \dv[2]{y}{x} = f(x) \quad \text{in} \quad (-\pi, \pi)
\end{gather*}
\begin{gather*}
    f(x) = \frac{a_0}{2} + \sum_n a_n \cos nx + b_n \sin nx = \sum_n A_n e^{in\pi x}
\end{gather*}
\begin{center}
$a_n$, $b_n$, and $A_n$ are known since $f(x)$ is known.\\
\end{center}
\indent Similarly, 
\begin{gather*}
    y(x) = \frac{c_0}{2} + \sumni x_n (-n^2)e^{-inx}
\end{gather*}
\begin{gather*}
    f(x) = A_0 + \sumni A_n e^{inx}\\
    A_0 = 0 \quad c_n = \frac{-A_n}{n^2} \quad c_0 = A_0 = 0
\end{gather*}
\begin{gather*}
    y(x) = - \sum_n \frac{A_n}{n^2}e^{inx}
\end{gather*}
\end{mdframed}


%%LL           AAAAA     PPPPPPPP
%%LL          AA   AA   PP      PP
%%LL         AA     AA  PP      PP
%%LL        AA       AA PP      PP
%%LL        AA       AA PP      PP
%%LL        AAAAAAAAAAA PPPPPPPPP
%%LL        AA       AA PP
%%LL        AA       AA PP
%%LL        AA       AA PP
%%LLLLLLLLL AA       AA PP
%%LLLLLLLLL AA       AA PP


\begin{mdframed}
\bigskip
\begin{center}
    \textbf{Laplace Transform}
\end{center}

In math book:
\begin{gather*}
    f(x) = \frac{1}{2\pi} \superint dk F(k) e^{ikx}, \quad F(k) = \superint dx f(x) e^{ikx} \;\;\;\; (\text{Fourier Transform})
\end{gather*}

Laplace Transform:
\begin{gather*}\label{laplacetransform}
    L(s) = \zuperint dx f(x) e^{-sx}
\end{gather*}
\indent Laplace of $f(x)$:
\begin{gather*}
    e^{-sx}\theta(x)f(x)
\end{gather*}
\begin{align*}
    F(k) &= \superint e^{-cx} \theta(x) f(x) e^{-ikx}\\
    &= \zuperint dx f(x) e^{(c+ik)x}
\end{align*}
Let $c+ik = s$
\begin{gather*}
    F(k) \to L(s)
\end{gather*}
\begin{gather*}
    L(s) = \zuperint dx f(x) e^{-sx} \quad s = c+ik
\end{gather*}
\begin{align*}
    e^{-cx}\theta(x) f(x) &= \frac{1}{2\pi}\superint dk F(k) e^{ikx}\\
    \theta(x)f(x) &= \frac{1}{2\pi} \superint dkF(k) e^{(c + ik)x}
\end{align*}
\begin{gather*}
    s = c + ik \quad ds = idk 
\end{gather*}
Limits:
\begin{gather*}
    s: \quad c-i\infty \to c+i\infty
\end{gather*}

\begin{gather*}
    \theta(x)f(x) = \frac{1}{2\pi i } \int_{c-i\infty}^{c+i\infty} ds L(s) e^{sx}
\end{gather*}
\bigskip
\hline

\textbf{\underline{Examples:}}\\
\indent (a)
\begin{align*}
    f(x) = 1 \qquad L(s) = \zuperint dx e^{-sx} = \frac{1}{s} \quad Res>0 \quad c>0
\end{align*}
\begin{align*}
    f(x) &= \frac{1}{2\pi i } \int_{c-i\infty}^{c+i\infty} ds \frac{1}{s} e^{sx}\\
    &= \frac{1}{2\pi i} \cdot 2\pi i  \cdot e^{(0)x} = 1
\end{align*}


(b)
\begin{gather*}
    f(x) = e^{-\beta x}
\end{gather*}
\begin{gather*}
    L(s) = \zuperint dx e^{-\beta x}e^{-sx} = \zuperint e^{-(\beta + s)x}\\
    \beta + c > 0 ,\qquad c > -\beta
\end{gather*}

\begin{gather*}
    = \frac{1}{\beta + s}
\end{gather*}
\begin{align*}
    f(x) &= \frac{1}{2\pi i } \int_{c-i\infty}^{c+i\infty} ds \frac{1}{\beta + s}e^{sx}\\
    &= \frac{1}{2\pi i }\cdot 2\pi i \cdot e^{-\beta x} = e^{-\beta x}
\end{align*}



%%New Section??

\bigskip
\hline
\begin{gather*}
    \dv[2]{u}{x} - \beta^2 x = h(x)
\end{gather*}

\begin{gather*}
    u = \frac{1}{2\pi} \superint dk F(k) e^{ikx} \qquad h = \frac{1}{2\pi} \superint dk H(k) e^{ikx}
\end{gather*}
\begin{gather*}
    \frac{1}{2\pi}\superint dk F(k)(-k^2 + \beta^2)e^{ikx} = \frac{1}{2\pi}\superint dk H(k) e^{ikx}
\end{gather*}
\begin{gather*}
    F(k) = \frac{H(k)}{k^2+\beta^2}
\end{gather*}
\begin{gather*}
    u = \frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty}dsL(s)e^{sx} \qquad h(x) = \frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty}dsM(s)e^{sx}
\end{gather*}
\begin{gather*}
    u''=\frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty} dsL(s)(+s^2)e^{sx}
\end{gather*}
\begin{gather*}
    L(s) = (s^2-\beta^2) = \frac{1}{s}\\
    L'''(s) = \zuperint dx \dv[]{u}{x}e^{-sx} = ue^{-sx}\zuperint u(-s)e^{-sx} = u(0) - sL(s)
\end{gather*}

\end{mdframed}


\begin{mdframed}
\bigkskip
\begin{center}
    \textbf{Laplace Transform}
    VIII\\
    05/18/2012
\end{center}
Laplace Transform

\begin{gather*}
    \lap (x') = sX(s) - x(0)
\end{gather*}
\begin{gather*}
    \lap (x'') = s^2X(s) - sx(0) - x'(0)
\end{gather*}
\begin{align*}
    &\Circled{1} \;\; x''(t) + \omega_0^2 x(t) = 0 \qquad x(0) = 0, \;\; x'(0) = v_0
\end{align*}
Sol:
\begin{align*}
    X(s) = \zuperint dte^{-st}x(t)
\end{align*}
\begin{align*}
    \left[s^2X(s) - sx(0) - x(0) \right] + \omega_0X(s) &= 0\\
    s^2X(s) - v_0 + \omega_0^2X(s) &= 0
    \\
    X(s) &= \frac{v_0}{s^2 + \omega_0^2}\\
 \implies x(t) &= (\frac{v_0}{\omega_0})\sin{\omega_0 t}
\end{align*}

\begin{gather*}
    \Circled{2} \quad x''+2\alpha x' + \omega_0^2 x = 0 
\end{gather*}
Sol:
\begin{align*}
    s^2X(s) - sx(0) - x'(0) + 2\alpha s X(s) + x(0) + \omega_0^2 X(s) &= 0\\
    s^2X(s) - v_0 + 2\alpha sX(s) + \omega_0^2 X(s) &= 0 \\
    X(s) = \frac{v_0}{s^2+2\alpha s + \omega_0^2} &= \frac{v_0}{(s-s_1)(s-s_2)}
\end{align*}
\begin{gather*}
    s_{1,2} = \frac{-2\alpha\sqrt{4\alpha^2 - 2\omega_0^2}}{2} = -\alpha \pm \sqrt{\alpha^2 - \omega_0^2} &= -\alpha \pm i\omega_0^2
\end{gather*}

\bigskip
\hline

\begin{align*}
    \lapinv\left(\frac{1}{(s+a)^2+b^2}\right) 
    &= \frac{1}{2\pi i} \int_{-i\infty+r}^{i\infty+r}dse^{st}\frac{1}{(s+a)^2+b^2}\\ 
    &= \frac{1}{2\pi i} \int_{-i\infty+r}^{i\infty+r}dse^{st} \frac{1}{(s-s_1)(s-s_2)}\\ &= \frac{1}{2\pi i} \cdot 2\pi i \left[\frac{e^{s_{1}t}}{s_1 - s_2} + \frac{e^{s_{2}t}}{s_2 - s_1}\right]\\
    & = \frac{1}{s_1 - s_2}[e^{s_1t}e^{s_2t}]\\ 
    &= \frac{1}{2ib}[e^{-(a+ib)t} - e^{-(a-ib)t}]\\
    &= \frac{1}{b}e^{-at}\frac{[e^{-ibt}-e^{ibt}]}{2i}\\
    &= -\frac{e^{-at}}{b}\sin{bt}
\end{align*}

\begin{gather*}
    \boxed{s_1 = -a+ib, \quad s_2 = -a-ib, \quad s_1 - s_2 = 2ib}
\end{gather*}


\bigkskip\hline


%%Another example?


\begin{gather*}
    ax+bx = y(t) \quad x(0) = 0
\end{gather*}

Sol:
\begin{gather*}
    a\left[sX(s) - x(0)\right] + bX(s) = y(s)\\
    X(s)[as+b] = y(s)\\
    \therefore \;\; X(s) = \frac{y(s)}{as+b}
\end{gather*}
\begin{gather*}
    y(t) = e^{-ct} \qquad y(s) = \frac{1}{s+c}
\end{gather*}
\begin{gather*}
    X(s) = \frac{1}{(as+b)(s+c)}
\end{gather*}
\begin{gather*}
    x(t) = \frac{1}{2\pi i}\int_{-i\infty+r}^{i\infty+r}ds \frac{e^{st}}{(as+b)(s+c)} = \frac{1}{2\pi i} \cdot 2\pi i \left[\frac{e^{-bt/a}}{c-b/a}+\frac{e^{-ct}}{-ac+b}\right] = \cdots
\end{gather*}


\hline

\begin{gather*}
\lap(\theta(x)) = \int_{t_0}^{\infty} dt e^{-st} = \frac{1}{s}\\
\frac{1}{2\pi i} \int_{-i\infty+r}^{i\infty+r}\frac{ds}{s}e^{st}\\
\lap(\theta(t-t_0)) = \int_{t_0}^\inftydt e^{-st} = \frac{e^{-st_0}}{s}    
\end{gather*}


\begin{gather*}
    \frac{1}{2\pi i} \int_{-i\infty+r}^{i\infty+r} ds \frac{e^{-st_0}}{s}e^{st} = \frac{1}{2\pi i }\left[\int_{-i\infty+r}^{i\infty+r}ds\frac{e^{-s(t-t_0)}}{s}\right] = \frac{1}{2\pi i} \cdot 2\pi i \cdot 1, \qquad t > t_0
\end{gather*}

\begin{gather*}
    n>0 \qquad \zuperint dt t^n e^{-st} = \frac{t^n e^{-st}}{-s} \eval_0^\infty - \zuperint dt n t^{n-1}\frac{e^{-st}}{-s} = \frac{n}{s}\zuperint dt t^{n-1}e^{-st} = \frac{n!}{s^{n+1}}
\end{gather*}



\begin{gather*}
    \lap\left(\dv[n]{f(t)}{t}\right) = s^n F(s) - s^{n-1}f(0) - s^{n-2}f'(0) + \cdots\\
    f(x) - f(x-x_0) = g(x)\\
    F(s) = \zuperint dx e^{-sx} f(x)\\
    \zuperint dx e^{-sx} f(x-x_0) = \zuperint dx e^{-s(x-x_0)}f(x-x_0)e^{-sx_0 } = e^{-sx_0}\zuperint dx e^{-s(x-x_0)}f(x-x_0) \qquad x-x_0 = y
\end{gather*}

\bigskip
\hline

\begin{huge}\Circled{2}\end{huge}

Example:
\begin{align*}
    f(x) = \begin{cases}
    e^x, \quad &x<0\\
    e^{-2x}, \quad &x>0
    \end{cases}
\end{align*}
This kind of inversion formula is useful if the asymptotic behavior of $f(x)$ is known.\\

\textbf{Example 16 P.350}
\begin{align*}
    f(x) = e^{-ik_0x} \qquad &F(k) = \frac{1}{\sqrt{2\pi}}\delta(k-k_0)\\
    f(x) = \delta(x-x_0) \qquad &F(k) = \frac{1}{\sqrt{2\pi}}e^{ikx_0}\\
    f(x) = \begin{cases}
    1 \quad &0<x<x_0\\
    0 \quad &\text{otherwise}
    \end{cases}
    \qquad &F(k) = \frac{1}{\sqrt{2\pi}}\frac{1}{ik}(e^{ikx_0}-1) =\frac{1}{\sqrt{2\pi}}\frac{2e^{i\frac{kx_0}{2}}}{k}\sin\frac{kx_0}{2}\\
    f(x) = e^{-a^2x^2} \qquad & F(k) = \frac{1}{2\pi}e^{-k^2/4a^2}
\end{align*}




\textbf{Laplace Transform}\\

$F(s) = \zuperint dx e^{-sx}f(x)$ is the Laplace transform of $f(x) = F_l(s)$ say.\\
\indent Then $F_l(s) = \sqrt{2\pi}F_+(is)$.\\
\indent Inversion formula tells us that if
\begin{align*}
    f_+(x) = \begin{cases}
    f(x) \quad &x>0\\
    0 \quad &x<0
    \end{cases}\qquad f_+(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty + \tau_0}^{\infty + \tau_0}F_+(k)
\end{align*}

\begin{align*}
    &\text{Let} \quad k = is \quad dk = ids \quad s = -ik\\
    &\therefore f_+(x) = \frac{1}{\sqrt{s\pi i}} \int_{-i\infty + \tau_0}^{i\infty + \tau_0} ds e^{sx} F_+(is) = \frac{1}{2\pi i} \int_{-i\infty + \tau_0}^{i\infty + \tau_0} ds e^{sx} F_l(s)
\end{align*}


\textbf{Example:}

\begin{align*}
    \text{Let} \quad f_+(x) = \begin{cases}
    e^{-x} \quad &x>0\\
    0 \quad &x<0
    \end{cases}
\end{align*}
\begin{gather*}
    \text{then} \quad \tau_0 = 0
\end{gather*}

\begin{gather*}
    F(s) = \zuperint dx e^{-sx}e^{-x} = \frac{1}{1+s}
\end{gather*}
\begin{align*}
    f_+(x) = \frac{1}{2\pi i}\int_{-i\infty}^{i\infty} ds \frac{e^{sx}}{1+s} = \begin{cases}
        e^{-x} \quad &x>0\\
        0 \quad &x<0
    \end{cases}
    \end{align*}
\begin{gather*}
    \text{take} \quad f(x) = e^{ipx} \quad \tau_0 = \epsilon
\end{gather*}
\begin{align*}
    f(x) = \begin{cases}
    1 \quad &x>0\\
    0 \quad &x<0
    \end{cases} \qquad F(s) = \frac{1}{s}
\end{align*}

\textbf{Some Simple Examples}

\begin{center}
    \begin{tabular}{c|c|c}
    Function & Fourier Transform & Laplace Transform \\
    $f(x)$ & $F(k)$ & $F(s)$\\
    $f'(x)$ & $ikF(k)$ & $sF(s) - f(0)$\\
    $f''(x)$ & $-k^2F(k)$ & $s^2F(s) - sf(0) - f'(0)$\\
    $\zuperint dx'f(x')$ & $\frac{F(k)}{ik}G\delta(k)$ (G is Arbitrary) & $\frac{F(s)}{s}$\\
    $f(x+a)$ & $e^{ika}F(k)$ & 
        $\begin{cases}
        e^{as}F(s) - e^{as}\int_{0}^{a}dxe^{-sx}f(x) \quad &a>0\\
        e^{as}F(s) \quad &a<0
        \end{cases}$\\
    $xf(x)$ & $iF'(k)$ & $-F'(s)$\\
    $\superint dx' f_1(x')f_2(x-x')$ & $F_1(k)F_2(k)$ & $F_1(s)F_2(s)$\\
    $f_1(x)f_2(x)$ & $\superint dk' F_1(k')F_2(k-k')$ & $\frac{1}{2\pi i} \int_{-i\infty + y}^{i\infty + y}ds'F_1(s')F_2(s')$
    \end{tabular}
\end{center}

\hline
\bigskip
Example:\\
\indent Show that $\frac{1}{2\pi i}\int_{-i\inft + \epsilon}^{i\inft + \epsilon}dse^{sx} = \delta(x)$ is useful in inverting the Laplace transform.\\
\indent e.g.
\begin{gather*}
    FT = \frac{1}{\sqrt{2\pi}}\superint dxe^{ikx}f'(x) = \frac{1}{\sqrt{2\pi}}\left[e^{-ikx}f(x)\eval_{-\infty}^{\infty}-\superint dx (-ik) f(x)\right] = ik\frac{1}{\sqrt{2\pi}}\superint dx e^{ikx}f(x) = ikF(k)
\end{gather*}

\begin{gather*}
    LT = \zuperint dx e^{-sx} f'(x) = \left[e^{-sx}f(x) \eval_0^{\infty} - \zuperint dx e^{-sx}(-s) f(x) \right] = -f(0) + sF(s)
\end{gather*}
Interesting that $sF(s)$ alone not LT.
\begin{gather*}
    \dv[]{f}{x} = g(x) 
\end{gather*}

Fourier Transform
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dke^{ikx}f(x)
\end{gather*}
\begin{gather*}
    g(x) = \frac{1}{\sqrt{2\pi}}\superint dke^{ikx}G(k)
\end{gather*}
\begin{gather*}
    \therfore \quad ikF(k) = G(k)
\end{gather*}

\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dke^{ikx}\frac{G(k)}{ik}
\end{gather*}

\Circled{1}
\begin{gather*}
    \text{If} \quad g(x) = e^{i\lambda_0x} \quad G(k) = \sqrt{2\pi}\delta(k-\lambda_) \quad \text{and} \quad f(x) = \frac{1}{i\lambda}e^{i\lambda_0x}\\
    \text{Trivial!}
\end{gather*}

\bigskip

Laplace Transform
\begin{gather*}
    sF(s) - f(0) = G(s)
\end{gather*}
\begin{gather*}
    F(s) = \frac{G(s)+f(0)}{s}
\end{gather*}
\begin{gather*}
    f(x) = \frac{1}{2\pi i}\int_{-i\infty + y}^{i\infty + y}dse^{sx}\frac{G(s)+f(0)}{s}
\end{gather*}

\Circled{1}
\begin{gather*}
    \text{If} \quad g(x) = e^{-ax} \quad G(s) = \frac{1}{s+a}
\end{gather*}
\begin{gather*}
    f(x) = \frac{1}{2\pi i}\int_{-i\infty + y}^{i\infty + y}dse^{sx}\left[\frac{1}{s(s+a)}+\frac{f(0)}{s}\right]
\end{gather*}
\begin{gather*}
    \therefore \quad f(x) = \frac{1}{2\pi i }\cdot 2\pi i \left[\frac{1}{(-a)}+\frac{1}{a} + f(0) \right]
\end{gather*}
\begin{gather*}
    f(x) = -\frac{e^{-ax}}{a} + \frac{1}{a} + f(0) \quad \text{equivalently} \quad f(x) = -\frac{e^{-ax}}{a} + constant
\end{gather*}


\hline


\noindent \Circled{1} $x''(t) + \omega_0^2x(t) = 0$ Free vih\\
\Circled{2} $x''(t) + 2\alpha x'(t) + \omega_0^2 x(t) = 0$ Free vih with damping\\
\Circled{3} $x'' + \omega_0^2x(t) = f(t) $ forced vih\\
\Circled{4} $x'' + 2\alpha x'(t) + \omega_0^2x(t) = f(t)$ forced vih with damping\\


\bigskip
\textbf{Laplace}\\
\Circled{1} $X(s) = \int dte^{-st}x(t) \quad x(0) = 0 \quad x'(0) = v_0$

\begin{gather*}
    s^2X(s) - sx(0) - x(0) + \omega_0^2X(s) = 0 \\
\end{gather*}
\begin{gather*}
    \therefore \quad X(s) = \frac{v_0}{s^2 + \omega_0^2}
\end{gather*}
\begin{gather*}
    \therefore \quad x(t) = \left(\frac{v_0}{\omega_0}\right)\sin(\omega_0t)
\end{gather*}


\textbf{Fourier}
\begin{gather*}
    F(\omega) = \frac{1}{\sqrt{2\pi}}\superint dtx(t) e^{i\omega t}
\end{gather*}
\begin{gather*}
    \superint dt(-\omega^2 + \omega_0^2)F(\omega)e^{-i\omega t} = 0 \qquad F(\omega) = a\delta(\omega-\omega_0) + b\delta (\omega + \omega_0)
\end{gather*}
\begin{gather*}
    x(t) = ae^{-i\omega_0 t} + be^{i\omega_0 t}
\end{gather*}


\Circled{4} \quad Laplace
\begin{gather*}
    s^2X(s) - sx(0) - x'(0) + 2\alpha \left[X(s) - x(0)\right] + \omega_0X(s) = F(s) 
\end{gather*}
\begin{gather*}
    \therefore \quad (s^2 + 2\alpha s + \omega_0^2)X(s) = v_0 + F(s)
\end{gather*}

\begin{gather*}
    \frac{1}{(s+a)^2 + b^2} = \frac{1}{(s+a+ib)(s+a-ib)}
\end{gather*}
\begin{gather*}
    \therefore \quad X(s) = \frac{v_0}{s^2+2\alpha s + \omega_0^2} + \frac{F(s)}{s^2 + 2\alpha s + \omega_0^2} = \frac{v_0}{(s+a)^2+\omega_0^2 - \alpha^2}+\frac{F(s)}{(s+a)^2+\omega_0^2 - \alpha^2}
\end{gather*}

\begin{gather*}
    \frac{1}{(s+a)^2-b^2} = \frac{1}{(s+a-b)(s+a+b)}
\end{gather*}

\begin{gather*}
     \frac{1}{2\pi i}\int_{-i\infty y}^{i\infty y} ds \frac{e^{st}}{(s+a)^2 + b^2} = e^{-at} \left[\frac{e^{-ibt}}{-bt}+\frac{e^{bt}}{2b}\right] = \frac{1}{b} e^{-at}\sin bt = \frac{1}{b}e^{-at} \left[\frac{e^{bt}+e^{-bt}}{2}\right]
\end{gather*}

\begin{gather*}
    \frac{1}{2\pi i} \int_{-i\infty y}^{i\infty y} ds \frac{e^{st}}{(s+a)^2 + (\omega_0^2 - \alpha^2)} \qquad a=\alpha, \quad b = \sqrt{\omega_0^2 + \alpha ^2} \quad \text{if} \;\; \omega_0>\alpha
\end{gather*}

\textbf{Consider:} $\omega_0>\alpha$
\begin{gather*}
    \frac{1}{2\pi i} \int_{-i\infty y}^{i\infty y} ds \frac{e^{st}}{(s+a)^2 + (\omega_0^2 - \alpha^2)} = \frac{1}{\sqrt{\omega_0^2-\alpha^2}}e^{-at} \sin (\sqrt{\omega_0^2 - \alpha^2}t)
\end{gather*}
\begin{gather*}
    \therefore \quad x(t) = \frac{v_0}{\sqrt{\omega_0^2-\alpha^2}}e^{-\alpha t}\sin(\sqrt{\omega_)^2 - \alpha^2})+ \int_0^tdt'\frac{v_0}{\sqrt{\omega_0^2-\alpha^2}}^{-\alpha t'}\sin(\sqrt{\omega_0^2-\alpha^2}t')f(t-t')
\end{gather*}
\begin{gather*}
    \left(\text{Can be done directly:} \quad \frac{1}{2\pi i}\int_{-i\infty y}^{i\infty y} ds 2^{st}\frac{F(s)}{(s+a)^2+\omega_0^2-\alpha^2} \quad F(s) = \int_0^{t_0} dt e^{-st}f_0\right)
\end{gather*}
\begin{gather*}
    (-\omega^2 - 2i\alpha \omega + \omega_0^2)X(\omega) = F(\omega)
\end{gather*}
\begin{gather*}
    \therefore \quad X(\omega) = \frac{F(\omega)}{\omega_0^2 - \omega^2 - 2i\alpha \omega}
\end{gather*}
\begin{gather*}
    x(t) = \frac{1}{\sqrt{2\pi}}\superint{d\omega}\frac{F(\omega)e^{i\omega t}}{\omega_0^2 -\omega^2 - 2i\alpha\omega} + \text{ Homogeneous Part}
\end{gather*}
\begin{gather*}
    \omega^2 + 2i\alpha\omega - \omega_0^2 = 0 \quad \to \quad \omega = -i\alpha \pm \sqrt{-\alpha + \omega_0^2}
\end{gather*}
\textbf{Consider $\omega_0 > \alpha$ (weakly damped oscillator)}
\begin{gather*}
    \omega_1 = \sqrt{\omega_0^2 - \alpha^2} - i\alpha \quad \omega_2 = \sqrt-{\omega_0^2 - \alpha^2}-i\alpha
\end{gather*}
\begin{gather*}
    x(t) = \frac{1}{\sqrt{2\pi}}\superint d\omega \frac{F(\omega)e^{-i\omega t}}{-(\omega - \omega_1)(\omega - \omega_2)}
\end{gather*}
\begin{gather*}
    \text{let} \quad f(t) = \begin{cases}
    0 \quad & t<0\\
    f_0 \quad & 0<t<t_0\\
    0 \quad & t>t_0
    \end{cases}
\end{gather*}
\begin{gather*}
    F(\omega) = \frac{1}{\sqrt{2\pi}}\superint dt f(t) e^{i\omega_0 t} = \frac{f_0}{\sqrt{2\pi}}\int_0^{t_0} dt f(t) e^{i\omega t} = \frac{f_0}{\sqrt{2\pi}}\frac{1}{i\omega}(e^{i\omega t_0} - 1)
\end{gather*}
\begin{gather*}
    \therefore \quad x(t) = - \frac{f_0}{2\pi i }\superint \frac{d\omega(e^{i\omega t_0}-1)e^{-i\omega t}}{\omega(\omega - \omega_1)(\omega - \omega_2)}
\end{gather*}
$t> t_0$ Take lower half
\begin{gather*}
    x(t) = - \frac{f_0}{2\pi i }(-2\pi i) \left\{\frac{[e^{i\omega_1 t_0} - 1]}{\omega_1(\omega_1 - \omega_2)} + \frac{[e^{i\omega_2 t_0} - 1]}{\omega_2(\omega_2 - \omega_1)}\right\}
\end{gather*}

$0<t<t_0$ take upper half for $e^{i\omega_0 t}$ part and lower half for 1 part
\begin{gather*}
    x(t) = -\frac{f_0}{2\pi i }(-2\pi i) \left\{\frac{(-1)e^{-\omega_1 t}}{\omega_1(\omega_1 - \omega_2)} + \frac{(-1)e^{-\omega_2 t}}{\omega_2(\omega_2 - \omega_1)}\right\}
\end{gather*}


$t<0$ take upper half for both
\begin{gather*}
    x(t) = 0
\end{gather*}


\hline

Some typical problems:\\
\Circled{1} Circuits L,R,C
\begin{gather*}
    L\dv[]{i(t)}{t}+Ri(t)+\frac{q(t)}{C} = e(t)
\end{gather*}
\begin{gather*}
    L(sI(s) - I(0)) + RI(s) + \frac{Q(s)}{C} = E(s)
\end{gather*}
But $i(t) = \dv[]{q(t)}{t}$
\begin{gather*}
    \therefore \quad I(s) = sQ(s) - q(0)
\end{gather*}
Let the initial condition be $i(0) = 0 \quad q(0) = 0$\\
then
\begin{gather*}
    LsI(s) + RI(s) + \frac{I(s)}{Cs} = E(s)
\end{gather*}
\begin{gather*}
    \frac{L}{s}\left[s^2L(s) + \frac{RsI(s)}{L} + \frac{1}{LC}\right] = E(s)
\end{gather*}
\begin{gather*}
    \therefore \quad I(s) = \frac{sE(s)}{L(s^2 + \frac{R}{L}s+\frac{1}{LC})} = \frac{sE(s)}{L(s^2 + 2\alpha s + \omega^2 + \alpha^2)} =\frac{sE(s)}{L[(s_a)^2+\omega^2]}
\end{gather*}
\begin{gather*}
    \alpha = \frac{R}{2L}, \quad \omega^2 = \frac{1}{L} - \frac{R^2}{4L^2}
\end{gather*}

\bigskip
\hline

Problem 1\\
Consider the condition:
\begin{gather*}
    e_0 = \begin{cases}
    e_0 \quad & 0<t<t_0\\
    0 \quad &t>t_0
    \end{cases}
\end{gather*}
obtain i(t) and q(t).\vspace{1 cm}\\
Problem 2\\
Consider the condition:
\begin{gather*}
    e_0(t) = e_0 \quad t>0 \quad \omega^2<0\\
    \lap\{\theta(t)\} = \frac{1}{s}
\end{gather*}

(Since all Laplace transform are such that the inverse = 0 for $t<0$, one doesn't make a distinction between $\theta(t)$ and 1.
\begin{gather*}
    \lapinv\{\theta(t-t_0)\} = \int_{t_0}^{\infty}dte^{-st} = \frac{1-e^{st_0}}{s}(\frac{e^{-st_0}}{s})
\end{gather*}
\vspace{1 cm}\\


Problem 3:\\
Prove that
\begin{gather*}
    \lap^1\left(\frac{1-e^{-st_0}}{s}\right) = \theta(t-t_0)
\end{gather*}

\hline

\Circled{2} Compled Pendulums (Lenple circuits)
\begin{gather*}
    mx_1'' = -\frac{mg}{l}x_1 +k(x_2-x_1) \quad x_1 = x_1(t) \to X_1(s)
\end{gather*}
\begin{gather*}
    mx_2'' = -\frac{mg}{l}x_2 + k(x_1-x_2) \quad x_2 = x_2(t) \to X_2(s)
\end{gather*}
\begin{gather*}
    \text{Let} x_1(0) = 0 = x_2(0) \qquad x_1'(0) = v_0 ,\quad x_2'(0) = 0
\end{gather*}
\begin{gather*}
    m[s^2X_1(s) -sx_1'(0) - x_2(0)] = -\frac{mg}{l}X_1 +k(X_2 - X_1)
\end{gather*}
\begin{gather*}
    m[s^2X_2(s) -sx_2'(0) - x_1(0)] = -\frac{mg}{l}X_2 +k(X_1 - X_2)
\end{gather*}
putting in the initial conditions:
\begin{gather*}
    m[s^2X_1 - sv_0] = -\frac{mg}{l}X_1 + k(X_2 - X_1)
\end{gather*}
\begin{gather*}
    ms^2X_2 = -\frac{mg}{l}X_2 + k(X_1 - X_2)
\end{gather*}

\vspace{1 cm}
Problem 4\\
Solve in terms of $X_1(s)$ and obtain $x_1(t)$\\
\Circled{3} A beam supported at two ends.
\begin{gather*}
    \dv[4]{y(x)}{x} = \frac{1}{EI}q(x)\\
    \text{EI rigidity, q(x) lood per unit length}
\end{gather*}
Let $q(x) = q_0$ constant lood $\frac{1}{EI} = \lambda$
\begin{gather*}
    \dv[4]{y(x)}{x} = \lambda q_0 \qquad \text{also} \quad y(0) = 0 = y(L) \quad y'(0) = 0 = y'(L)
\end{gather*}
\begin{gather*}
    s^4Y(s) - s^3y(0) - s^2y'(0) - sy''(0) - y'''(0) = \frac{\lambda q_0}{s}
\end{gather*}
Let $y''(0) = a$, and $y'''(0) = b$
\begin{gather*}
    s^4Y(s) -as - b = \frac{\lambda q_0}{s}
\end{gather*}
\begin{gather*}
    \therefore \quad Y(s) = \frac{\lambda q_0}{s^5} + \frac{b}{s^4} + \frac{a}{s^3}
\end{gather*}
\begin{gather*}
    y(s) = \lambda q_0 \frac{x^4}{4!} + b\frac{x^3}{3!} + a\frac{x^2}{2!} = \lambda q_0 \frac{x^4}{24} + b\frac{x^3}{6} + a\frac{x^2}{2}
\end{gather*}
\begin{gather*}
    y(L) = 0 \quad \therefore \quad \lambda q_0 \frac{L^4}{24} + b\frac{L^3}{6} + a\frac{L^2}{2} = 0 \quad \therefore \quad \frac{a}{2} + b\frac{L}{6} = -\frac{\lambda q_0 L^2}{24}
\end{gather*}
\begin{gather*}
    y'(L) = 0 \quad \therefore \quad \lambda q_0 \frac{L^3}{6} + b\frac{L^2}{2} + aL = 0 \quad \therefore \quad a + b\frac{L}{2} = - \frac{\lambda q_0 L^2}{6}
\end{gather*}
\begin{gather*}
    \frac{a}{2}-\frac{a}{3} = - \frac{\lambda q_0 L^2}{24} + \frac{\lambda q_0 L^2}{18} = \left(\frac{-3+4}{72}\right)\lambda q_0 L^2
\end{gather*}
\begin{gather*}
    \frac{a}{2} = \frac{\lambda q_0 L^2}{72} \quad \therefore \quad a = \frac{\lambda q_0 L^2}{12}
\end{gather*}
\begin{gather*}
    \therefore \quad \frac{bL}{2} = -\frac{\lambda q_0 L^2}{6} - \frac{\lambda q_0 L^2}{12}= -\frac{\lambda q_0 L^2}{4}
\end{gather*}
\begin{gather*}
    \therefore \quad b = -\frac{\lambda q_0 L^2}{2}
\end{gather*}
\vspace{1 cm}

Problem 5\\
\begin{gather*}
    \text{Let} \quad q = \begin{cases}
    0 \quad & =<x<L/2\\
    q_0 \quad & L/2 < x < L
    \end{cases} \quad y(0) = 0 = y(L) \quad y''(0) = 0 = y''(L)
\end{gather*}
\Circled{4} Difference Equation
\begin{gather*}
    f(x) - af(x-x_0) = g(x) 
\end{gather*}
\begin{gather*}
    F(s) - ae^{-sx_0 F(s)} = G(s) 
\end{gather*}
\begin{gather*}
    \therefore \quad F(s) = \frac{G(s)}{1-ae^{-sx_0}} = G(s) + \sum_{n=1}^{\infty}a^ne^{nx_0s}G(s)
\end{gather*}
\begin{gather*}
    f(x) = g(x) + \sum a^n g(x-nx_0) = g(x) + ag(x-x_0) + a^2g(x-2x_0) + \cdots
\end{gather*}

\vspace{1 cm}
\noindent Problem 6\\
Given $f(x+\Delta) = f(x)$ prove $F(s) = \frac{\int_0^\Delta dx e^{-sx} f(x)}{1-e^{-\Delta s}}$
\begin{gather}
    F(s) = \zuperint dx e^{-sx} f(x) = \int_0^\Delta dx e^{-sx}f(x) + \int_\Delta^{2\Delta}dxe^{-sx}f(x) \cdots
\end{gather}
\begin{gather*}
    \int_\Delta^{2\Delta}dxf(x) \stackrel{x' = x-\Delta}{=} \int_0^\Delta dx' e^{-sx'-\Delta s}f(x'+\Delta) = e^{-s\Delta}\int_0^\Delta e^{-sx'}f(x')
\end{gather*}
\begin{gather*}
    \therefore \quad F(s) = (1 + e^{-\Delta s}+e^{-2\Delta s}+ \cdots)\int_0^\Delta dx e^{-sx} f(x) = \frac{\int_0^\Delta dx e^{-sx} f(x)}{1-e^{-\Delta s}}
\end{gather*}
\vspace{1 cm}


Problem 7 : Do No.7 p.218\\
Green's Function\\
\begin{gather*}
    \text{If} \quad L(x) A(x) = B(x) 
\end{gather*}a
\begin{gather*}
    \text{then} \quad L(x) G(x,x') = \delta (x-x') \quad \text{is the equation for G}
\end{gather*}
\begin{gather*}
    A(x) = \int dx'G(x,x')B(x')
\end{gather*}
\begin{gather*}
    \text{Since} \quad LA(x) = \int dx'(LG)B(x') = \int dx'\delta(x-x')B(x') = B(x)
\end{gather*}

\bigskip\hline

\underline{Fourier Transform}
\begin{gather*}
    f(x) = \sum_{n=-\infty}^\infty C_n e^{i\frac{n\pi x}{L}}
\end{gather*}
\begin{gather*}
    C_n = \frac{1}{2L}\int_{-l}^L dx f(x) e^{-i\frac{n\pi x}{L}}
\end{gather*}
\begin{gather*}
    k = \frac{n\pi}{L} \quad \text{infinite interval}
\end{gather*}
\begin{gather*}
    f(x) = \sum_n \Delta n C_n e^{ikx} = \sum \Delta k \left(\frac{L}{\pi}C_n\right)e^{ikx}=\sum \Delta k C(k) e^{ikx}
\end{gather*}
\begin{gather*}
    C(k) = \frac{L}{\pi}C_n = \frac{1}{2\pi}\int_{-L}^L dx f(x) e^{-ikx}
\end{gather*}
Now let $L\to \infty \quad \Delta k$ becomes infinitesimal and k becomes continuous
\begin{gather*}
    \text{Then} \quad f(x) - \superint dx C(x) e^{ikx}
\end{gather*}
\begin{gather*}
    C(k) = \frac{1}{2\pi} \superint dx f(x) e^{-ikx}
\end{gather*}
\begin{gather*}
    \text{Define} \quad C(k) = \frac{1}{\sqrt{2\pi}}F(k)
\end{gather*}
then
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dk F(k) e^{ikx}
\end{gather*}
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\pi}} \superint dx f(x) e^{-ikx}
\end{gather*}
Do FT's of:\\
   \begin{enumerate}
       \item $e^{iax}$
       \item $\sin ax$
       \item $e^{-ax^2} \to \frac{1}{\sqrt{2a}}e^{-k^2/4a}$
       \item $\frac{1}{x^2 + a} \to \frac{1}{a}\sqrt{\frac{\pi}{2}}e^{-a|k|}$
       \item $f(t) = \begin{cases}
        0 \quad &t<0\\
        e^{-t/T}sin\omega_0t \quad &t>0
       \end{cases}$
       \item $e^{ax}$
       \item $x^2$
       \item $e^x$ For all x
   \end{enumerate}

Do 7 and 8 after 
\begin{gather*}
    \superint dk |F(k)|^2 = \superint dx |f(x)|^2
\end{gather*}

\hline

Integral Transforms
\begin{gather*}
    \delta(x) : \quad \int_{-a}^a dx \delta (x) = 1 \qquad \int_{-a}^a dx \delta (x) f(x) = f(0) 
\end{gather*}
A function $ = \infty$ at $x=0$ and $=0$ at $x\neq 0$ $\implies$ "Physical"\\
Several different limiting procedure by which $\delta (x) $ is defined.

\begin{gather*}
    \delta (x) = \lim_{\epsilon \to 0} \frac{1}{\pi} \frac{\epsilon}{x^2 + \epsilon^2}
\end{gather*}
\begin{gather*}
    \delta (x) = \lim^{\beta \to \infty} \sqrt{\frac{\beta}{\pi}}e^{-\beta x^2}
\end{gather*}
\begin{gather*}
    \delta (x) = \lim_{L\to \infty}\frac{1}{2\pi}\int_{-L}^L d\lambda e^{-i\lambda x} = \lim_{L\to\infty} \frac{1}{\pi}\frac{\sin(Lx)}{x}
\end{gather*}
\begin{gather*}
    \frac{1}{x' - (x+ i\epsilon)} = P\left(\frac{1}{x'-x}\right)+ i \pi \delta (x'-x)
\end{gather*}
\begin{gather*}
    \int_{-a}^a dx e^{-i\lambda x} = \frac{e^{-i\lambda x}-e^{i\lambda x}}{-i\lambda} = 2\frac{\sin\lambda a}{\lambda}
\end{gather*}
\begin{gather*}
    \int_{-a}^a dx \delta (x) = \frac{1}{\pi}\int_{-L}^L d\lambda \frac{\sin\lambda a}{\lambda} = \frac{1}{\pi}\cdot \pi = 1
\end{gather*}

It satisfies the following relations:
\begin{align*}
    &i) \quad \delta (-x) = \delta (x)\\
    &ii) \quad \delta (ax) = \frac{1}{|a|}\delta (x)\\
    &iii) \quad \delta (f(x)-f(x_0)) = \frac{1}{\partial f / \partial x \eval_{x=x_0}}\delta (x-x_0)\\
    &iv) \quad x\delta (x) = 0\\
    &v) \quad \delta (x^2 - a^2) = \frac{1}{2|\pi|}[\delta (x+a) + \delta (x-a)]\\
    &vi) \quad \superint dx \delta (x-a) \delta (x-b) = \delta (a-b)
\end{align*}
\begin{gather*}
    \theta(x) : \quad \theta(x) = 0 , \quad x<0 \quad \text{and} \quad \theta(x) = 1, \quad x>0
\end{gather*}
\begin{gather*}
    \theta'(x) = \delta (x) 
\end{gather*}
\bigskip
\hline
Fourier Transforms
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\pi}}\superint dx e^{ikx} f(x) 
\end{gather*}
Now
\begin{gather*}
    \frac{1}{\sqrt{2\pi}}\superint dje^{-ikx'}F(k) = \frac{1}{2\pi}\superint dk e^{-ikx-}\superint dx e^{ikx}f(x) = \superint dx f(x) \frac{1}{2\pi }\superint dk e^{-ik(x-x')}= \superint dx f(x) \delta (x-x') = f(x')
\end{gather*}
Inverse Holds
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dk e&{-ikx}F(k)
\end{gather*}

\hline
Find FT's of 
\begin{gather*}
    f(x) = \frac{1}{x^2+a^2} \quad , \quad f(x) = e^{-ax^2}
\end{gather*}
\hline
Multi-dimensional Transforms
\begin{gather*}
    F(k_1, k_2) = \frac{1}{2\pi} \superint dx\superint dy e^{i(k_1x+k_2y}f(x,y)
\end{gather*}
\begin{gather*}
    f(x,y) = \frac{1}{2\pi}\superint dk_1 \superint dk_2 e^{-i(k_1x+k_2y)}F(k_1,k_2)
\end{gather*}

Three Dimensional:
\begin{gather*}
    F(\vec k) = \frac{1}{(2\pi)^{3/2}}\int d^3xe^{i\vec k \cdot \vec r}f(\vec r) \quad , \quad f(\vec r) = \frac{1}{(2\pi)^{3/2}}\int d^3ke^{-i \vec k \vec r}F(\vec k)
\end{gather*}

Convolution Integral:
\begin{gather*}
    \frac{1}{\sqrt{2\pi}}\superint dke^{-ikx}F(k)G(k) = \frac{1}{\sqrt{2\pi}}\superint dke^{-ikx}F(k) \frac{1}{\sqrt{2\pi}}\superint dx' e^{ikx'} g(x')\\
    = \frac{1}{2\pi }\superint dx' g(x') \superint dk^{-ik(x-x')}F(k) = \frac{1}{\sqrt{2\pi}}\superint x' f(x-x')g(x')
\end{gather*}
also
\begin{gather*}
    =\frac{1}{\sqrt{2\pi}}\superint dx' g(x-x') f(x')
\end{gather*}

Similarly
\begin{gather*}
    \frac{1}{\sqrt{2\pi}}\superint dke^{-ikx}F(k)G(k)H(k) = \frac{1}{2\pi}\superint dx' \superint dx'' f(x-x'-x'')g(x')j(x'')\\
    =\frac{1}{2\pi}\superint dx'\superint dx'' f(x-x')g(x-x')h(x'')
\end{gather*}

Now, for $x = 0$
\begin{gather*}
    \frac{1}{\sqrt{2\pi}}\superint dkF(k)G(k) = \frac{1}{\sqrt{2\pi}}\superint dx' g(-x')f(x')
\end{gather*}

Now 
\begin{gather*}
    G(k) = \frac{1}{\sqrt{2\pi}}\superint dx'e^{ikx'}g(x')
\end{gather*}
\begin{gather*}
    \therefore \quad G^*(k) = \frac{1}{\sqrt{2\pi}}\superint dx' e^{-ikx'}g^*(x')
\end{gather*}
\begin{gather*}
    \therefore \quad \frac{1}{\sqrt{2\pi}}\superint dk^{-ikx}F(k)G^*(k) = \frac{1}{\sqrt{2\pi}}\superint dk^{-ikx}F(k)\frac{1}{\sqrt{2\pi}}\superint dx' e^{-ikx'}g^*(x')\\
    =\frac{1}{2\pi}\superint dx'g^*(x') \frac{1}{\sqrt{2\pi}}\superint dk^{-ik(x+x')}F(k) \\
    = \frac{1}{\sqrt{2\pi}}\superint dx'g^*(x')f(x+x')
\end{gather*}
\begin{gather*}
    \therefore \quad \superint dkF(k)G^*(k) = \superint dx' f(x')g^*(x')
\end{gather*}
Hence
\begin{gather*}
    \superint dk|F(k)|^2 = \superint dx'|f(x')|^2
\end{gather*}
Exceptional Cases
\begin{gather*}
    \text{Let} \quad f(x) = \begin{cases}
    x^{-1/2} \quad &x>0\\
    0 \quad &x<0
    \end{cases}
\end{gather*}
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\pi}}\superint dxe^{ikx}f(x) = \frac{1}{\sqrt{2\pi}}\zuperint dxe^{ikx}x^{-1/2}
\end{gather*}
\begin{gather*}
    \int_C dze^{ikz}z^{-1/2} = 0 \quad \text{no trouble at} \; z = 0 \quad \frac{dz}{z^{-1/2}} \text{ is O.K.}
\end{gather*}
\begin{gather*}
    \therefore \quad \zuperint dz^{ikz}z^{-1/2} = \int_0^{i\infty}dxe^{ikz}z^{-1/2} \stackrel{z=i\pi/2}{=} \zuperint dye^{i\pi/2}e^{-ky}e^{-i\pi/4}y^{-1/2} = e^{i\pi/4} \zuperint dye^{-ky}y^{-1/2}
\end{gather*}
\begin{gather*}
    \text{Let} \quad ky = t \quad \zuperint dze^{ikz}z^{-1/2} = \frac{e^{i\pi/4}}{\sqrt{k}}\zuperint dte^{-t}t^{-1/2} = \frac{e^{i\pi/4}}{\sqrt{k}} 2 \zuperint du e^{-u^2} \\
    (\sqrt{t} = u \quad \frac{dt}{2\sqrt{t}} = du)
\end{gather*}
\begin{gather*}
    \zuperint dxe^{-ax^2} = \frac{1}{2}\sqrt{\frac{\pi}{a}} \qquad \zuperint dze^{ikz}z^{-1/2} = \frac{e^{i\pi/4}}{\sqrt{k}}\sqrt{\pi}
\end{gather*}
\begin{gather*}
    \therefore \quad \text{for} \quad k> 0 \quad f(k) = \frac{1}{\sqrt{2\pi}} \frac{e^{i\pi/4}}{\sqrt{k}}\sqrt{\pi}= \frac{1}{\sqrt{k}}\frac{1}{\sqrt{2}}\left[\cos\frac{\pi}{4} + i\sin\frac{\pi}{4}\right] = \frac{1}{\sqrt{k}}\frac{1}{2}[1+i]
\end{gather*}
By analytic continuation this is defined for all k, e.g. For $k = -\abs{k} = \abs{k}e^{i\pi}$
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\abs{k}}}e^{-i\pi/4} = \frac{1}{\sqrt{-2k}}e^{i\pi/4}
\end{gather*}
$\therefore$ in principle no need to come out the step below!!!\\
Also not that the cut has to be along the negative real axis otherwise for $k<0$ we would be falling about $(xe^{2\pi i})^{-1/2}$.

\begin{gather*}
    \text{For} \quad k<0\\
    \text{Let} \quad k = -\omega
\end{gather*}
\begin{gather*}
    \zuperint dze^{-i\omega z}z^{-1/2} = \int_0^{i\infty}dze^{-i\omega z} \stackrel{z=e^{-i\pi/2}y}{=} \zuperint dye^{2\pi i/2}e^{-\omega y}e^{-i\pi/4}y^{-1/2}\\ = e^{-i\pi/4} \zuperint dye^{-\omega y}y^{-1/2} = \frac{e^{i\pi/4}}{\sqrt{\omega}}\sqrt{\pi}
\end{gather*}
\begin{gather*}
    \therefore \quad \text{for} \quad k<0 \quad F(k) = \frac{1}{\sqrt{2\pi}}\frac{e^{}-i\pi/4}{\sqrt{\omega}}\sqrt{pi}= \frac{1}{\sqrt{\omega}}\frac{1}{\sqrt{2}}\left[\cos\frac{\pi}{4} - i \sin \frac{\pi}{4}\right] = \frac{1}{\sqrt{\omega}}\frac{1}{2}[1-i]
\end{gather*}
\begin{gather*}
    \therefore \quad \text{For any k} \quad F(k) = \frac{1}{\sqrt{\abs{k}}}\frac{1}{2}\left[1+ i\frac{\abs{k}}{k}\right]
\end{gather*}
\begin{gather*}
    \text{Or} \quad F_+(k) = \frac{1}{\sqrt{2k}}e^{i\pi/4} \qquad F_-(k) = \frac{1}{\sqrt{2\pi}}e^{-i\pi/4}
\end{gather*}
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}} \superint dke^{-ikx}F(k) = \frac{1}{\sqrt{2\pi}}\left\{\int_{-\infty}^0dk^{-ikx}F(k) + \zuperint dke^{-ikx}F(k)\right\}
\end{gather*}
\begin{gather*}
    = \frac{1}{\sqrt{2\pi}}\left\{\int_{-\infty}^0 dk \frac{1}{\sqrt{-2k}}e^{-i\pi/4}e^{-ikx} + \zuperint dk \frac{1}{\sqrt{2k}}e^{i\pi/4}e^{-ikx}\right\}
\end{gather*}
\begin{gather*}
    \frac{1}{\sqrt{2\pi}}\left\{\zuperint dk \frac{1}{\sqrt{2k}}e^{-i\pi/k}e^{ikx}+\zuperint dk \frac{1}{\sqrt{2k}}e^{i\pi/4}e^{-ikx}\right\}
\end{gather*}
\begin{gather*}
    = \begin{cases}
    \frac{1}{\sqrt{2\pi}}\left[[e^{-i\pi/4}\cdot e^{i\pi/4}+e^{i\pi/4}e^{-i\pi/4}]\frac{1}{\sqrt{2x}}\sqrt{\pi}\right] \quad &x>0\\
    \frac{1}{\sqrt{2\pi}}\left[[e^{-i\pi/4}\cdot e^{-i\pi/4}+e^{i\pi/4}e^{i\pi/4}]\frac{1}{\sqrt{2x}}\sqrt{\pi}\right] \quad &x<0
    \end{cases}
\end{gather*}
\begin{gather*}
    = \begin{cases}
    x^{-1/2} \quad &x>0\\
    0 \quad &x<0
    \end{cases}
\end{gather*}
\bigskip
Examples(2)
\begin{gather*}
    f(x) = \begin{cases}
    e^x \quad &x>0\\
    0 \quad &x<0
    \end{cases} \text{(Weiner-Hopf method)}
\end{gather*}

Parseval's theorem doesn't hold
\begin{gather*}
    F(k) = \frac{1}{\sqrt{2\pi}}\superint dxe^{ikx}e^x = \frac{1}{\sqrt{2\pi}}\zuperint dx e^{(1+ik)x} = \frac{1}{\sqrt{2\pi}}\frac{1}{1+ik}
\end{gather*}


If blindly calculate the inverse of $f(x)$ then we get
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\superint dk e^{-ikx} \frac{2}{\sqrt{2\pi}}\frac{1}{1+ik} = \frac{1}{2\pi}2\pi i \frac{e^x}{i} =\begin{cases}
    e^x \quad &x<0\\
    0 \quad &x>0
    \end{cases}
\end{gather*}

In other words, we get back $f(x)$ which satisfies Parseval's theorem and not the original (bad) $f(x)$. To obtain correct $f(x)$ do the following.

\begin{gather}
    \text{Let} \quad g(x) = e^{-\tau_0 x}\\
    f(x) =\begin{cases}
    e^{-(\tau_0 - 1)x} \quad x>0\\
    0 \quad &x<0
    \end{cases} \quad \tau_0 = 1+\epsilon
\end{gather}
\begin{gather*}
    G(k) = \frac{1}{\sqrt{2\pi}}\superint dxe^{ikx}g(x) = \frac{1}{\sqrt{2\pi}}\zuperint dxe^{ikx}e^{-\tau_0 x}f(x)
\end{gather*}
\begin{gather*}
    \therefore \quad G(k-i\tau_0) = F(k)
\end{gather*}
\begin{gather*}
    g(x) =\frac{1}{\sqrt{2\pi}}\superint dke^{-ikx}G(k) = \frac{1}{\sqrt{2\pi}}\superint dke^{-ikx}F(k+i\tau_0)
\end{gather*}
\begin{gather*}
    k+i\tau_0 = k'
\end{gather*}
\begin{gather*}
    \therefore \quad g(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty +i\tau_0}^{\infty + -\tau_0}e^{-i(k'-i\tau_0)x}F(k')
\end{gather*}
\begin{gather*}
    \therefore \quad f(x)e^{-\tau_0 x} = \frac{1}{\sqrt{2\pi}}e^{-\tau_0x}\int_{-\infty +i\tau_0}^{\infty + i\tau_0}dke^{-ikx}F(k)
\end{gather*}
\begin{gather*}
    \therefore \quad f(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty + i\tau_0}^{\infty + \tau_0}
\end{gather*}
In our example, $\tau_0 = 1 + \epsilon$, we then get back the original result.\\
In general, if :
\begin{gather*}
    f_+(x) = \begin{cases}
    f(x) \quad &x>0\\
    0 \quad &x<0 
    \end{cases} \quad \text{and} \quad f_+(x) \to Ae^{\tau_+x} \text{ as } x \to +\infty \quad \tau_0>0
\end{gather*} 
then
\begin{gather*}
    g(x) = e^{-\tau_0x}f(x) \quad \tau_0 > \tau_+
\end{gather*}
Then
\begin{gather*}
    F(k) = F_+(k) = \frac{1}{\sqrt{2\pi}}\zuperint dke^{ikx}f_+(x)
\end{gather*}
and
\begin{gather*}
    f_+(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty+i\tau_0}^{\infty + i\tau_0} dke^{-ikx}F_+(k) \quad \tau_0 > \tau_+ 
\end{gather*}
Most general situaion:
\begin{gather*}
    \text{if} \quad f(x) = \begin{cases}
    f_+(x) \quad &x>0\\
    f_-(x) \quad &x<0
    \end{cases}
\end{gather*}
\begin{gather*}
    f_+(x) \to Ae^{\tau_+x} \quad as \quad x \to \infty \quad \tau_+>0
\end{gather*}
\begin{gather*}
    f_-(x) \to Be^{-\tau_-x} \quad as \quad x \to -\infty \quad \tau_->0
\end{gather*}
Then consider
\begin{gather*}
    g_+(x) = \begin{cases}
    e^{-\tau_0x}f_+(x)\quad &x>0\\
    0 \quad & x<0
    \end{cases}
    \qquad 
    g_-(x) = \begin{cases}
    0 \quad &x>0\\
    e^{\tau_1x}f_-(x) \quad &x<0
    \end{cases}\\
    \tau_0>\tau_+ \quad \tau_1 < \tau_-
\end{gather*}
Then
\begin{gather*}
    F_+(k) = \frac{1}{\sqrt{2\pi}}\zuperint dxe^{ikx}f_+(x) ;\qquad F_-(k) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{0}dxe^{ikx}f_-(x)
\end{gather*}
and
\begin{gather*}
    f(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty + i\tau_0}^{\infty + i\tau_0}dke^{-ikx}F_+(x) = \frac{1}{2\pi}\int_{-\infty-\tau_1}^{\infty-i\tau_1}]dke^{-ikx}F_-(k)
\end{gather*}








\end{mdframed}



\begin{mdframed}
\bigskip
\begin{large}
\textbf{Dirac Delta Function}
\end{large}\\
The Dirac delta function, $\delta(x)$, in one dimension, is defined by the following two relations:
\begin{gather*}
    \delta(x) = 0 \quad \text{for} \quad x \neq 0\\
    \superint dx \delta(x) = 1
\end{gather*}
The above two relations then also imply that the limits of integration can be different than $\pm \infty$, as long as they include the point $x = 0$. Indeed, one can write:
\begin{gather*}
    \int_b^c dx \delta(x) = 1 \quad for b<0<c
\end{gather*}
One can extend the definition to include a point that is not the origin.
\begin{gather*}
    \delta(x-a) = 0 \quad \text{for} \quad x\neq a\\
    \superint dx \delta(x-a) = \int_b^c dx \delta(x-a) = 1 \quad \text{for} \quad b<a<c
\end{gather*}
Where $a$ is a real number.
\bigskip
\hline
\underline{\textbf{Properties of the $\delta$ - Function}}\\
Following are some of the interesting properties satisfied by the $\delta$ - function.
\begin{gather*}
    \delta(-x) = \delta(x)
\end{gather*}
This can be proved by noting that $\delta(-x)$ satisfies the same properties as $\delta(x)$ given in (2. 169) and (2. 170), namely, it vanishes for $x\neq0$, and, through change of variables, $x \to -x$, its integral over $(-\infty, \infty)$ is found to be the same as that of $\delta (x)$. This establishes the fact that $\delta(x)$ os am even function of $x$.
\begin{gather*}
    \superint dx f(x) \delta (x-a) = f(a)
\end{gather*}
where $f(x)$ is assumed to be regular, without any singularity, along the interval of integration.
\begin{gather*}
    \superint dxf(x) \delta(x-a) = \int_{a-\epsilon}^{a+\epsilon} dx f(x) \delta(x-a) = f(a) \int_{a-\epsilon}^{a+\epsilon} dx \delta(x-a) = f(a)
\end{gather*}
Where $\epsilon$ is a small positive quantity.
\begin{gather*}
    \delta(ax) = \frac{1}{\abs{a}}\delta(x)
\end{gather*}
This relation can be derived by changing variables, $ax = y$, and taking account of the even nature of the $\delta$-function.
\begin{gather*}
    \delta(x^2-a^2) = \frac{\delta(x-a) + \delta(x+a)}{2\abs{a}}
\end{gather*}
To prove this we first note that both sides vanish for $x\neq a$. Furthermore, we can write the integral on the left-hand side in the following manner. 
\begin{gather*}
    \superint \delta (x^2-a^2) = \superint \delta[((x-a)(x+a))] = \int_{a-\epsilon}^{a+\epsilon} \delta[(-2a(x+a))]+\int_{a-\eta}^{a+\eta} \delta[(2a(x-a))]
\end{gather*}
where $\epsilon$ and $\eta$ are small positive quantities. From (2.177) we recover the right-hand side of (2.178)
\begin{gather*}
    \delta(f(x)) = \sum_n \frac{\delta(x-x_n)}{\abs{\dv[]{f}{x}}_{x=x_n}}
\end{gather*}
where $f(x)$ is a regular function, and $x_n$ are the (real) zeros of $f(x)$. This is a generalization of the cases (d) where there were only two zeros, $x=\pm a$. To prove this one expands $f(x)$ around each of its zeros e.g. around $x=x_n$
\begin{gather*}
    f(x) = f(x_n)+(x-x_n)\left(\dv[]{f}{x}\right)_{x=x_n}+ \cdots = (x-x_n)\left(\dv[]{f}{x}\right)_{x=x_n}
\end{gather*}
where in the last equality we have used the fact that $f(x_n)=0$, and have kept only the leading term in $(x-x_n)$. Therefore,
\begin{gather*}
    \delta(f(x)) = \delta\left[(x-x_n)\left(\dv[]{f}{x}\right)_{x=x_n}\right]
\end{gather*}
Using (2.177) we obtain (2.180)\\
\indent As a strict mathematical function, the relation (2.169) for $\delta(x)$ will imply that the right hand side of (2.170) must also be zero, because a function that is everywhere zero except at one point (or at finite number of points) gives an integral that will also be zero. However, it can be represented in terms of limiting functions.\\
\indent
\underline{\textbf{Representations of the $\delta$-Function}}\\
\indent We consider the following four well-known limiting functions and discuss their role in mathematical problems, particularly in Quantum Mechanics.\\
\indent The most common example of a $\delta$-function behavior in Quantum Mechanical problems, or in Fourier transforms, is the following limiting function:
\begin{gather*}
    (i) \quad \lim_{L\to\infty}\frac{1}{\pi}\frac{\sin Lx}{x}
\end{gather*}
This function oscillates along $\pm x$-axis with zeros at $x = \pm\pi L, \cdots , \pm n \pi L$. Where $n$ is an integer. The spacing between two consecutive zeros is given by $\pi/L$. Therefore, as $L\to\infty$, this spacing becomes increasingly narrower with zeros almost overlapping with each other, while at the same time the function $\sin(Lx/x)$ itself goes to zero as $x\to\pm\infty$. Thus for all practical purposes this function vanishes along the $\pm x$-axis.\\
That is
\begin{gather*}
    \lim_{L\to\infty} \frac{1}{\pi}\frac{\sin(Lx)}{x}\to 0 \quad \text{for} \quad x \neq 0
\end{gather*}
The only exceptional point is the origin itself, $x=0$. At this point $\sin Lx = Lx$ and $\sin (Lx/\pi x)=L/\pi$ which grows as $L\to \infty$. Thus the function goes to infinity at one point, while vanishing everywhere else, which is a classic situtation for a $\delta$-function. As far as its integral is considered one can show, using standard techniques e.g. through the complex variables method, that
\begin{gather*}
    \frac{1}{\pi}\superint dx \frac{\sin Lx}{x}= 1
\end{gather*}
independent of $L$.\\
Thus as a limiting function one can express the $\delta$-function as
\begin{gather*}
    \delta(x) = \lim_{L\to \infty} \frac{1}{\pi}\frac{\sin Lx}{x}
\end{gather*}
The following equivalent integral form
\begin{gather*}
    \frac{1}{2}\int_{-L}^L dk e^{ikx}= \frac{\sin Lx}{x}
\end{gather*}
enables us to write, through the relation (2.186), the following
\begin{gather*}
    \delta(x) = \frac{1}{2\pi}\superint dke^{ikx}
\end{gather*}
where it is understood that
\begin{gather*}
    \frac{1}{2\pi}\superint dk e^{ikx} = \lim_{L\to\infty} \frac{1}{2\pi}\itint{-L}{L}dke^{ikx}
\end{gather*}
More generally, one can write
\begin{gather*}
    \delta(x-a) = \frac{1}{2\pi}\superint dk e^{ik(x-a)}
\end{gather*}
The following Gaussian also mimics a $\delta$-Function behavior
\begin{gather*}
    \lim_{\beta\to 0} \sqrt{\frac{\beta}{\pi}}e^{-\beta x^2}
\end{gather*}
We note that for $x\neq 0 $ the right ahnd side vanishes in the limit $\beta \to 0$, but at $x=0$, in the same limit, it goes to infinity like $\sqrt{\beta/\pi}$this is once again a typical $\delta$-function behavior. From the well-known Gaussian integral.
\begin{gather*}
    \superint dy e^{-y^2} = \sqrt{\pi}
\end{gather*}
we deduce, by a change of variables, $\sqrt{\beta}x=y$, that
\begin{gather*}
    \sqrt{\frac{\beta}{\pi}}\superint dx e^{-\beta x^2} = 1
\end{gather*}
thus one can write
\begin{gather*}
    \delta(x) = \lim_{\beta\to \infty} ]\sqrt{\frac{\beta}{\pi}}e^{-\beta x^2}
\end{gather*}a
Another way of writing the above result is by taking $\beta = \frac{1}{\alpha(t-t')}$, and write a very general condition, replacing $x$ by $(x-x')$
\begin{gather*}
    \delta (x-x') = \lim_{t\to t'} \sqrt{\frac{1}{\alpha(t-t')\pi}}exp\left[-\frac{(x-x')^2}{\alpha(t-t')}\right]
\end{gather*}
Here is another $\delta$-function type behavior
\begin{gather*}
    (iii) \quad \lim_{\epsilon\to -}\frac{1}{2\pi}\frac{\epsilon}{x^2 + \epsilon^2}
\end{gather*}
We find that for $x \neq 0$, this function vanishes in the limit $\epsilon \to 0$. However, if $x=0$, the function behaves like $\frac{1}{\pi\epsilon}$ and goes to infinity in the same limit (the limit has to be taken after the value of $x$ is chosen). Furthermore, one can easily show through standard integration techniques that
\begin{gather*}
    \frac{1}{\pi}\superint dx \frac{\epsilon}{x^2 + \epsilon^2} = 1
\end{gather*}
Once again we can identify
\begin{gather*}
    \delta(x) = \lim_{\epsilon\to 0}\frac{1}{\pi}\frac{\epsilon}{x^2 + \epsilon^2}
\end{gather*}
This form often occurs in the so called dispersion relations and in Green's Function problems e.g.
\begin{gather*}
    \lim_{\epsilon\to 0} \frac{1}{\pi}\zuperint dx' \frac{f(x')}{x'-x-i\epsilon}
\end{gather*}
where one writes
\begin{gather*}
    \lim_{\epsilon\to 0}\left[\frac{1}{x'-x-i\epsilon}\right] = \lim_{\epsilon\to 0}\left[\frac{(x'-x)}{(x'-x)^2+\epsilon^2}\right] + \lim_{\epsilon\to 0}\left[\frac{i\epsilon}{(x'-x)^2+\epsilon^2}\right] = \lim_{\epsilon\to 0}\left[\frac{(x'-x)}{(x'-x)^2+\epsilon^2}\right]+i\pi\delta(x'-x)
\end{gather*}
This relation is often written as
\begin{gather*}
    \frac{1}{x'-x-i\epsilon} = P\left(\frac{1}{x'-x}\right) + i\pi\delta(x'-x)
\end{gather*}
where $P$ in the first term on the right stands for what is called the "principal part". It vanishes at the point $x' = x$ and hence excludes that point when integrated over it. The contribution of the singularity at $x'=x$ is now contained in the second term.\\
Thus
\begin{gather*}
    \lim_{\epsilon\to 0}\frac{1}{\pi}\zuperint dx'\frac{f(x')}{x'-x-i\epsilon} = \frac{1}{\pi}P\zuperint dx'\frac{f(x')}{x'-x}+if(x)
\end{gather*}
Finally an interesting representation is given by the function:
\begin{gather*}
    (iv) \quad \dv[]{}{x}\theta(x)
\end{gather*}
where $\theta(x)$ is called the step-function
\begin{gather*}
    \theta(x) = 0 \quad \text{for} \quad x \leq 0\\
    \text{and} \quad \theta(x) = 1 \quad \text{for} \quad x >0
\end{gather*}
Notice that the derivative on either side of $x=0$ vanishes, but at the point $x=0$ it becomes infinite. The integral can be carried out and is given by
\begin{gather*}
    \superint dx \dv[]{}{x}\theta(x) = \theta(\infty) -\theta(-\infty) = 1
\end{gather*}
We also note that the above result holds if we took the integration limits to be $(-L,L)$. Thus one can write
\begin{gather*}
    \delta(x) = \dv[]{}{x}\theta(x)
\end{gather*}
This $\theta$-function representation appear in Green's function calculations, often for finite-dimensional problems.\\
\indent In summary, we note that for the above representations of $\delta(x)$, as long as $x \neq 0$, each of the functions on the right-hand side vanish. But when they are integrated over an interval that includes the point $x=0$ the result is 1, keeping in mind that the limits are to be taken after the integration is carried out. Thus the limiting process is non-uniform.\\



\textbf{\underline{Three Dimensions}}\\
\indent We define the $\delta$-function in the cartesian system as a product of three one-dimensional $\delta$-functions as follows
\begin{gather*}
    \delta^{(3)}(r-r') = \delta(x-x')\delta(y-y')\delta(z-z')
\end{gather*}
therefore it satisfies
\begin{gather*}
    \delta^{(3)} = 0 \quad \text{if} \quad x\neq x' \quad \text{or} \quad y\neq y' \quad \text{or} \quad z\neq z'
\end{gather*}
and
\begin{gather*}
    \int_{(\infty)} d^3r\delta^{(3)}(r-r') = 1
\end{gather*}
which implies, as in the one-dimensional case, that 
\begin{gather*}
    \int_{(\infty)}d^3rf(r)\delta^{(3)}(r-r') = f(r')
\end{gather*}
Since
\begin{gather*}
    \delta(x-x') = \frac{1}{2\pi}\superint dke^{ik(x-x')}
\end{gather*}
We can express the three-dimensional $\delta$-function as the product
\begin{gather*}
    \delta^{(3)}(r-r') = \left[\frac{1}{2\pi}\superint dk_xe^{ik_x(x-x')}\right]\left[\frac{1}{2\pi}\superint dk_ye^{ik_y(y-y')}\right]\left[\frac{1}{2\pi}\superint dk_ze^{ik_z(z-z')}\right]
\end{gather*}
which can be written in a compact form as
\begin{gather*}
    \delta^{(3)}(r-r') = \frac{1}{(2\pi)^3}\int d^3ke^{ik(r-r')}
\end{gather*}
where $d^3k$ is the three-dimensional volume element in the $k$-space given by 
\begin{gather*}
    d^3k = dk_xdk_ydk_z
\end{gather*}
We have defined the vector \textbf{k} as a three dimensional vector with components $(k_x,k_y,k_z)$ so that
\begin{gather*}
    k \cdot (r-r') = k_x(x-x') + k_y(y-y') + k_z(z-z')
\end{gather*}
In spherical coordinates, $(r,\theta , \phi)$, the $\delta$-function is easily defined, once again, in terms of products of three one-dimensional $\delta$-functions
\begin{gather*}
    \delta_{(3)} = A(r,\theta ,\phi)\delta(r-r')\delta(\theta-\theta')\delta(\phi-\phi')
\end{gather*}
where the relation between the cartesian and spherical coordinates is given by 
\begin{gather*}
    x = r\sin\theta \cos \phi \quad y = r\sin\theta\sin\phi \quad z= r\cos\theta
\end{gather*}
The parameter $A$ in 2.217 can be determined from the definition 2.214.
\begin{gather*}
    1= \int_{(\infty)}d^3r\delta^{(3)}(r-r') = \zuperint dr \itint{0}{\pi}d\theta \itint{0}{2\pi}d\phi J A(r,\theta,\phi)\delta(r-r')\delta(\theta-\theta')\delta(\phi-\phi')
\end{gather*}
where on the right hand side we have converted $d^3r$ from cartesian to spherical coordinates through the relation
\begin{gather*}
    d^3r = dxdydz = Jdrd\theta d\phi
\end{gather*}
with $J$ as the Jacobian, $\pdv{(x,y,z)}{(r,\theta,\phi)}$, defined as
\begin{gather*}
   J = \mqty| \pdv{x}{r} & \pdv{x}{\theta} & \pdv{x}{\phi}\\
    \pdv{y}{r} & \pdv{y}{\theta} & \pdv{y}{\phi}\\
    \pdv{z}{r} & \pdv{z}{\theta} & \pdv{z}{\phi} |
\end{gather*}
We find
\begin{gather*}
    J = r^2\sin\theta
\end{gather*}
Substituting this in the above relation we obtain
\begin{gather*}
    d^3r = r^2\sin\theta drd\theta d\phi
\end{gather*}
Relation 2.219 gives
\begin{gather*}
    \zuperint dr \itint{0}{\phi} d\theta \itint{0}{2\pi} d\phi (r^2\sin\theta)A(r,\theta,\phi)\delta(r-r')\delta(\theta-\theta')\delta(\phi-\phi')
\end{gather*}
Hence
\begin{gather*}
    A(r,\theta,\phi) = \frac{1}{r^2\sin\theta}
\end{gather*}
and
\begin{gather*}
    \delta^{(3)}(r-r') =  \frac{1}{r^2\sin\theta}\delta(r-r')\delta(\theta-\theta')\delta(\phi-\phi')    
\end{gather*}
It is often more convenient to use $\cos \theta$ as a variable rather than $\theta$ so that we can write the integrals more simply
\begin{gather*}
    \zuperint dr \itint{0}{\pi} d\theta \itint{0}{2\pi} d\phi r^2\sin\theta \quad \to \quad \zuperint dr r^2 \itint{-1}{1} d\cos\theta \itint{0}{2\pi} d\phi
\end{gather*}
in which case we obtain the relation
\begin{gather*}
    \delta^{(3)}(r-r') = \frac{1}{r^2}\delta(r-r')\delta(\cos\theta - \cos\theta')\delta(\phi - \phi')
\end{gather*}
This expression is consistent with the relation (2.177) for converting $\delta(\theta-\theta')$ to $\delta(\cos\theta - \cos\theta')$.









\end{mdframed}









\end{document}


\documentclass[12pt,a4paper]{article}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{microtype} 
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}


\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newcommand{\mycomment}[1]{}

\geometry{top=1in, bottom=1in, left=1in, right=1in}


\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Dos Santos}
\rfoot{Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}



\begin{document}

\section{Modern Algebra II}


Before beginning work on anything, start by reviewing the definitions that were important in Math 171.\\

\noindent\underline{Exercise:} If $N\subset H$, then there is a homomorphism $\bar f : G/N \to G'$ such that $\Bar{f}(aN) = f(a)$ for all $a$. In particular, if $N=H$, then $\Bar{f}$ is injective.

\subsection{Solving Polynomials}

\subsubsection{Cubic Equation}

\underline{Exercise:} Solve a cubic equation by Cardano's method.

\subsubsection{Ferrari Quartic Function}
We begin by applying a change of variables which is left as an exercise.
\[
a_4t^4 + a_3t^3 + a_2t^2 + a_1t + x_0 = 0
\]
\[
x^4 = -px^2 - qx - r
\]
\[
(x^2 + a)^2 = x^4 + 2ax^2 + a^2 = (2a - p)x^2 + qx - (a^2 - r)
\]
\[
q^2 - 4ac = 0
\]
\[
\implies q^2 - a(2a-p)(a^2-r) = 0
\]
Which is now a cubic polynomial in $a$. So we solve for $a$ by \underline{Cardano's} method.

\subsubsection{Quintic Equation}

\subsubsection{Solvable}

\textbf{Definition:} A group $G$ is \textit{solvable} if there exists a finite sequence of subgroups $G = H_0 \geq H_1 \geq \cdots \geq H_n = \{e\}$ such that each $H_{i+1}$ is a normal subgroup of $H_i$ (denoted $H_{i+1} \trianglelefteq H_i$) and the quotient group $H_i / H_{i+1}$ is Abelian for all $0 \leq i < n$.\\

\noindent\underline{Corollary:} Any ablien group is solvable.\\

\noindent\underline{Exercise:} Show that for $n\geq 5$ $S_n$ is not solvable.\\


\underline{\textbf{Lemma:}} $f:G\to G'$ hom of groups. $H = \ker f$, $N$ normal subhroup of $G$. Assume $N\subset H$. Then $f$ induces a hom. $\bar f: g/N \to G'$ defined by $\bar f(xN) = f(x), x\in G$
\begin{proof}
    By assumption, $xN = yN$ therefore $x \in yN$ and $y\in xN$ thus $x = y \cdot b \in N$. Since $N\subset H$, $f(b) = e$ so $f(x) = f(y)f(b) = f(y)$.
\end{proof}

\underline{Exercise:} Assume $G$ is solvable. $H$ is a subgroup of $G$. Prove $H$ is solvable.


\newpage

\section{Linear Algebra II}

\subsection{Vector Spaces}

\begin{definition}
    Let $V$ be a vector space, let $U,W$ be subspaces of $V$ define their 'sum', $U+W$ as the following set:
    \[
    U+W = \{u + w : u\in U, w\in W\}
    \]
\end{definition}

We can loosely say without proof $U + W = \mathbb{R}^2$

\begin{center}
    
\begin{tikzpicture}[scale=1.5, >=Stealth]
    % Draw axes
    \draw[<->] (-2,0) -- (2,0) node[right] {$x$};
    \draw[<->] (0,-2) -- (0,2) node[above] {$y$};

    % Draw subspace U
    \draw[thick, blue, ->] (0,0) -- (1.5,1) node[anchor=south west] {$U$};
    % Draw subspace W
    \draw[thick, red, ->] (0,0) -- (-1,1.5) node[anchor=south east] {$W$};

    % Draw U + W (the entire plane here is implied to be U + W)
    \node at (1.5,-1.5) [circle,fill,inner sep=1pt]{};
    \node at (1.5,-1.5) [right] {$U + W = \mathbb{R}^2$ (implied)};

    % Optional: Highlighting the plane to indicate U+W, uncomment the line below
    % \fill[green!20,opacity=0.3] (-2,-2) rectangle (2,2); % This fills the entire visible area to represent U+W

\end{tikzpicture}

\end{center}


\begin{definition}
    We say that a sum $U+W$ is \underline{direct} if for all $v \in U+W$ there are unique $u,v$ such that $v = u + v$. In this case we can write:
    \[
    U\oplus V
    \]
\end{definition}

\underline{Example 1:} $U + V$ Direct\\

Since $(a,0) + (0,b) = (a,b)$ is the only way to write $(a,b)$.\\

\underline{Example 2:} $U + Y + V$ not direct.\\

Since $(a,0) + (0,b) + (0,0) = (a,b)$ and $(a+1,0) + (0,b-1) + (-1,1) = (a,b)$\\


\begin{definition}
    $U+V$ is direct $\iff$ $U\cap W = \{0\}$
\end{definition}

\subsection{Transformations}

\begin{definition}
    Suppose $T:V\to W$ is a linear map, then the kernel of $T$ is the subspace of $V$ defined by 
    \[
    \ker(T) = \{v\in V : T(v) = 0\}
    \]
\end{definition}
\begin{definition}
    The image of $T$ is $T$ applied to entire domain.
\end{definition}

\underline{Exercise:} Consider $T:C^1(\mathbb{R}) \to C^0(\mathbb{R})$. What is the kernel and the image.\\

Every continuous function is integrable so use FTC 1. I.E. The map is surjective.\\

The kernel is all constant functions as $\dv{}{x} C = 0$.












\begin{algorithm}[H]
    \caption{Find Maximum Value}
    \label{alg:find_max}
    \SetAlgoLined
    \KwData{An array $array$ of length $n$}
    \KwResult{The maximum value in the array}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{An array $array$}
    \Output{The maximum value $max$}
    
    $max \leftarrow array[0]$\;
    \For{$i \leftarrow 1$ \KwTo $n$}{
        \If{$array[i] > max$}{
            $max \leftarrow array[i]$\;
        }
    }
    \Return{$max$}\;
\end{algorithm}

Algorithm~\ref{alg:find_max} outlines the procedure for finding the maximum value in an array.

\end{document}



\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{tikz}
\usetikzlibrary{positioning, arrows}

\title{script}
\author{tomas dos santos}
\date{June 2024}

\begin{document}

\section*{Projectivizing}
\begin{itemize}
        \item \textbf{Projection:}
        \begin{itemize}
            \item By leveraging the properties of the special linear group $SL_2(\mathbb{R})$, we can projectivize the cocycles. This means we transform our system into one where the state space is the projective line $\mathbb{RP}^1$. Specifically, we look at:
            \[
            (T, A): \Omega \times \mathbb{RP}^1 \rightarrow \Omega \times \mathbb{RP}^1
            \]
            Here, $\Omega$ is our base space, $T$ is a transformation on $\Omega$, and $A$ represents the cocycle acting on $\mathbb{RP}^1$. This projection is crucial because it allows us to focus on the angular component in $\mathbb{RP}^1$, simplifying the analysis of the dynamics.
        \end{itemize}

        \item \textbf{Commutative Diagram:}
        \begin{itemize}
            \item We seek a map $U$ such that the following diagram commutes:
        \end{itemize}
    \end{itemize}

    \[
    \begin{tikzpicture}[>=stealth, node distance=3cm, auto]
        \node (A1) at (0, 1.5) {$\Omega \times \mathbb{R}^2$};
        \node (A2) at (3, 1.5) {$\Omega \times \mathbb{R}^2$};
        \node (P1) at (0, 0) {$\Omega \times \mathbb{RP}^1$};
        \node (P2) at (3, 0) {$\Omega \times \mathbb{RP}^1$};
        
        \draw[->] (A1) to node {$(T, A)$} (A2);
        \draw[->] (A1) to node [swap] {$U$} (P1);
        \draw[->] (A2) to node {$U$} (P2);
        \draw[->] (P1) to node [swap] {$(T, \tilde{A})$} (P2);
    \end{tikzpicture}
    \]

    \begin{itemize}
        \item This approach of projectivizing cocycles is not limited to $\mathbb{RP}^1$; it can also be applied to complex projective spaces like $\mathbb{CP}^1$. This generalization further extends the utility of projectivization in analyzing more complex systems.
    \end{itemize}

    By projectivizing the cocycles, we transform a potentially complicated linear dynamic system into a simpler form that focuses on the projective space's angular dynamics, making the analysis more tractable. This process helps in understanding the long-term behavior and stability of the system.


\section*{Example}

\textbf{Proof of the Implication $(\impliedby)$}

    First, we consider the implication:
    \[ | \text{Tr}(A) | > 2 \implies (T, A) \in \mathcal{UH} \]

    \begin{itemize}
        \item If $| \text{Tr}(A) | > 2$, then $A$ is a hyperbolic matrix. This implies that $A$ has two distinct real eigenvalues, $\lambda$ and $\frac{1}{\lambda}$, with $| \lambda | > 1$.
        \item Let $v$ be a normalized eigenvector associated with the eigenvalue $\lambda$. Then, applying $A^n$ to $v$ gives:
        \[
        || A^n v || = | \lambda^n | \cdot || v || = | \lambda |^n || v || = | \lambda |^n
        \]
        Since $| \lambda | > 1$, we have exponential growth in the norm of $A^n v$. Specifically, $|| A^n v || > \frac{1}{2} | \lambda |^n$ for sufficiently large $n$.
        \item This exponential growth condition implies that $(T, A) \in \mathcal{UH}$, meaning the operator is uniformly hyperbolic.
    \end{itemize}

    \vspace{0.4cm}

    \textbf{Proof of the Implication $(\implies)$}

    Next, we consider the reverse implication:
    \[ (T, A) \in \mathcal{UH} \implies | \text{Tr}(A) | > 2 \]

    \begin{itemize}
        \item If $| \text{Tr}(A) | \leq 2$, then the eigenvalues of $A$ must satisfy $| \lambda | \leq 1$. This means the eigenvalues do not exhibit exponential growth.
        \item Since $| \lambda | \leq 1$, the operator $A$ fails the exponential growth condition required for uniform hyperbolicity.
        \item Consequently, if $| \text{Tr}(A) | \leq 2$, we have $(T, A) \notin \mathcal{UH}$.
    \end{itemize}

\section*{Kingman's}

First, let's consider the setup. We have a measure space $(X, \mathcal{B}, \mu)$ equipped with a measure-preserving transformation $T$. We assume that this space is ergodic. In simple terms, ergodicity means that any subset of $X$ that remains unchanged by $T$ either has full measure or zero measure. This property is crucial as it ensures that our system does not decompose into smaller, invariant subsystems.

    Now, onto the theorem itself:

    \textbf{Kingman's Theorem} states that if we have a sequence of functions $\{f_n\} : X \rightarrow \mathbb{R}$ that are integrable (in $L^1$), and these functions satisfy the subadditive condition:
    \[
    f_{n+m}(\omega) \leq f_n(\omega) + f_m(T^n\omega)
    \]
    then there exists a constant function $f$ such that:
    \[
    \lim_{n\rightarrow \infty} \frac{1}{n} f_n(\omega) = f \geq -\infty
    \]
    for almost every $\omega \in X$.

    In simpler terms, this means that as $n$ grows, the average value of $f_n$ converges to some constant $f$ almost everywhere in $X$.

    Moreover, if we have a uniform bound on these functions, specifically $||f_n||_\infty \leq Cn$ for some constant $C$, then this constant $f$ can be explicitly characterized as:
    \[
    f = \inf_{n \geq 1} \frac{1}{n} \int f_n \, d\mu
    \]

    To summarize, Kingman's Theorem provides a powerful tool for understanding the long-term behavior of subadditive processes in ergodic systems. It guarantees convergence to a constant and even gives us a way to calculate this constant under certain conditions.

    Thank you for your attention, and I hope you found this explanation of Kingman's Theorem insightful!

\section*{Non U Hype}

Finally, let's discuss what it means for our operator to be non-uniformly hyperbolic, denoted as $(T, A) \in \mathcal{NUH}$.

    \vspace{0.4cm}

    \begin{definition}
    \centering
        $(T,A) \in \mathcal{NUH}$ if $(T,A) \notin \mathcal{UH}$ and $L(A) > 0$.
    \end{definition}
    
    \vspace{0.4cm}
    
    We note that $(T,A) \in \mathcal{UH} \implies L(A) > 0$. This relationship highlights that the condition $L(A) > 0$ alone does not suffice to classify an operator as uniformly hyperbolic. Therefore, the definition of non-uniformly hyperbolic operators is non-trivial and meaningful in distinguishing between different types of hyperbolicity.

    \vspace{0.4cm}

    In summary, understanding non-uniformly hyperbolic operators allows us to explore more complex dynamical behaviors that do not fit into the strict framework of uniform hyperbolicity but still exhibit significant chaotic characteristics. This concept is crucial in the study of dynamical systems and their stability properties.



\end{document}



\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{microtype} 
\usepackage{geometry}
\usepackage{hyperref} 
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}
\geometry{top=1in, bottom=1in, left=1in, right=1in}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newcommand{\mycomment}[1]{}



\pagestyle{fancy}
\fancyhf{}
\lhead{Spring 2024}
\rhead{Tomas Sbardelotto Dos Santos}
\rfoot{Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}


\def\Z{\mathbb Z}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\title{}
\date{}
\begin{document}
{\bf Homework 1}\\
{\bf Due date: Monday, April 8 at 11:59pm}
\begin{enumerate}
\item
Let $X$ be the set of continuous functions on $[0,1]$. Let $\varphi \colon
[0,1] \to (0,\infty)$ be continuous. Define
\begin{equation*}
d(f,g) := \int_0^1 |f(x)-g(x)|\varphi(x)~dx .
\end{equation*}
Show that $(X,d)$ is a metric space. \\
\begin{mdframed}
    \textbf{Solution:}
    \begin{proof}.\\
        1. For some $f,g\in X$, suppose $d(f,g) = 0$:\\
        
        Let $f,g\in X$ since we have $\varphi(x) > 0$ and $f,g,\varphi$ are continuous functions which are non-negative, the integral will only be $0$ if
        \[
        \abs{f(x)-g(x)}\varphi(x) \equiv 0
        \]
        This means $\abs{f(x) - g(x)} = 0 \implies f(x) = g(x)$ for all $x$.
        
        2. For any $f,g \in X$, $d(f,g) = d(g,f)$:\\
        
        Since $\abs{f(x) - g(x)} = \abs{g(x) - f(x)}.$
        \[
        d(f,g) = \int_0^1 \abs{f(x) - g(x)}\varphi(x) \; dx = \int_0^1 \abs{g(x) - f(x)}\varphi(x) \; dx = d(g,f)
        \]
        3. For any $f,g,h \in X$, $d(f,h) \leq d(f,g) + d(g,h)$:\\
        \begin{align*}
        d(f,h) &= \int_0^1 \abs{f(x) - h(x)}\varphi(x)\;dx\\
        &= \int_0^1 \abs{f(x) - h(x) + g(x) - g(x)}\varphi(x)\;dx\\
        &\leq \int_0^1 (\abs{f(x) - g(x)} + \abs{g(x) - h(x)}) \varphi(x) \; dx\\
        &= d(f,g) + d(g,h)
        \end{align*}
        4. Finally, since $\varphi(x) > 0$ and $\abs{f(x) - g(x)} \geq 0$ the integral is non-negative. 

        Thus $(X,d)$ is a metric.
        
    \end{proof}
\end{mdframed}

\newpage

\item Let $C^1([a,b],\R)$ be the set of once continuously differentiable
functions on $[a,b]$.
Define
\begin{equation*}
d(f,g) := \|{f-g}\|_u + \|{f'-g'}\|_u,
\end{equation*}
where $\|{\cdot}\|_u$ is the uniform norm. Prove that $d$ is a metric. \\

\begin{mdframed}
    \textbf{Solution:}
    \begin{proof}
        The uniform norm is defined as:
        \[
        \norm{f(x)}_u = \sup_{x\in[a,b]} \abs{f(x)}
        \]
        1. $d(f,g)$ is clearly non-negative since we are taking the absolute values. \\
        
        2. Let $f = g$ then, $f'=g'$ and for all $x\in[a,b]$
        \[
        \abs{f(x)-g(x)} = 0 = \abs{f'(x) - g'(x)}
        \]
        So the supremum of both over $[a,b]$ will also be $0$.\\

        Now let $d(f,g) = 0$, then
        \[
        \sup_{x\in[a,b]} \abs{f(x) - g(x)} + \sup_{x\in[a,b]} \abs{f'(x) - g'(x)} = 0
        \]
        Since both terms are non-negative, they must be zero. In other words, the supremum of the difference between $f,g$ is on the interval $[a,b]$ is zero. So $f=g$.\\

        3. By the property that $\abs{f(x) - g(x)} = \abs{g(x) - f(x)}$
        \[
        d(f,g) =  \sup_{x\in[a,b]} \abs{f(x) - g(x)} + \sup_{x\in[a,b]} \abs{f'(x) - g'(x)} = d(g,f)
        \]
        4. Finally, for any $f,g,h \in C^1([a,b])$
        \[
        d(f,h) = \norm{f-h} + \norm{f'-h'} = \norm{f - g + g - h} + \norm{f' - g' + g' - h'}
        \]
        \[
        \leq \norm{f-g} + \norm{g-h} + \norm{f'-g'} + \norm{g'-h'} = d(f,g) + d(g,h)
        \]
        where the inequality follows from the properties of the uniform norm and the triangle inequality for absolute values.\\

        Thus $d$ is a metric on $C^1([a,b])$.
    \end{proof}
\end{mdframed}

\item Let $\left(X, d_X\right)$ and $\left(Y, d_Y\right)$ be metric spaces.
\begin{enumerate}
\item Show that $(X \times Y, d)$ with $d\left(\left(x_1, y_1\right),\left(x_2,
y_2\right)\right):=d_X\left(x_1, x_2\right)+d_Y\left(y_1, y_2\right)$ is a metric
space.
\item Show that $(X \times Y, d)$ with $d\left(\left(x_1, y_1\right),\left(x_2,
y_2\right)\right):=\max \left\{d_X\left(x_1, x_2\right), d_Y\left(y_1, y_2\right)\right\}$ is a metric space.
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}\\
    (a)
    \begin{proof}
        1. $d_X$ and $d_Y$ are metric spaces. So for any elements in $X$ and $Y$, $d_X$ and $d_Y$ are non-negative so $d_X + d_Y$ will also be non-negative.\\
        
        2. Suppose $d_X((x_1,y_1),(x_2,y_2)) = 0$.
        \[
        d_X(x_1,x_2) + d_Y(y_1,y_2) = 0
        \]
        Since both $d_X,d_Y$ are non-negative, $x_1=x_2$ and $y_1=y_2.$\\

        Now suppose $(x_1,y_1) = (x_2,y_2)$ then
        \[
        d((x_1,y_1),(x_2,y_2)) = d_X(x_1,x_2) + d_Y(y_1,y_2) = 0 + 0 = 0
        \]
        Since $(X,d)$ and $(Y,d)$ are also metrics.\\
        3. 
        \[
        d((x_1,y_1),(x_2,y_2)) = d_X(x_1,x_2) + d_Y(y_1,y_2) = d_X(x_2,x_1) + d_Y(y_2,y_1)
        \]
        \[
        = d((x_2,y_2),(x_1,y_1))
        \]
        4. Let $(x_1,y_1),(x_2,y_2),(x_3,y_3) \in X\times Y$ then
        \[
        d((x_1,y_1),(x_3,y_3)) =  d_X(x_1,x_3) + d_Y(y_1,y_3)
        \]\[
         \leq d_X(x_1,x_2) + d_Y(y_1,y_2) + d_X(x_2,x_3) + d_Y(y_2,y_3)
        \]\[
        = d((x_1,y_1),(x_2,y_2)) + d((x_2,y_2),(x_3,y_3))
        \]
    \end{proof}

    (b)

    \begin{proof}
          1. $d_X$ and $d_Y$ are metric spaces. So for any elements in $X$ and $Y$, $d_X$ and $d_Y$ are non-negative so $\max\{d_X(x_1,x_2),d_Y(y_1,y_2)\}$ will also be non-negative.\\
          
         2. Suppose $d((x_1,y_1),(x_2,y_2)) = 0$.
        \[
        \max\{d_X(x_1,x_2),d_Y(y_1,y_2)\} = 0
        \]
        Since both $d_X,d_Y$ are non-negative, $x_1=x_2$ and $y_1=y_2.$\\

        Now suppose $(x_1,y_1) = (x_2,y_2)$ then
        \[
        d((x_1,y_1),(x_2,y_2)) = \max\{d_X(x_1,x_2),d_Y(y_1,y_2)\} = \max\{0,0\} = 0
        \]
        Since $(X,d)$ and $(Y,d)$ are also metrics.\\
        3. 
        \[
        d((x_1,y_1),(x_2,y_2)) = \max\{d_X(x_1,x_2),d_Y(y_1,y_2)\} = \max\{d_X(x_2,x_1),d_Y(y_2,y_1)\}
        \]
        \[
        = d((x_2,y_2),(x_1,y_1))
        \]
        4. Let $(x_1,y_1),(x_2,y_2),(x_3,y_3) \in X\times Y$ then
        \[
        d((x_1,y_1),(x_3,y_3)) =  \max\{d_X(x_1,x_3),d_Y(y_1,y_3)\}
        \]\[
         \leq \max\{d_X(x_1,x_2),d_Y(y_1,y_2)\} + \max\{d_X(x_2,x_3),d_Y(y_2,y_3)\}
        \]\[
        = d((x_1,y_1),(x_2,y_2)) + d((x_2,y_2),(x_3,y_3))
        \]
    \end{proof}
\end{mdframed}

\item For every $x \in \R^n$ and every $\delta > 0$ define the {rectangle}
$$R(x,\delta) :=
(x_1-\delta,x_1+\delta) \times
(x_2-\delta,x_2+\delta) \times \cdots \times
(x_n-\delta,x_n+\delta).$$
\noindent Show that these sets generate the same open
sets as the balls in standard metric. That is, show that a set $U \subset \R^n$
is open in the sense of the standard metric if and only if for every
point $x \in U$, there exists a $\delta > 0$ such that $R(x,\delta) \subset
U$. \\
\begin{mdframed}
    \textbf{Solution:}\\
    \begin{proof}
        Let $U\subset\R^n$ be open, then by definition for every point $x\in U$ there is an epsilon neighborhood around $x$ that is contained in $U$. Thus we choose delta small enough such that $R(x,\delta) \subset B_\varepsilon (x)$. To show the converse, notice $R(x,\delta) \subset U$ is open in $\R^n$ and is completely contained in $U$ for every $x$ there is a neighborhood of $x$ contained in $U$ which satisfies the idea of openness.
    \end{proof}
\end{mdframed}


\item Show that in any metric space, every open set can be written as a union of
closed sets.
\begin{mdframed}
    \textbf{Solution:}
    \begin{proof}
        Metric spaces are Hausdorff. Singletons are closed in Hausdorff spaces. Any set $U$ can be expressed as
        \[
        U = \bigcup_{x\in U}\{x\} 
        \]
        Additionally we could say
        \[
        U = \bigcup_{n\in\mathbb{N}} \left\{ x\in X \;:\; d(x,U^c) \geq \frac{1}{n} \right\}
        \]
    \end{proof}
\end{mdframed}

\item Let $X$ be a set and $d_1$, $d_2$ be two metrics on $X$.
Suppose there exists an $\alpha > 0$ and $\beta > 0$
such that
$$\alpha \, d_1(x,y) \leq d_2(x,y) \leq \beta \, d_1(x,y)\;\text{ for all}\; x,y \
in X.$$
\noindent Show that $U$ is open in $(X,d_1)$ if and only if $U$ is open in $
(X,d_2)$. \\
\end{enumerate}
\begin{mdframed}
    \textbf{Solution:}
    \begin{proof}
        Let $U$ be open in $(X,d_1)$. Then for all points $x\in U$, there is an $r$ such that the ball $B_r(x) = \{ y : d_1(x,y) <  r / \beta\} \subset U$. Since $d_2(x,y) \leq \beta d_1(x,y)$ the ball $B' = \{y : d_2(x,y) < r \}$ will be in $U$
        \[
        B_r(x) \subset U \implies B'_r(x) = \{y : d_2(x,y) < r \} \subset U
        \]
        Thus for each $x$ we found an open ball contained in $U$.\\

        Now suppose $U$ is open in $X, d_2$ then for any $x\in U$ we can find $\epsilon$ such that $B_\epsilon (x) = \{y : d_2(x,y) < \epsilon / \alpha \} \subset U$. Once again,
        \[
       B_r(x) \subset U \implies B'_r(x) = \{y : d_1(x,y) < \epsilon\} \subset U
        \]
        So we found an open ball around $x$ contained in $U$.
    \end{proof}
    
\end{mdframed}



\end{document}



\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}





\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\intab}[1]{\int_{a}^{b} #1 \, dx}
\newcommand{\defint}[2]{\int_{#1}^{#2}}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newenvironment{exercise}[3][]
{\begin{tcolorbox}[title={#2\ #1},colback=white]}
{\end{tcolorbox}}
\newcommand{\newexercise}[1]{\noindent {\textbf{#1}}\hrulefill}



\usepackage{amsmath, amsthm, amsfonts, amssymb}

\begin{document}


\noindent Tomas Sbardelotto dos Santos
\begin{center}
    Real Anal Homework 4
\end{center}


\noindent Section 6.1: 2, 5, 12, 14\\
\vspace{0.2cm}
\newexercise{2. }
\begin{proof}
    (a) We consider the pointwise limit of the sequence of functions $f_n(x) = \frac{e^{\frac{x}{n}}}{n}$ for $x \in \mathbb{R}$. We have
    \[
    \lim_{n\to\infty} f_n(x) = \lim_{n\to\infty} \frac{e^{\frac{x}{n}}}{n}.
    \]
    Since $e^{\frac{x}{n}} \to 1$ as $n \to \infty$ for any fixed $x$, and $\frac{1}{n} \to 0$, the limit of the product is $1 \cdot 0 = 0$. Hence, the pointwise limit of $f_n(x)$ is $0$ for all $x \in \mathbb{R}$.

    (b) To show that this convergence is not uniform on $\mathbb{R}$, consider any $x = n\ln (n\epsilon)$ for $n > \frac{1}{\epsilon}$. For this choice of $x$, we have
    \[
    \left|\frac{e^{\frac{x}{n}}}{n} - 0\right| = \left|\frac{e^{\ln (n\epsilon)}}{n}\right| = \epsilon,
    \]
    which does not converge to $0$ as $n \to \infty$. Therefore, the convergence is not uniform on $\mathbb{R}$.

    (c) If we restrict $x$ to the interval $[0,1]$, then we consider the maximum value of the function $e^x$ on this interval, which is $e^1 = e$. For a given $\epsilon > 0$, we choose $N > \frac{e}{\epsilon}$. Then for all $n \geq N$ and $x \in [0,1]$, we have
    \[
    \left|\frac{e^{\frac{x}{n}}}{n} - 0\right| \leq \left|\frac{e^{\frac{1}{n}}}{n}\right| \leq \frac{e}{n} < \epsilon.
    \]
    Hence, the convergence is uniform on the interval $[0,1]$.
\end{proof}

\newexercise{5. }
\begin{proof}
    Suppose $f_n$ and $g_n$ converge uniformly respectively. Then for any $\epsilon>0$ there is $N_1, N_2$ such that when $n>N_1$ and $n>N_2$
    \[
    \abs{f_n(x) -f(x)}<\epsilon/2 \quad \text{and} \quad \abs{g_n(x)-g(x)}<\epsilon/2
    \]
    Choose $M = \max{N_1,N_2}$ then for any $n\geq M$, by triangle inequality
    \[
    \abs{(f_n(x)+g_n(x))-(f(x)+g(x))} \leq \abs{f_n(x)-f(x)} + \abs{g_n(x)-g(x)} < \frac{\epsilon}{2} + \frac{\epsilon}{2} =\epsilon
    \]
    Thus $f_n+g_n$ converges uniformly. 
    
\end{proof}

\newexercise{12. }

\begin{proof}
    Since $f(x)$ is bounded, there exists $M$ such that $\abs{f(x)}\leq M$ for all $x$. Since $f_n$ converges uniformly to $f$ then for any $\epsilon>0$ there is $N$ such that when $n\geq N$
    \[
    \abs{f_n(x) - f(x)} < \epsilon
    \]
    Applying the triangle inequality we attain
    \begin{align*}
    \abs{f_n(x)} &\leq \abs{f_n(x) - f(x)} + \abs{f(x)}\\
                &< \epsilon + M
    \end{align*}
    Which shows that $f_n(x)$ is bounded.
\end{proof}

\newexercise{14. }

\begin{proof}
(a) We want to show that the sequence of functions \( f_n(x) = \sum_{k=0}^n x^k \) converges uniformly to \( \frac{1}{1-x} \) for \( x \in [-c, c] \) whenever \( 0 \leq c < 1 \). For any \( \epsilon > 0 \), we need to find \( N \) such that for all \( n \geq N \) and all \( x \in [-c, c] \),
\[
\left| \sum_{k=0}^n x^k - \frac{1}{1-x} \right| < \epsilon.
\]
The \( n^{th} \) partial sum for the geometric series is \( \frac{1-x^{n+1}}{1-x} \). Plugging this into our inequality, we get
\[
\left| \frac{1-x^{n+1}}{1-x} - \frac{1}{1-x} \right| = \left| \frac{-x^{n+1}}{1-x} \right| < \left| \frac{\abs{c}^{n+1}}{1-x} \right| \leq \abs{\frac{\abs{c}^{N+1}}{\abs{1-\abs{c}}}} < \epsilon.
\]
Since \(c < 1 \), \( \abs{c}^{n+1} \) goes to 0 as \( n \) goes to infinity and \(\frac{1}{1-c}\) is finite. Thus, we can find \( N \) such that for all \( n \geq N \), \( \left| \frac{x^{n+1}}{1-x} \right| < \epsilon \) for all \( x \in [-c, c] \), proving uniform convergence.

(b) If we consider \( x \in (-1, 1) \), we have that the sequence of functions converges pointwise to \( \frac{1}{1-x} \) if and only if 
\[
\sup_{x \in (-1, 1)} \left| \frac{1-x^{n+1}}{1-x} - \frac{1}{1-x} \right| = \sup_{x \in (-1, 1)} \left| \frac{x^{n+1}}{1-x} \right|
\]
approaches 0 as \( n \) goes to infinity. However, as \( x \) approaches 1, \( \frac{x^{n+1}}{1-x} \) does not approach 0. In fact, it approaches infinity. Hence, the sequence does not converge uniformly on \( (-1, 1) \).
\end{proof}


\noindent Section 6.2: 1, 15, 18, 20


\newexercise{1. }

\begin{proof}
The derivative of \( f_n(x)  = \sqrt{x^2 + (\frac{1}{n})^2} \) is given by
\[
\dv{f_n}{x} = \frac{x}{\sqrt{x^2 + \left(\frac{1}{n}\right)^2}},
\]
which is defined for all \( x \in [-1, 1] \) since \( \frac{1}{n} \) is nonzero for all \( n \in \mathbb{N} \). The pointwise limit of \( f_n(x) \) as \( n \to \infty \) is \( \sqrt{x^2} \), which simplifies to \( |x| \). We will now show that this convergence is uniform.

For every \( x \in [-1, 1] \) and \( n \in \mathbb{N} \),
\[
|f_n(x) - |x|| = \left|\sqrt{x^2 + \left(\frac{1}{n}\right)^2} - |x|\right|.
\]
Since \( x^2 \geq 0 \), we have \( \sqrt{x^2} = |x| \). Therefore,
\[
|f_n(x) - |x|| = \left|\sqrt{x^2 + \left(\frac{1}{n}\right)^2} - \sqrt{x^2}\right| = \left|\sqrt{x^2 + \left(\frac{1}{n}\right)^2} - |x|\right|.
\]
Applying the estimation for the difference of square roots, we get
\[
|f_n(x) - |x|| \leq \frac{\left(\frac{1}{n}\right)^2}{\sqrt{x^2 + \left(\frac{1}{n}\right)^2} + |x|} \leq \frac{\left(\frac{1}{n}\right)^2}{\sqrt{\left(\frac{1}{n}\right)^2}} = \frac{1}{n}.
\]
Since the right-hand side does not depend on \( x \) and goes to 0 as \( n \) approaches infinity, we have uniform convergence of \( f_n \) to \( |x| \) on \([-1, 1]\).

However, note that \( |x| \) is not differentiable at \( x = 0 \), which shows that the uniform limit of differentiable functions may not be differentiable.
\end{proof}


\newexercise{15. }


Part A:

\begin{proof}
    We know that since each $f_n : \mathbb{R} \to \mathbb{R}$ are continuous then the restriction $f_n:[-k,k] \to \mathbb{R}$ must also be continuous for all $k\in \mathbb{N}$. Since $f_n$ converges uniformly to $f$, by theorem 6.2.2  we know that the limit $f: [-k,k] \to \mathbb{R}$ is also continuous. Since $k$ is arbitrary $f:\mathbb{R} \to \mathbb{R}$ must be continuous.
\end{proof}
Part B:
\begin{proof}
    Since $f_n$ converges uniformly to $f$ on compact subsets and $[-k,k]$ is a compact interval, $f_n$ converges uniformly to $f$ for every $k\in \mathbb{N}$. Thus let $k$ be such that $[a,b] \subseteq [-k,k]$. Since each $f_n$ is Riemann integrable and converges uniformly to $f$ then by theorem 6.2.4, $f$ is uniformly continuous and 
    \[
    \int_a^b f(x) \; dx = \lim_{n\to\infty} \int_a^b f(x) \; dx
    \]
\end{proof}

\newexercise{18. }


Part A:
\begin{proof}

Consider the sequence \( \{f_n\} \) defined by \( f_n(x) = \sqrt{x + \frac{1}{n}} \) for \( x \in [0,1] \). Each function \( f_n \) is Lipschitz continuous on \( [0,1] \) because the derivative of \( f_n \), which is \( \frac{1}{2\sqrt{x + \frac{1}{n}}} \), is bounded on the interval \( [0,1] \) for each \( n \).

We claim that \( \{f_n\} \) converges uniformly to \( f(x) = \sqrt{x} \). For all \( x \in [0,1] \) and for all \( n \), we have
\[ |f_n(x) - f(x)| = \left|\sqrt{x + \frac{1}{n}} - \sqrt{x}\right| = \frac{\frac{1}{n}}{\sqrt{x + \frac{1}{n}} + \sqrt{x}} \leq \frac{1}{n} \to 0 \text{ as } n \to \infty. \]
Therefore, \( f_n \) converges uniformly to \( f \) on \( [0,1] \).

However, \( f(x) = \sqrt{x} \) is not Lipschitz continuous on \( [0,1] \) because its derivative, \( \frac{1}{2\sqrt{x}} \), becomes unbounded as \( x \) approaches 0.

\end{proof}

Part B:

\begin{proof}

Assume \( f_n \) are Lipschitz with uniform Lipschitz constant \( K \), that is for all \( n \) and for all \( x, y \in S \),
\[ |f_n(x) - f_n(y)| \leq K|x - y|. \]

Given \( \{f_n\} \) converges pointwise to \( f \), we want to show that for all \( x, y \in S \),
\[ |f(x) - f(y)| \leq K|x - y|. \]

Since \( f_n \to f \) pointwise, for any \( x, y \in S \) and for any \( \epsilon > 0 \), there exists \( N \) such that for all \( n \geq N \),
\[ |f_n(x) - f(x)| < \frac{\epsilon}{2} \quad \text{and} \quad |f_n(y) - f(y)| < \frac{\epsilon}{2}. \]

Now consider for \( n \geq N \),
\begin{align*}
|f(x) - f(y)| &\leq |f(x) - f_n(x)| + |f_n(x) - f_n(y)| + |f_n(y) - f(y)| \\
&< \frac{\epsilon}{2} + K|x - y| + \frac{\epsilon}{2} \\
&= K|x - y| + \epsilon.
\end{align*}
Since \( \epsilon \) is arbitrary, we have \( |f(x) - f(y)| \leq K|x - y| \). Thus, \( f \) is Lipschitz continuous with Lipschitz constant \( K \).
\end{proof}



\newexercise{20. }

PART A:

\begin{proof}
Since \( f \) is analytic in \((a - \rho, a + \rho)\), it can be represented by its Taylor series at \( a \):
\[
f(x) = \sum_{n=0}^{\infty} c_n (x - a)^n
\]

We aim to prove by induction that $c_n = 0$ for all $n \geq 0$.


Given $f(x) = 0$ for all $x \in (a - \rho, a + \rho)$, we have:

\[
f(a) = c_0 + \sum_{n=1}^{\infty} c_n (a - a)^n = c_0
\]

Since $f(a) = 0$, it follows that $c_0 = 0$.


Assume $c_k = 0$ for all $k \leq N$, where $N$ is an arbitrary but fixed positive integer.


Considering the $(N+1)$-th derivative of $f$ at $x = a$:

\[
f^{(N+1)}(a) = \sum_{n=N+1}^{\infty} \frac{n!}{(n-(N+1))!} \cdot c_n \cdot (a - a)^{n-(N+1)}
\]

This simplifies to:

\[
f^{(N+1)}(a) = (N+1)! \cdot c_{N+1}
\]

Given that $f(x) = 0$ for all $x$, all its derivatives at $x = a$ are also zero, which implies:

\[
f^{(N+1)}(a) = 0 \implies c_{N+1} = 0
\]

By induction, $c_n = 0$ for all $n \geq 0$

Given that \( c_n = 0 \) for all \( k \), the coefficients of this Taylor series are zero:
\[ c_n = 0 \quad \text{for all } n. \]


Thus, every term in the series for \( f(x) \) is zero, and we have \( f(x) = 0 \) for all \( x \in (a - \rho, a + \rho) \).
\end{proof}

PART B:

\begin{proof}
From part a, we know that \( f(x) = 0 \) for all \( x \) in the small interval around \( a \), hence all the derivatives of \( f \) at \( a \) must be zero. By the principle of analytic continuation, if an analytic function is zero on any open interval, then its Taylor series is identically zero on the interval of convergence.

Since \( f \) is analytic on \( (a - \rho, a + \rho) \) and \( f(x) = 0 \) on \( (a - \varepsilon, a + \varepsilon) \subset (a - \rho, a + \rho) \), it follows from the identity theorem for analytic functions that \( f(x) = 0 \) on the whole interval \( (a - \rho, a + \rho) \).
\end{proof}


\begin{proof}

\[
(0,1) \quad \textbf{but} \quad \langle (0,1) \rangle

\]
    
\end{proof}




\end{document}




\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\intab}[1]{\int_{a}^{b} #1 \, dx}
\newcommand{\defint}[2]{\int_{#1}^{#2}}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newenvironment{exercise}[3][]
{\begin{tcolorbox}[title={#2\ #1},colback=white]}
{\end{tcolorbox}}
\newcommand{\newexercise}[1]{\noindent {\textbf{#1}}\hrulefill}



\usepackage{amsmath, amsthm, amsfonts, amssymb}

\begin{document}

\noindent Tomas Sbardelotto Dos Santos\\
\begin{center}
    \textbf{Homework 3}
\end{center}
\noindent SECTION 5.3: 2, 4, 5, 10\\
\noindent {\textbf{5.3.2}}\hrulefill

     
    \begin{proof}
        To compute the integral, we must use the Fundamental Theorem of Calculus and chain rule. Notice, $\sin(x^2)$ is the composition of continuous functions which is also continuous. In addition, our integrand involves $x^2$ and $0$ which are continuously differentiable everywhere.\\
        Define
        \begin{align*}
            G(x^2) &= \defint{0}{x^2} \sin(s^2) ds \\
            \dv{}{x} G(x^2) = 2xG'&(x^2) = 2x\sin(x^4) \quad \text{(Chain Rule and FTC Pt. 1)}  
        \end{align*}    
        This completes the proof.
    \end{proof}


\noindent{\textbf{5.3.4}}\hrulefill

     
    \begin{proof}
        We apply Theorem 5.3.5:
        \begin{gather*}
            \dv{}{x} [F(x)] = \dv{}{x} [\defint{0}{x} f(s) ds ]= f(x) 
        \end{gather*}
        According to the additive property of integration:
        \begin{gather*}
            \defint{c}{x} f = \defint{a}{x} f - \defint{a}{c} f
        \end{gather*}
        Since $\defint{a}{c}$ is constant with respect to $x$ we have
        \begin{gather*}
            \dv[]{}{x}F(x) = \dv[]{}{x} \brackets{\defint{a}{x} f(s) ds - \defint{a}{c} f(s) ds} = f(x)
        \end{gather*}
        This completes the proof.
    \end{proof}


    \noindent{\textbf{5.3.5}}\hrulefill

     
    \begin{proof}
        To prove integration by parts, we use the product rule and the Fundamental Theorem of Calculus 1.\\
        We are given $F(x)$ and $G(x)$ are differentiable on $[a,b]$. Thus we can use the product rule to find the derivative of $F(x)G(x)$
        \begin{gather*}
            \dv[]{}{x}\brackets{F(x)G(x)} = F(x)G'(x) + F'(x)G(x)
        \end{gather*}
        By rearranging the above formula we get
        \begin{gather*}
            F(x)G'(x) = \dv[]{}{x}\brackets{F(x)G(x)} - F'(x)G(x)
        \end{gather*}
        Now we integrate both sides and apply the Fundamental Theorem of Calculus
        \begin{gather*}
            \intab{F(x)G'(x)} = \intab{\dv[]{}{x}\brackets{F(x)G(x)}} - \intab{F'(x)G(x)}
        \end{gather*}
        \begin{gather*}
            \intab{F(x)G'(x)} = F(b)G(b) - F(a)G(a) - \intab{F'(x)G(x)}
        \end{gather*}
        This completes the proof.
    \end{proof}

\noindent{\textbf{5.3.10}}\hrulefill

 
\begin{proof}
    Suppose $f$ is Riemann integrable. Let $f$ be an odd function or $f(x) = -f(-x)$.
    \begin{align*}
        \int_{-a}^a f(x) \; dx &= \int_0^{a} f(x) \; dx + \int_{-a}^{0} f(x) \; dx\\
        &= \int_0^{a} f(x) \; dx - \int_{0}^{-a} f(x) \; dx\\
        &= \int_0^{a} f(x) \; dx - \int_{0}^{a} f(-x) \; d(-x)\\
        &= \int_0^{a} f(x) \; dx - \int_{0}^{a} f(x) \; dx = 0
    \end{align*}
    Now suppose $f$ is an even function then
    \begin{align*}
        \int_{-a}^a f(x) \; dx &= \int_0^{a} f(x) \; dx + \int_{-a}^{0} f(x) \; dx\\
        &= \int_0^{a} f(x) \; dx + \int_{0}^{a} f(x) \; dx = 2\int_{0}^a f(x) \; dx
    \end{align*}
    This completes the proof.
    
    
\end{proof}



\noindent SECTION 5.4: 2, 4, 7, 11

\noindent {\textbf{5.4.2}}\hrulefill

\textbf{Part a)}


\begin{proof}
  Consider the function $f(x) = b^x$. We know from prop 5.4.2 we can define
  \begin{gather*}
      f(x) = b^x = \exp (x\ln(b))
  \end{gather*}
  Since $b$ is given and $b\neq 1$, $\ln b = r \in \mathbb{R}$ which is nonzero. Thus the above expression can be written as 
  \begin{gather*}
      f(x) = e^{rx}
  \end{gather*}
  We can see that the range of the above expression is equal to the range of $e^x$ which is $(0,\infty)$. Since the exponential function is continuous, by the Intermediate value theorem we can find some $y\in (0,\infty)$ such that $y = f(x)$. We know that such a $y$ is unique by the injectivity of the exponential function.\\
  \textbf{Part 2a:}\\
  Since $b^x$ can be written as the composition of bijective functions, which is also bijective, there exists an inverse function. We can denote this inverse function to be 
  \begin{gather*}
      f^{-1} (y) = \log_b(y) = x
  \end{gather*}

  
\end{proof}

\begin{proof}
Let $y = \log_b(x)$.  Then $b^y = x$. Taking the natural logarithm of both sides, we have $\ln(b^y) = \ln(x)$.  Using the power rule for logarithms, $y \ln(b) = \ln(x)$.  Therefore, $y = \dfrac{\ln(x)}{\ln(b)}$, or equivalently, $\log_b(x) = \dfrac{\ln(x)}{\ln(b)}$.
\end{proof}

\textbf{Part c)}

\begin{proof}
From part (b), we have $\log_c(x) = \dfrac{\ln(x)}{\ln(c)}$ and $\log_c(b) = \dfrac{\ln(b)}{\ln(c)}$. Dividing these equations, we get 
\begin{gather*}
    \dfrac{\log_c(x)}{\log_c(b)} = \dfrac{\ln(x)}{\ln(b)} = \log_b(x) 
\end{gather*}
\end{proof}

\textbf{Part d)}

\begin{proof}. \\
{(i)}
    \begin{gather*}
        \log_b(xy) = \frac{\ln(xy)}{\ln(b)} = \frac{\ln(x) + \ln(y)}{\ln(b)} = \frac{\ln(x)}{\ln(b)} + \frac{\ln(y)}{\ln(b)} = \log_b(x) + \log_b(y)
    \end{gather*}
(ii)
    \begin{gather*}
        \log_b(x^y) = \frac{\ln(x^y)}{\ln(b)} = \frac{y\ln(x)}{\ln(b)} = y\log_b(x)
    \end{gather*}
\end{proof}


\noindent {\textbf{5.4.4}}\hrulefill

\begin{proof}
    \subsection*{Part 1:}
Given a geometric series with a common ratio \( r = -t \) and the first term \( a = 1 \), the sum of the first \( n+1 \) terms is:
\[
S_{n+1} = \frac{a(1 - r^{n+1})}{1 - r}
\]
Substituting \( a = 1 \) and \( r = -t \), we get:
\[
1 - t + t^2 - \cdots + (-1)^n t^n = \frac{1 - (-t)^{n+1}}{1 - (-t)}
\]
Simplifying, we obtain:
\[
1 - t + t^2 - \cdots + (-1)^n t^n = \frac{1}{1 + t} - \frac{(-1)^{n+1} t^{n+1}}{1 + t}
\]
as required.

\subsection*{Part 2:}
The Taylor series for \( \ln(1 + x) \) at \( x = 0 \) is given by:
To find the Taylor series for \( \ln(1+x) \) around \( x = 0 \), we need to calculate the derivatives of \( f(x) = \ln(1+x) \) and evaluate them at \( x = 0 \). The Taylor series is given by:

\[
f(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \cdots
\]

For \( f(x) = \ln(1+x) \), the derivatives are:

\[
f'(x) = \frac{1}{1+x}, \quad f''(x) = -\frac{1}{(1+x)^2}, \quad f'''(x) = \frac{2}{(1+x)^3}, \quad \ldots, \quad f^{(n)}(x) = \frac{(-1)^{n+1}(n-1)!}{(1+x)^n}
\]

Evaluating these at \( x = 0 \) gives:

\[
f'(0) = 1, \quad f''(0) = -1, \quad f'''(0) = 2, \quad \ldots, \quad f^{(n)}(0) = (-1)^{n+1}(n-1)!
\]

The Taylor series for \( \ln(1+x) \) at \( x = 0 \) thus becomes:

\[
\ln(1+x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \cdots
\]
\[
= 0 + x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots
\]
\[
= \sum_{n=1}^{\infty} \frac{(-1)^{n+1} x^n}{n}
\]

This series converges for \( x \in (-1, 1\;] \).

\subsection*{Part 3:}
The alternating harmonic series is given by:
\[
\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n} = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \cdots
\]
This series converges to \( \ln(2) \), which can be seen by setting \( x = 1 \) in the series for \( \ln(1 + x) \):
\[
\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n} = \ln(2)
\]
Thus, the limit of the alternating harmonic series is \( \ln(2) \).

\end{proof}


\noindent {\textbf{5.4.7}}\hrulefill


\begin{proof}
    We can apply L'Hopital's rule:
    \begin{gather*}
        \lim_{x\to\infty} \frac{f(x)}{g(x)} = \lim_{x\to\infty} \frac{f'(x)}{g'(x)}
    \end{gather*}
    Plugging in $f(x) = \ln(x)$ and $g(x) = x$
    \begin{gather*}
        \lim_{x\to\infty} \frac{\ln(x)}{x} = \lim_{x\to\infty} \frac{\frac{1}{x}}{1} = \lim_{x\to\infty} \frac{1}{x} = 0
    \end{gather*}
\end{proof}


\noindent\textbf{5.4.11}
\hrulefill\\

\begin{proof}
   
Consider the substitution \( u = \frac{1}{x} \), which implies that \( x = \frac{1}{u} \) and \( u \to \infty \) as \( x \to 0^+ \). The limit can be rewritten as:
\[
\lim_{u \to \infty} \frac{e^{-u}}{\left(\frac{1}{u}\right)^m} = \lim_{u \to \infty} \frac{u^m}{e^u}.
\]

By applying L'Hôpital's rule \( m \) times, since both the numerator and the denominator go to infinity, we get:
\[
\lim_{u \to \infty} \frac{m!}{e^u} = \frac{m!}{\lim_{u \to \infty} e^u}.
\]

Since \( e^u \) grows much faster than any polynomial function of \( u \), we have:
\[
\lim_{u \to \infty} e^u = \infty.
\]

Therefore, the limit is:
\[
\lim_{u \to \infty} \frac{m!}{e^u} = \frac{m!}{\infty} = 0.
\]

This concludes the proof that the given limit is 0 for any natural number \( m \).

\end{proof}



\noindent{\textbf{5.5.3}}
\hrulefill

\begin{proof}
Consider the function \( f(x) = \frac{1}{x(x+1)} \), which is positive, continuous, and decreasing for \( x \geq 1 \). We can apply the integral test to estimate the sum of the series \(\sum_{n=1}^{\infty} \frac{1}{n(n+1)}\).

\[
\int_{N}^{\infty} f(x) \, dx < \sum_{n=N}^{\infty} f(n) < f(N) + \int_{N}^{\infty} f(x) \, dx
\]

Notice \(N=10\) yields \(\frac{1}{110}<0.01\). We compute the integral from $N=10$ to infinity:

\[
\int_{10}^{\infty} \frac{1}{x(x+1)} \, dx = \ln(\frac{11}{10}) = 0.095
\]

Since the first term goes to $0$ as $x \to \infty$. Thus

\[
\int_{N}^{\infty} f(x) \, dx = 0.095 < \sum_{n=N}^{\infty} f(n) < f(N) + \int_{N}^{\infty} f(x) \, dx = \frac{1}{110} + 0.095
\]

Adding \(\sum_{n=1}^{N-1} f(n) = 0.90\) gives us:

\[
0.90 + 0.095 < \sum_{n=1}^{\infty} f(n) < 0.90 + 0.104
\]
\[
0.995 < \sum_{n=1}^{\infty} f(n) < 1.004
\]

\noindent\textbf{Part b:}
Since the series telescopes, computing the sum directly yields:
\begin{gather*}
    \sum_{n=1}^{\infty} f(n) = \sum_{n=1}^{\infty} \frac{1}{n} - \frac{1}{n+1} = 1 - \lim_{n\to\infty} \frac{1}{n+1} = 1
\end{gather*}
Thus using the integral test, our estimate was within 0.01 of the target value.

\end{proof}


\noindent{\textbf{5.5.6}}\hrulefill



\begin{proof}
We are given that for all \( t \geq T \),
\[
\left| f(t) \right| \leq Me^{at}.
\]
Thus, we can apply the comparison test to determine if \( F(s) \) exists for \( s > a \). Consider the integral
\[
\left| \int_0^\infty f(t)e^{-st} dt \right| \leq \int_0^T \left| f(t)e^{-st} \right| dt + \int_T^\infty \left| f(t)e^{-st} \right| dt.
\]
For the first integral from 0 to \( T \), since \( f(t) \) is Riemann integrable on every interval \( [0, b] \), it is bounded and we can say there exists some \( M_1 \) such that
\[
\int_0^T \left| f(t)e^{-st} \right| dt \leq M_1 < \infty.
\]
For the second integral, we have for \( t \geq T \),
\[
\left| f(t) \right| \leq Me^{at},
\]
so
\[
\int_T^\infty \left| f(t)e^{-st} \right| dt \leq \int_T^\infty Me^{at}e^{-st} dt = M \int_T^\infty e^{(a-s)t} dt.
\]
Since \( s > a \), \( a - s < 0 \), and the exponential function \( e^{(a-s)t} \) is decreasing. We can evaluate this improper integral:
\[
M \int_T^\infty e^{(a-s)t} dt = \left. \frac{-M}{s-a} e^{(a-s)t} \right|_T^\infty = \frac{M}{s-a} e^{-(s-a)T},
\]
which converges since \( s > a \).

Combining the results from both intervals, we conclude
\[
\left| \int_0^\infty f(t)e^{-st} dt \right| \leq M_1 + \frac{M}{s-a} e^{-(s-a)T},
\]
which implies that the integral converges for every \( s > a \), hence the Laplace transform of \( f \) exists.
\end{proof}

\begin{exercise}{5.5.7}
Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a Riemann integrable function on every interval $[a, b]$, and such that $\int_{-\infty}^{\infty} | f(x) | \, dx < \infty$. Show that the Fourier sine and cosine transforms exist. That is, for every $\omega \geq 0$, the following integrals converge:
\[
\int_{-\infty}^{\infty} f(t) \sin(\omega t) \, dt, \quad \text{and} \quad \int_{-\infty}^{\infty} f(t) \cos(\omega t) \, dt.
\]
Furthermore, show that $F_s$ and $F_c$ are bounded functions.
\end{exercise}



\begin{proof}
First, we note that since \( f \) is Riemann integrable over every finite interval and \(\int_{-\infty}^{\infty} |f(x)| \, dx < \infty\), \( f \) must also be absolutely integrable over the real line.

To show that the Fourier sine and cosine transforms exist and are bounded, we will use the fact that sine and cosine functions are bounded by \(-1\) and \(1\).

For the Fourier sine transform, we have:
\begin{align*}
\left| F^s(\omega) \right| &= \frac{1}{\pi} \left| \int_{-\infty}^{\infty} f(t) \sin(\omega t) \, dt \right| \\
&\leq \frac{1}{\pi} \int_{-\infty}^{\infty} |f(t)| |\sin(\omega t)| \, dt \\
&\leq \frac{1}{\pi} \int_{-\infty}^{\infty} |f(t)| \, dt \\
&< \infty.
\end{align*}
This proves the convergence of \(F^s(\omega)\). Additionally, since \(\sin(\omega t)\) is bounded, \(F^s(\omega)\) is also a bounded function.

A similar argument applies to the Fourier cosine transform \(F^c(\omega)\):
\begin{align*}
\left| F^c(\omega) \right| &= \frac{1}{\pi} \left| \int_{-\infty}^{\infty} f(t) \cos(\omega t) \, dt \right| \\
&\leq \frac{1}{\pi} \int_{-\infty}^{\infty} |f(t)| |\cos(\omega t)| \, dt \\
&\leq \frac{1}{\pi} \int_{-\infty}^{\infty} |f(t)| \, dt \\
&< \infty.
\end{align*}
Thus, \(F^c(\omega)\) is also bounded.

Therefore, both \(F^s(\omega)\) and \(F^c(\omega)\) are bounded and their respective integrals converge for all \(\omega \geq 0\).
\end{proof}


\begin{exercise}{5.5.12}
Suppose \( f \colon [1, \infty) \to \mathbb{R} \) is such that \( g(x) := x^2 f(x) \) is a bounded function. Then the integral \( \int_1^\infty f \) converges.
\end{exercise}

\begin{proof}
Since \( g(x) \) is bounded, there exists a constant \( M \) such that \( |g(x)| \leq M \) for all \( x \in [1, \infty) \). This implies that
\[
|f(x)| \leq \frac{M}{x^2}.
\]

Consider the integral
\[
\int_1^\infty |f(x)| \, dx.
\]

Since \( f \) is Riemann integrable on every interval \([1, b]\), we can write
\[
\int_1^\infty |f(x)| \, dx = \lim_{b \to \infty} \int_1^b |f(x)| \, dx.
\]

Applying the comparison test with \( \frac{M}{x^2} \), since \( \frac{M}{x^2} \) is a p-series with \( p = 2 > 1 \), it converges. Thus, by the comparison test, the integral of \( |f(x)| \) also converges.

Now, consider the improper integral of \( f \) itself. Since \( |f(x)| \) has a convergent integral, by the absolute convergence test, the integral of \( f(x) \) also converges. Therefore, we conclude that
\[
\int_1^\infty f(x) \, dx
\]
converges.
\end{proof}


\end{document}





\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}

\begin{document}
\noindent Tomas Sbardelotto Dos Santos\\
June 16, 2023
\begin{center}
    Math 145B Spring Portfolio
\end{center}

\begin{mdframed}
1. Prove path-connected implies connected.
\begin{proof}
    Let $(X,\mathcal{T})$ is a topological space. Suppose $X$ is path-connected. Let $p$ be a point in $X$, then by definition of path-connected, for all $x\in X$ there exists a path $\gamma_x : [0,1] \to X$ such that $\gamma_x (0) = x$ and $\gamma_x (1) = p$.\\\\
    Since $[0,1]$ is connected and $\gamma_x$ is continuous, $\gamma_x ([0,1])$ is also connected.\\\\
    Consider the union $\bigcup_{x\in X} \gamma_x ([0,1])$. This is the union of connected spaces which all share a common point which is also connected. Since the union contains paths from all $x$ to $p$, X must also be connected.
\end{proof}
Explanation: This was one of my favorite proofs. It was a fun way of looking at connectedness and path-connectedness. 
\end{mdframed}

\begin{mdframed}
    2. Prove $\mathbb{R}^n$ is locally compact.
    \begin{proof}
        Let $x\in\mathbb{R}^n$, and consider the open ball $B_\epsilon (x)$ for some $\epsilon > 0$. Take the closed ball $B_\delta[x]$ for some $\delta \geq \epsilon$. Thus $B_\epsilon (x) \subseteq B_\delta[x]$. The delta ball is closed and bounded by definition and therefore also compact. Therefore we have constructed an open neighborhood of $x$ that is contained in a compact set. Thus $\mathbb{R}^n$ is locally compact. 
    
    \end{proof}
    Explanation: I enjoyed this proof because I enjoy simple proofs. Also because we spent a really long time trying to figure it out why $\mathbb{R}^n$ is compact in discussion only to learn that there was a typo on the handout.
\end{mdframed}
\begin{mdframed}
    3. Generalized Intermediate Value Theorem
    \begin{proof}
        Let $X$ be a connected space and let $r\in\mathbb{R}$ where $r$ is between $f(a)$ and $f(b)$ for $a,b\in X$.
        Suppose $f(c)\neq r$ for all $c\in X$. Then $f(X)$ is the disjoint union of intervals $[f(X)\cap (-\infty,r)]\cup[f(X)\cap(r,\infty)]$. This contradicts the fact that a continuous image of a connected space must be connected. Thus there must be some $c\in X$ such that $f(c) =r$.
        \end{proof}
    Explanation: This proof was fun because the intermediate value theorem was one of the first theorems we learned in calculus and I had never written a proof for it. 
\end{mdframed}
\begin{mdframed}
    4. Connected components are closed.
    \begin{proof}
        Suppose $A$ is a connected component. Then for all connected sets $B$ in $X$, if $A\subseteq B$, then $A=B$. However, $A\subseteq \overline{A}$ and if $A$ is connected, then so is $\overline{A}$. Therefore by definition of connected component $A=\overline{A}$ meaning $A$ is closed.
    \end{proof}
    Explanation: I enjoyed this proof because it did not require much work. It came quite naturally when looking at the problem. In addition, I wanted to write the closure symbol in latex and this problem featured the closure of a set. 
\end{mdframed}
\begin{mdframed}
    5. Prove that $(X,\mathcal{T})$ is disconnected if and only if there is a subset $A$ with $\emptyset \subsetneq A \subsetneq X$, that is both open and closed.
    \begin{proof}
        Suppose $(X,\mathcal{T})$ is disconnected. then there are nonempty, open sets $A,B\in \mathcal{T}$ such that $A\cap B = \emptyset$ and $A\cup B = X$. By the prior definition, we know $A^c = X-A = B$ and $B^c = X-B = A$. Since the complements of $A$ and $B$ are open, $A$ and $B$ are closed.\\
        Now supposed there exists a set $A$, with $\emptyset \subsetneq A \subsetneq X$, that is both open and closed. Then there is a set $A\in \mathcal{T}$ where $A^c \in \mathcal{T}$. By definition, $A\cup A^c =X$ and $A\cap A^c = \emptyset$. Therefore $A$ and $A^c$ form a separation of $X$ which means $X$ is disconnected.
    \end{proof}
    Explanation: I enjoyed this proof because it flowed quite naturally. Both sides of the implication basically mirrored each other which made it fun to write. 
\end{mdframed}
\end{document}



\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\intab}[1]{\int_{a}^{b} #1 \, dx}
\newcommand{\defint}[2]{\int_{#1}^{#2}}
\newcommand{\brackets}[1]{\left[ #1 \right]}


\usepackage{amsmath, amsthm, amsfonts, amssymb}

\begin{document}
\noindent Tomas Sbardelotto Dos Santos\\
\begin{center}
    Math 151B Homework 2
\end{center}

SECTION 4.4:

\begin{mdframed}
\underline{\textbf{4.4.1}}
\begin{proof}
First, we show that \( f \) is injective. Assume \( f(a) = f(b) \) for some \( a, b \in \mathbb{R} \), \( a \neq b \). Without loss of generality, assume \( a < b \). By the Mean Value Theorem, there exists \( c \) between \( a \) and \( b \) such that
\[ f'(c) = \frac{f(b) - f(a)}{b - a} = 0. \]
However, this contradicts the given condition that \( f'(x) > 0 \) for all \( x \). Hence, \( f \) must be injective.

Since \( f \) is continuously differentiable and injective, it is strictly increasing. Therefore, it is surjective onto its image \( J = f(\mathbb{R}) \), making \( f \) invertible on \( J \).

Now, we prove the continuous differentiability of \( f^{-1} \). Let \( y \in J \) and \( x = f^{-1}(y) \). By the Inverse Function Theorem, since \( f \) is continuously differentiable and \( f'(x) \neq 0 \), \( f^{-1} \) is continuously differentiable at \( y \). Moreover, the derivative of the inverse is given by
\[ (f^{-1})'(y) = \frac{1}{f'(f^{-1}(y))} = \frac{1}{f'(x)}. \]
Given \( f'(x) > 0 \), it follows that \( (f^{-1})'(y) > 0 \) for all \( y \in J \).

Thus, \( f \) is invertible on \( J \), \( f^{-1} \) is continuously differentiable, and \( (f^{-1})'(y) > 0 \) for all \( y \in J \).
\end{proof}

\end{mdframed}








\begin{mdframed}
\underline{\textbf{4.4.4}}

\begin{proof}
    Define \(f(y) = y^n\) which is continuous and differentiable, notice that \(f'(y) = ny^{n-1} > 0\) for all \(y\neq 0\). By the Intermediate Value Theorem, since $f$ is strictly increasing and continuous, for any real number $x$, there exists $y$ such that $f(y) = x$. Also, since $f$ is strictly increasing the root is unique.\\

    Since \(f(y) = y^n\) is continuously differentiable and positive for all \(y \neq 0\), by the inverse function theorem, the inverse function \(g(x)\) is differentiable wherever \(f'(y)\) is nonzero. Thus we find:
    \begin{gather*}
        g'(x) = \frac{1}{f'(g(x))} = \frac{1}{ng(x)^{n-1}} = \frac{1}{nx^{\frac{n-1}{n}}}
    \end{gather*}
    $g'(x)$ is not differentiable at $x=0$ as it involves division by 0.
\end{proof}

    
\end{mdframed}





\begin{mdframed}
    \underline{\textbf{4.4.5}}

    \begin{proof}
For (\( k=1 \)): By the inverse function theorem, if \( f \) has a continuous first derivative (which is non-zero everywhere on its domain), then its inverse \( g \) exists and has a continuous first derivative given by
\[ g'(y) = \frac{1}{f'(g(y))}. \]
This establishes the base case.

Assume the statement is true for \( k-1 \), i.e., if \( f \) has \( k-1 \) continuous derivatives, then its inverse \( g \) also has \( k-1 \) continuous derivatives. We need to show that if \( f \) has \( k \) continuous derivatives, then \( g \) has a \( k \)-th continuous derivative.

We know that \( g \) is \( (k-1) \)-times differentiable with continuous derivatives up to \( (k-1) \)-th order. We must now show that the \( k \)-th derivative of \( g \) is continuous. Consider the formula for the derivative of the inverse function:
\[ g'(y) = \frac{1}{f'(g(y))}. \]
Differentiating both sides \( k-1 \) times with respect to \( y \) and applying the chain and product rules, we get expressions involving up to the \( k \)-th derivative of \( f \) and up to the \( (k-1) \)-th derivative of \( g \). Each of these derivatives is continuous by hypothesis and the inductive assumption.

Hence, the \( k \)-th derivative of \( g \) is a combination of continuous functions and by the properties of continuous functions, is itself continuous.
\end{proof}

\end{mdframed}


SECTION 5.1: 3, 5, 7

\begin{mdframed}
    \underline{\textbf{5.1.3}}
    \begin{proof}
        We are given that 
        \begin{gather*}
            \lim_{k\to \infty} (U(P_k, f) - L(P_k, f)) = 0\\
            \lim_{k\to \infty} U(P_k,f) = \lim_{k\to \infty} L(P_k,f)
        \end{gather*}
        Thus given any $\varepsilon > 0$ we can find $N\in \mathbb{N}$ such that whenever $k\geq N$
        \begin{gather*}
            U(P_k, f) - L(P_k, f) < \varepsilon
        \end{gather*}
        Which satisfies the definition of Riemann Integrability.
       
    \end{proof}
    \begin{proof}
        Since \( f \) is Riemann integrable on \( [a, b] \), there exists a Riemann sum that converges to the integral of \( f \) over \( [a, b] \). We denote this integral as \( I \), i.e., 
\[ I = \int_{a}^{b} f \]

Given that the upper and lower sums for the partitions \( P_k \) converge to the same limit, let's denote this common limit as \( L \), i.e.,
\[ L = \lim_{k \to \infty} U(P_k, f) = \lim_{k \to \infty} L(P_k, f) \]

We need to show that \( I = L \). By the definition of the Riemann integral, for every \( \varepsilon > 0 \), there exists a partition \( P \) of \( [a, b] \) such that 
\[ |S(P, f) - I| < \frac{\varepsilon}{2} \]
where \( S(P, f) \) is any Riemann sum of \( f \) over the partition \( P \).

Since \( \lim_{k \to \infty} U(P_k, f) = L \) and \( \lim_{k \to \infty} L(P_k, f) = L \), there exists an \( N \in \mathbb{N} \) such that for all \( k \geq N \),
\[ |U(P_k, f) - L| < \frac{\varepsilon}{2} \]
and
\[ |L(P_k, f) - L| < \frac{\varepsilon}{2} \]

For such \( k \), we have
\[ L(P_k, f) \leq S(P_k, f) \leq U(P_k, f) \]
This implies 
\[ |S(P_k, f) - L| < \varepsilon \]

Since \( S(P_k, f) \) can be made arbitrarily close to \( I \) and \( L \) simultaneously, it follows that \( I = L \), and hence
\[ \int_{a}^{b} f = \lim_{k \to \infty} U(P_k, f) = \lim_{k \to \infty} L(P_k, f) \]
    \end{proof}

    

    
    
\end{mdframed}


\begin{mdframed}
    \underline{\textbf{5.1.5}}
    \begin{proof}
        Let $\varepsilon > 0 $ be given. Let $P$ be a partition such that
        \begin{gather*}
            P = \{x_0, \cdots, x_n\} \\
            -1 = x_0 < x_1 < \cdots < 0 = x_k < \cdots < x_n = 1 
        \end{gather*}
        The difference between $U(P, f)$ and $L(P,f)$ would be at most the length of the interval $(x_{k-1}, x_k)$. Thus choose a partition of $[-1,1]$ such that $x_k - x_{k-1} < \varepsilon$.\\
        \underline{Computation of the Integral:}\\
        \begin{gather*}
            \int^1_{-1} f = \int^0_{-1} f + \int^1_{0} f = (0-(-1))(0) + (1-0)(1) = 1
        \end{gather*}
    \end{proof}
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{5.1.7}}
    \begin{proof}
        Since $f$ is Riemann integrable, we know that given an $\varepsilon > 0$ we can find a partition $P$ on $[a,b]$ such that
        \begin{gather*}
            U(P,f) - L(P,f) < \varepsilon
        \end{gather*}
        The Riemann sum for $f$ is defined as $S(P, f, c_k) = \sum^n_{k=1}f(c_k)\Delta x_k$ for sample points $c_k \in [a,b]$ and satisfies
        \begin{gather*}
            L(P,f) \leq S(P,f,c_k) \leq U(P,f)
        \end{gather*}
        By definition of integral we also know that
        \begin{gather*}
           L(P,f) \leq \int^b_a f \leq U(P,f)
        \end{gather*}
        Thus by the squeeze theorem we can conclude
        \begin{gather*}
            \abs{\int^b_a f - S(P,f,c_k)} < \varepsilon
        \end{gather*}
    \end{proof}
    
\end{mdframed}







\begin{mdframed}
\underline{\textbf{5.2.2}}

    \begin{proof}
To establish the equality of the integrals, we will show that
\[
\lim_{\| \bar{P} \| \to 0} S(\bar{P}, f+g) = \lim_{\| \bar{P} \| \to 0} S(\bar{P}, f) + \lim_{\| \bar{P} \| \to 0} S(\bar{P}, g),
\]
where \( S(\bar{P}, h) \) denotes a Riemann sum of a function \( h \) with respect to a partition \( \bar{P} \) and \( \| \bar{P} \| \) is the norm of the partition \( \bar{P} \).

For any given partition \( \bar{P} \), the Riemann sum of \( f+g \) can be written as:
\[
S(\bar{P}, f+g) = \sum_{i=1}^{n} (f(x_i^*) + g(x_i^*)) \Delta x_i,
\]
where \( x_i^* \) is a sample point in the \( i \)-th subinterval of \( \bar{P} \), and \( \Delta x_i \) is the width of the \( i \)-th subinterval.

By the linearity of summation, we have:
\[
S(\bar{P}, f+g) = \sum_{i=1}^{n} f(x_i^*) \Delta x_i + \sum_{i=1}^{n} g(x_i^*) \Delta x_i = S(\bar{P}, f) + S(\bar{P}, g).
\]

Taking the limit as the mesh of the partition \( \bar{P} \) approaches zero, we obtain:
\[
\int_{a}^{b} [f(x) + g(x)] \, dx = \lim_{\| \bar{P} \| \to 0} S(\bar{P}, f+g) = \lim_{\| \bar{P} \| \to 0} S(\bar{P}, f) + \lim_{\| \bar{P} \| \to 0} S(\bar{P}, g) = \int_{a}^{b} f(x) \, dx + \int_{a}^{b} g(x) \, dx.
\]
This confirms that the integral of the sum is equal to the sum of the integrals.

\end{proof}
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{5.2.4}}: Mean Value Theorem of Integrals
    \begin{proof}
        We will show that there exists $c \in [a,b]$ such that 
        \begin{gather*}
            f(c)(b-a) = \intab{f(x)}
        \end{gather*}
        Since $[a,b]$ is closed and bounded, by the Extreme Value theorem we can find $m,M$ such that $m \leq f(x) \leq M$ for all $x\in [a,b]$. We can then apply Prop 5.1.10 which states that
        \begin{align*}
            m(b-a) \leq &\intab{f(x)} \leq M(b-a)\\
           \implies m\ \leq \frac{1}{(b-a)}&\intab{f(x)} \leq M
        \end{align*}
        Thus $\frac{1}{(b-a)}\intab{f(x)}$ is between $m$ and $M$ so by the Intermediate Value Theorem since $f(x)$ is continuous there must exist $c \in [a,b]$ such that
        \begin{gather*}
            f(c) = \frac{1}{(b-a)}\intab{f(x)} \;\; \textbf{or} \;\; f(c)(b-a) = \intab{f(x)}
        \end{gather*}
        This completes the proof.
    \end{proof}
    
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{5.2.5}}\\
    Let \( f: [a,b] \to \mathbb{R}\) be a continuous function such that $f(x) \geq 0 $ for all $x \in [a,b]$ and $\intab{f(x)} = 0$. Show that $f(x) = 0$ for all $x$.

    \begin{proof}
        Suppose on the contrary that \( f: [a,b] \to \mathbb{R}\) be a continuous function such that $f(x) \geq 0 $ for all $x \in [a,b]$ and $\intab{f(x)} = 0$ and there exists $x_0$ such that $f(x_0) \neq 0$. Then 
        \begin{gather*}
            \intab{f(x)} = \lim_{\| P \| \to 0} \sum_P f(x^*_i) \Delta x_i
        \end{gather*}
        As the partition of $[a,b]$ becomes infinitely fine, the value $x_0$ will eventually be included in $P$ thus making
        \begin{gather*}
            \intab{f(x)} = \lim_{\| P \| \to 0} \sum_P f(x^*_i) \Delta x_i > 0
        \end{gather*}
        which contradicts our assumption that $\intab{f(x)} = 0$. Thus $f(x) = 0$ for all $x$.
    \end{proof}
    
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{5.2.8}}


\begin{proof}
We will show that for any $\alpha, \beta, \gamma \in [a,b]$ the equality holds
\begin{gather*}
    \int_\alpha^\gamma f(x) \; dx = \int_\alpha^\beta  f(x) \; dx + \int_\beta^\gamma f(x) \; dx
\end{gather*}
We know the equality holds when $\alpha < \beta < \gamma$ or $\gamma < \beta < \alpha$ by Proposition 5.2.2.\\
Without loss of generality suppose $\alpha < \gamma$. There are two other cases: $\beta < \alpha$ and $\beta > \gamma$.\\
\emph{Case 1}: $\beta < \alpha$\\
In this case, $\beta < \alpha < \gamma$ so we can use the fact that $\int_\alpha^\beta f = - \int_\beta^\alpha f$ so our equality returns to what we know is true
\begin{gather*}
    \int_\alpha^\gamma f(x) \; dx = \int_\alpha^\beta  f(x) \; dx + \int_\beta^\gamma f(x) \; dx\\
    \int_\alpha^\gamma f(x) \; dx = -\int_\beta^\alpha  f(x) \; dx + \int_\beta^\gamma f(x) \; dx\\
    \int_\beta^\alpha f(x) \; dx + \int_\alpha^\gamma f(x) \; dx = \int_\beta^\gamma f(x) \; dx
\end{gather*}
\emph{Case 2:} $\beta > \gamma$\\
In this case, $\alpha < \gamma < \beta$ so we can use the fact that $\int_\beta^\gamma f = - \int_\gamma^\beta f$ so our equality returns to what we know is true
\begin{gather*}
    \int_\alpha^\gamma f(x) \; dx = \int_\alpha^\beta  f(x) \; dx + \int_\beta^\gamma f(x) \; dx\\
    \int_\alpha^\gamma f(x) \; dx = -\int_\gamma^\beta  f(x) \; dx + \int_\alpha^\beta f(x) \; dx\\
    \int_\alpha^\gamma f(x) \; dx + \int_\gamma^\beta f(x) \; dx = \int_\alpha^\beta f(x) \; dx
\end{gather*}
There is a similar case for $\gamma < \alpha$.
\end{proof}
\end{mdframed}

SECTION 5.3: 2,4,5,10

\begin{mdframed}
    \underline{\textbf{5.3.2}}
    \begin{proof}
        To compute this, we must use the Fundamental Theorem of Calculus and chain rule. Notice, $\sin(x^2)$ is the composition of continuous functions which is also continuous. In addition, our integrand involves $x^2$ and $0$ which are continuously differentiable everywhere.\\
        Define
        \begin{align*}
            G(x^2) &= \defint{0}{x^2} \sin(s^2) ds \\
            \dv{}{x} G(x^2) = 2xG'&(x^2) = 2x\sin(x^4) \quad \text{(Chain Rule and FTC Pt. 1)}  
        \end{align*}    
        This completes the proof.
    \end{proof}
\end{mdframed}





\begin{mdframed}
    \underline{\textbf{5.3.4}}
    \begin{proof}
        We apply Theorem 5.3.5:
        \begin{gather*}
            \dv{}{x} [F(x)] = \dv{}{x} [\defint{0}{x} f(x) dx ]= f(x) 
        \end{gather*}
        According to the additive property of integration:
        \begin{gather*}
            \defint{c}{x} f = \defint{a}{x} f - \defint{a}{c} f
        \end{gather*}
        Since $\defint{a}{c}$ is constant with respect to $x$ we have
        \begin{gather*}
            \dv[]{}{x}F(x) = \dv[]{}{x} \brackets{\defint{a}{x} f(s) ds - \defint{a}{c} f(s) ds} = f(x)
        \end{gather*}
        This completes the proof.
    \end{proof}


    
\end{mdframed}

\end{document}



\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}


\usepackage{amsmath, amsthm, amsfonts, amssymb}


\begin{document}
\noindent Tomas Sbardelotto Dos Santos
\begin{center}
    \textbf{Math 151B Homework 1}
\end{center}



SECTION 4.1: 2, 3, 5, 8, 15


\begin{mdframed}
\textbf{\underline{4.1.2}}

\begin{proof}
Let \( u(x) \) and \( v(x) \) be differentiable functions. We want to prove the derivative of the quotient \( \frac{u(x)}{v(x)} \):

\begin{align*}
\left( \frac{u}{v} \right)' &= \left( u \cdot \frac{1}{v} \right)' \\
&= u' \cdot \frac{1}{v} + u \cdot \left( \frac{1}{v} \right)' \quad \text{(Product rule)} \\
&= u' \cdot \frac{1}{v} + u \cdot \left( -\frac{1}{v^2} \cdot v' \right) \quad \text{(Chain rule for \( \frac{1}{v} \))} \\
&= \frac{u'}{v} - \frac{uv'}{v^2} \\
&= \frac{u'v - uv'}{v^2}.
\end{align*}

Therefore, the quotient rule \( \left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2} \) is proved.
\end{proof}
\end{mdframed}

\begin{mdframed}
\textbf{\underline{4.1.3}}

\begin{proof}
    Let \(f(x) = x^n\) with \(n > 0\) if \(x = 0\). Then we see:
    \begin{align*}
        f'(c) = \lim_{x\to c} \frac{x^n - c^n}{x - c} &= \lim_{x\to c} \frac{(x-c)(x^{n-1} + cx^{n-2} + c^2x^{n-3} + \cdots + c^{n-1})}{x - c}\\ &= \lim_{x\to c} (x^{n-1} + cx^{n-2} + c^2x^{n-3} + \cdots + c^{n-1}) = nc^{n-1}
    \end{align*}
    Thus $f'(c) = nc^{n-1}$.
\end{proof}


    
\end{mdframed}

\begin{mdframed}
    \textbf{\underline{4.1.5}}
    \begin{proof}
    To prove that \( f \) is differentiable at 0, we need to show that the limit
\[ \lim_{h \to 0} \frac{f(h) - f(0)}{h} \]
exists. Since \( f(0) = 0 \), this simplifies to
\[ \lim_{h \to 0} \frac{f(h)}{h}. \]
For \( h \in \mathbb{Q} \), \( f(h) = h^2 \), and for \( h \notin \mathbb{Q} \), \( f(h) = 0 \). In either case, \( \frac{f(h)}{h} = h \) if \( h \) is rational, and 0 if \( h \) is irrational. As \( h \) approaches 0, both \( h \) and 0 approach 0, thus
\[ \lim_{h \to 0} \frac{f(h)}{h} = 0. \]
Therefore, \( f \) is differentiable at 0, with \( f'(0) = 0 \).

\[ \lim_{h \to x} f(h) \]
does not exist or is not equal to \( f(x) \). Since the rationals and irrationals are dense in \( \mathbb{R} \), for any \( x \neq 0 \), there are sequences of rational numbers \( \{q_n\} \) and irrational numbers \( \{r_n\} \) such that \( q_n \to x \) and \( r_n \to x \) as \( n \to \infty \). Then \( f(q_n) = q_n^2 \to x^2 \) and \( f(r_n) = 0 \to 0 \). Since \( x^2 \neq 0 \), the limits of \( f(q_n) \) and \( f(r_n) \) as \( n \to \infty \) are different, hence the limit of \( f(h) \) as \( h \to x \) does not exist. Thus, \( f \) is discontinuous at every point \( x \neq 0 \).

    \end{proof}
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{4.1.8}}
    \begin{proof}
    To prove \((f^n)'(x) = n(f(x))^{n-1}f'(x)\) we use the Chain rule. Let \( g(x) = x^n\), notice:
    \begin{gather*}
        (f(x))^n = g(f(x))
    \end{gather*}
    Thus, taking the derivative we get:
    \begin{gather*}
        (f^n)'(x) = (g(f(x)))' = g'(f(x))f'(x) = n(f(x))^{n-1}f'(x)
    \end{gather*}
    
    \end{proof} 
\end{mdframed}

\begin{mdframed}
\textbf{\underline{4.1.15}}
    \begin{proof}
        Given that \(g(c) = f(c) = 0\):
        \begin{gather*}
            \lim_{x\to c} \frac{f(x)}{g(x)} = \lim_{x\to c} \frac{f(x) - f(c)}{g(x) - g(c)} = \lim_{x\to c} \frac{\frac{f(x) - f(c)}{x-c}}{\frac{g(x) - g(c)}{x - c}} = \frac{f'(c)}{g'(c)} = \lim_{x\to c}\frac{f'(x)}{g'(x)}
        \end{gather*}
    \end{proof}
\end{mdframed}

SECTION 4.2: 1, 3, 6, 8, 9

\begin{mdframed}
    \textbf{\underline{4.2.1}}
    \begin{proof}
    We will prove the second item. Suppose $f$ is decreasing. For all $x, c \in I$ with $x \neq c$, we have
    \[
      \frac{f(x) - f(c)}{x - c} \leq 0.
    \]
    Taking a limit as $x$ approaches $c$, we see that $f'(c) \leq 0$.
    
    For the other direction, suppose $f'(x) \leq 0$ for all $x \in I$. Take any $x, y \in I$ where $x < y$, and note that $[x, y] \subseteq I$. By the mean value theorem, there is some $c \in (x, y)$ such that
    \[
      f(y) - f(x) = f'(c)(y - x).
    \]
    As $f'(c) \leq 0$ and $y - x > 0$, then $f(y) - f(x) \leq 0$ or $f(x) \geq f(y)$, and so $f$ is decreasing.
    \end{proof}
    
\end{mdframed}

\begin{mdframed}
    \textbf{\underline{4.2.3}}
    \begin{proof}
        We need to prove there exists a constant \( L \) such that for all \( x, y \in \mathbb{R} \),
\[ |f(x) - f(y)| \leq L |x - y|. \]

Let \( x, y \in \mathbb{R} \) and without loss of generality, assume that \( x < y \). By the Mean Value Theorem, there exists some \( c \) in the open interval \( (x, y) \) such that
\[ f'(c) = \frac{f(y) - f(x)}{y - x}. \]

Taking absolute values and using the boundedness of \( f' \), we have
\[ \left| \frac{f(y) - f(x)}{y - x} \right| = |f'(c)| \leq M. \]

Multiplying both sides by \( |y - x| \), we obtain
\[ |f(y) - f(x)| \leq M |y - x|. \]

Setting \( L = M \), we find that \( f \) satisfies the Lipschitz condition with Lipschitz constant \( L \). Thus, \( f \) is Lipschitz continuous.


    \end{proof}
\end{mdframed}


\begin{mdframed}
    \underline{\textbf{4.2.6}}
    \begin{proof}
        Take any two points \( x_1, x_2 \in I \) with \( x_1 < x_2 \). According to the Mean Value Theorem, there exists a point \( c \) in the open interval \( (x_1, x_2) \) such that
\[ f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}. \]

Since \( f'(x) > 0 \) for all \( x \in I \), and \( c \in (x_1, x_2) \subset I \), it follows that \( f'(c) > 0 \). Therefore,
\[ \frac{f(x_2) - f(x_1)}{x_2 - x_1} > 0. \]

This implies that \( f(x_2) - f(x_1) > 0 \), and hence \( f(x_2) > f(x_1) \).

Since \( x_1 \) and \( x_2 \) were arbitrary points with \( x_1 < x_2 \) in \( I \), this shows that \( f \) is strictly increasing on \( I \).

    \end{proof}
\end{mdframed}

\begin{mdframed}
\textbf{\underline{4.3.8}}
    \begin{proof}
        Given that \( f \) and \( g \) are differentiable functions on \( (a, b) \) with \( f'(x) = g'(x) \) for all \( x \in (a, b) \), define a new function \( h(x) = f(x) - g(x) \). The derivative of \( h \) with respect to \( x \) is given by
\[ h'(x) = f'(x) - g'(x). \]
Since \( f'(x) = g'(x) \), it follows that \( h'(x) = 0 \) for all \( x \in (a, b) \).

By the Mean Value Theorem, for any two points \( x_1, x_2 \in (a, b) \), there exists a \( c \in (x_1, x_2) \) such that
\[ h'(c) = \frac{h(x_2) - h(x_1)}{x_2 - x_1}. \]
Because \( h'(x) = 0 \), this simplifies to
\[ 0 = \frac{h(x_2) - h(x_1)}{x_2 - x_1}, \]
which implies that \( h(x_2) = h(x_1) \).

Since \( x_1 \) and \( x_2 \) were arbitrary, \( h(x) \) must be constant on \( (a, b) \). Thus, there exists a constant \( C \) such that for all \( x \in (a, b) \),
\[ f(x) - g(x) = C \]
or equivalently,
\[ f(x) = g(x) + C. \]
    \end{proof}
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{4.2.9}}
    \begin{proof}
First, we prove that \( g(x) \neq 0 \) for \( x \neq c \) in \( (a, b) \). Assume for contradiction that there exists \( x_0 \neq c \) such that \( g(x_0) = 0 \). Since \( g \) is differentiable at \( c \) and \( g(c) = 0 \), by the Mean Value Theorem, there exists \( c_1 \in (c, x_0) \) or \( c_1 \in (x_0, c) \) such that
\[ g'(c_1) = \frac{g(x_0) - g(c)}{x_0 - c} = \frac{0 - 0}{x_0 - c} = 0, \]
contradicting the assumption that \( g'(x) \neq 0 \) for \( x \neq c \). Therefore, \( g(x) \neq 0 \) for \( x \neq c \).

Now, let \( L \) be the limit of \( \frac{f'(x)}{g'(x)} \) as \( x \) approaches \( c \). To show that
\[ \lim_{x \to c} \frac{f(x)}{g(x)} = L, \]
we use the definition of the derivative at \( c \) for both \( f \) and \( g \):
\[ f'(c) = \lim_{x \to c} \frac{f(x) - f(c)}{x - c}, \quad g'(c) = \lim_{x \to c} \frac{g(x) - g(c)}{x - c}. \]
Since \( f(c) = g(c) = 0 \), these become
\[ f'(c) = \lim_{x \to c} \frac{f(x)}{x - c}, \quad g'(c) = \lim_{x \to c} \frac{g(x)}{x - c}. \]
Given that \( g'(x) \neq 0 \) for \( x \neq c \) and \( g(x) \neq 0 \) for \( x \neq c \), we can write
\[ L = \lim_{x \to c} \frac{f'(x)}{g'(x)} = \lim_{x \to c} \frac{\frac{f(x)}{x - c}}{\frac{g(x)}{x - c}} = \lim_{x \to c} \frac{f(x)}{g(x)}. \]
Thus, we have shown that
\[ \lim_{x \to c} \frac{f(x)}{g(x)} = \lim_{x \to c} \frac{f'(x)}{g'(x)} = L. \]
\end{proof}
    
\end{mdframed}
SECTION 4.3: 1-3, 5, 7
\begin{mdframed}
    \underline{\textbf{4.3.1}}\\
    \emph{Solution:}
        \begin{gather*}
            P^0_n(x) = \sum^n_{k=0} \frac{x^k}{k!}
        \end{gather*}

\end{mdframed}

\begin{mdframed}
    \underline{\textbf{4.3.2}}
    \begin{proof}
We can write \( p \) as
\[ p(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)^2 + \dots + a_d(x - x_0)^d, \]
where \( a_n \) are the coefficients of the polynomial.

The \( n \)th derivative of \( p \) at \( x_0 \) is given by \( p^{(n)}(x_0) = n!a_n \) for \( n \leq d \), since the \( n \)th derivative of \( (x - x_0)^n \) with respect to \( x \) is \( n! \) and the derivatives of terms of higher order are 0.

Substituting \( p^{(n)}(x_0) = n!a_n \) into the Taylor polynomial, we get
\[ P_d(x) = \sum_{n=0}^{d} a_n(x - x_0)^n. \]

Since this is exactly the expression for \( p(x) \), \( P_d(x) \), is equal to \( p(x) \).
\end{proof}
\end{mdframed}

\begin{mdframed}
    \underline{\textbf{4.3.3}}
    \begin{proof}
        Let \( f(x) = |x|^3 \). To find the Taylor series of \( f \) around \( x = 0 \), we must compute the derivatives of \( f \) at \( x = 0 \).

For \( x > 0 \), \( f(x) = x^3 \), and the derivatives are:
\[ f'(x) = 3x^2, \quad f''(x) = 6x, \quad f^{(3)}(x) = 6. \]

For \( x < 0 \), \( f(x) = (-x)^3 = -x^3 \), and the derivatives are:
\[ f'(x) = -3x^2, \quad f''(x) = -6x, \quad f^{(3)}(x) = -6. \]

At \( x = 0 \), we can compute \( f'(0) \) and \( f''(0) \) as the limits of \( f'(x) \) and \( f''(x) \) as \( x \) approaches 0 from the right and left:
\[ f'(0) = \lim_{x \to 0} f'(x) = 0, \quad f''(0) = \lim_{x \to 0} f''(x) = 0. \]

However, when we try to compute the third derivative at 0, we see that the limit from the right is different from the limit from the left:
\[ \lim_{x \to 0^+} f^{(3)}(x) = 6, \quad \lim_{x \to 0^-} f^{(3)}(x) = -6. \]

Since these two one-sided limits are not equal, \( f^{(3)}(0) \) does not exist.
    \end{proof}

\end{mdframed}

\begin{mdframed}
    \textbf{\underline{4.3.5}}\\
    \begin{proof}
    First we find the remainder term of the Taylor Polynomial which gives us:
        \begin{gather*}
            R^{x_0}_n(x) = \frac{f^{n+1}(x_0)(x-x_0)^{n+1}}{(n+1)!}
        \end{gather*}
    Next, we plug into our equation to find:
        \begin{gather*}
            \lim_{x\to x_0}\frac{R^{x_0}_n(x)}{(x-x_0)^n} = \lim_{x\to x_0} \frac{\frac{f^{n+1}(x_0)(x-x_0)^{n+1}}{(n+1)!}}{(x-x_0)^n} = \frac{f^{n+1}(x_0)}{(n+1)!}\lim_{x\to x_0} (x-x_0) = \frac{f^{n+1}(x_0)}{(n+1)!} \cdot 0 = 0
        \end{gather*}
    Thus we find the limit is 0.
    \end{proof}
\end{mdframed}

\begin{mdframed}
    \textbf{\underline{4.3.7}}
        \begin{proof}
Consider the Taylor expansion of \( f \) around \( 0 \). Since \( f \) and its derivatives up to the second are continuous everywhere and \( f''(x) = a \) is constant, the Taylor series of \( f \) around \( 0 \) is given by:

\[ T(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + R(x), \]

where \( R(x) \) is the remainder term in the Taylor series. Because \( f''(x) = a \) is constant, all higher-order derivatives of \( f \) are zero, and therefore, the remainder term \( R(x) \) is zero for any \( x \).

Given that \( f(0) = c \), \( f'(0) = b \), and \( f''(0) = a \), the Taylor polynomial simplifies to:

\[ T(x) = c + bx + \frac{a}{2}x^2. \]

Since the remainder term \( R(x) \) is zero, \( T(x) \) is exactly \( f(x) \), and we have:

\[ f(x) = c + bx + \frac{a}{2}x^2. \]

To prove the uniqueness, suppose there exists another function \( g \) with all the same properties as \( f \). By the same argument, \( g \) would also have a Taylor series around \( 0 \) that matches \( f \):

\[ g(x) = c + bx + \frac{a}{2}x^2. \]

Since the Taylor series uniquely determines the function within its radius of convergence and the series for \( f \) and \( g \) are identical, \( f \) and \( g \) must be the same function.

Thus, \( f(x) = c + bx + \frac{a}{2}x^2 \) is the unique differentiable function satisfying \( f''(x) = a \), \( f'(0) = b \), and \( f(0) = c \).
\end{proof}

\end{mdframed}


\begin{mdframed}
    Let \( f: [a, b] \rightarrow \mathbb{R} \) have \( n + 1 \) continuous derivatives and let \( x_0 \in [a, b] \). We consider the \( n \)th Taylor polynomial for \( f \) at \( x_0 \) and the remainder \( R_n(x) = f(x) - P_n(x) \), where \( P_n(x) \) is the \( n \)th Taylor polynomial of \( f \) at \( x_0 \). The normalized remainder is given by
\[ R_n^*(x) = \frac{R_n(x)}{(x - x_0)^n}. \]

We want to prove that
\[ \lim_{x \to x_0} R_n^*(x) = 0. \]

By Taylor's theorem, we have
\[ R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0)^{n+1} \]
for some \( c \) between \( x_0 \) and \( x \). Thus, we can write the normalized remainder as
\[ R_n^*(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0). \]

Taking the limit as \( x \) approaches \( x_0 \), we get
\[ \lim_{x \to x_0} R_n^*(x) = \lim_{x \to x_0} \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0). \]

Since \( f^{(n+1)} \) is continuous, \( f^{(n+1)}(c) \) approaches \( f^{(n+1)}(x_0) \) as \( x \) approaches \( x_0 \). Moreover, \( (x - x_0) \) approaches \( 0 \). Therefore, the product of these two terms also approaches \( 0 \), and we conclude that
\[ \lim_{x \to x_0} R_n^*(x) = 0. \]
\end{mdframed}



\end{document}



\section{Fick's Laws}

Fick's Laws tell us about diffusion and give us a way to model it. In the one dimensional case we have something like this:\\

The first law tells us that particles like to move from areas of higher concentration to areas of lower concentration. In this case, $D$ is some positive function, the function is negative because particles tend to move away, and $u$ stands for the concentration at a given area
\[
\phi(t, x) = -D\pdv{u}{x} u(t, x)
\]
For higher dimensions, we can basically just think of $x$ as a position in higher dimensional space.

The computation is as follows:

Fick's First Law
\[
\phi(x, t) = -D\nabla u(x,t)
\]
\[
\dv[]{}{t} \int_V u(x,t) \; dV = \int_S u(x,t) \; dS = \int_V \nabla u(t,x) \; dV = \int_V \nabla (-D\nabla u(x,t)) \; dV
\]
\[
\int_V \pdv{u}{t} u(t,x) \; dV = \int_V \nabla (-D\nabla u(x,t)) \; dV
\]
\[
\pdv{u}{t} u(t,x) = -D\nabla^2 u(x,t)
\]

\end{document}


% arguelles v2.3.0
% author: Michele Piazzai
% contact: michele.piazzai@uc3m.es
% license: MIT

% Copied from GitHub: https://github.com/piazzai/arguelles

\documentclass[compress,10pt]{beamer}

\usetheme{Arguelles}

\title{Modeling Ideological Conflicts}
\subtitle{Opinion Dynamics}
\event{}
\date{}
\author{Tomas Sbardelotto Dos Santos}
\institute{Math 168 $\cdot$ University of California, Riverside}
\usepackage{xcolor}
\definecolor{darkred}{RGB}{215, 0, 0} % This defines a dark red color.



\begin{document}

\frame[plain]{\titlepage}

\Section{Introduction}



\begin{frame}{Introduction}
    \indent The mathematical concept at the heart of this paper is the basic model on opinion dynamics we went over in lecture 3.\cite{lecture}
    \vspace{-0.3cm}
    \[
    \dot p = -rp(1-p)(1-2p) 
    \]
    
    \indent The idea behind opinion dynamics models are to model the change in perception of the individuals of a population based on social interactions. We then track the proportion of the population that ends up on either side.
    
    \indent The authors of this paper make slight changes to our original equations and add another state which tracks the number of people with a moderate viewpoint labeled $AB$. They attempt to maximize the number of moderates in the population.
    
\end{frame}

\begin{frame}
The paper's authors include 4 states $A$, $B$, $AB$, and $A_c$. With the following interactions:

\begin{minipage}{0.5\textwidth}
    \begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Speaker} & \multicolumn{2}{c}{\textbf{Listener}} \\
\cline{2-3}
& pre-interaction & post-interaction \\
\hline
\( A, A_c \) & \( B \) & \( AB \) \\
& \( AB \) & \( A \) \\
\hline
\( B \) & \( A \) & \( AB \) \\
& \( AB \) & \( B \) \\
\hline
\end{tabular}
\end{table}
\end{minipage}\hfill
\begin{minipage}{0.25\textwidth}
    \begin{table}[]
    \textbf{Key:}
        \centering
        \begin{tabular}{ c | c }
            A & Opinion 1\\
            B & Opinion 2\\
            AB & neutral\\
            A_c & A zealot 
        \end{tabular}
    \end{table}
\end{minipage}\\
\vspace{0.2cm}
    We can model the change in the proportions of $A$s, $B$s, and $AB$s as
\begin{align*}
    \dot n_A &= (p + n_A)n_{AB} - n_Bn_{AB} \\
    \dot n_B &= n_Bn_{AB} - (p + n_A)n_{AB} 
\end{align*}
    $n_A, n_B, n_{AB}$ denote the fractions of the population that subscribe to certain viewpoint. \cite{moderation}
\end{frame}

\begin{frame}
    
\end{frame}

\begin{frame}
With the given interactions shown above, the authors were attempting different means of maximizing the percentage of $AB$ (neutral) participants. 

The first being altering the stubbornness of the moderates with a variable $s$. The equations now became:
    \begin{align*}
        \dot n_A = \textcolor{darkred}{(1-s)}(p + n_A)n_{AB} - n_Bn_{AB}\\
        \dot n_B = \textcolor{darkred}{(1-s)}n_Bn_{AB} - (p + n_A)n_{AB}
    \end{align*}
    Where $s=0$ gives us the original equations and an increase in $s$ would decrease the likelihood for a moderate to change. Next they added evangelism to the $AB$ group, which allows moderates to actively deradicalize $A$ and $B$:
    \begin{align*}
    \dot n_A = (p + n_A)n_{AB} - n_Bn_{AB} - \textcolor{darkred}{rn_An_{AB}}\\
    \dot n_B = n_Bn_{AB} - (p + n_A)n_{AB} - \textcolor{darkred}{rn_Bn_{AB}}    
    \end{align*}
    Finally, an element of nonsocial deradicalization was added. They accomplish this by adding the variable $u$
    \begin{align*}
        \dot n_A = (p + n_A)n_{AB} - n_Bn_{AB} - \textcolor{darkred}{un_A}\\
        \dot n_B = n_Bn_{AB} - (p + n_A)n_{AB} - \textcolor{darkred}{un_B}
    \end{align*}
    where $u$ is the rate which the radicals abandon their radical position in response to the nonsocial stimulus


\end{frame}

\section{Results}
\begin{frame}{Results}

    \begin{minipage}{0.75\textwidth}
        \textbf{\underline{Stubbornness}}\\
    Works at very low $s$ but fails as $s$ increases. Stubborn $AB$ population leads to decreased $A$ and $B$ which makes it possible for $A_c$ to take over, driving $AB$ to extinction.\\
    \textbf{\underline{Evangelism}}\\
     If the moderates’ campaign of persuasion is successful from the start, then they do in fact maintain a robust equilibrium population. However, if they fail to sustain this level, they can instigate their own extinction similar to the stubborn population.\\
    \textbf{\underline{Nonsocial Deradicalization}}\\
    Generally increases $AB$ toward limit of $(1-p)$ which shows it is the only reliable method of increasing $AB$ population
    \end{minipage}\hfill
    \begin{minipage}{0.2\textwidth}
        \includegraphics[width=\linewidth]{graph1.png}
        \includegraphics[width=\linewidth]{graph2.png}
        \includegraphics[width=\linewidth]{graph3.png}
    \end{minipage}

\end{frame}


\begin{frame}{References}
\bibliographystyle{plain}
    \bibliography{refs}
\end{frame}


\end{document}


\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{ulem}
\usepackage{cancel}
\usepackage{circledsteps}
\usepackage{physics}
\usepackage{lipsum}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tcolorbox}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\intab}[1]{\int_{a}^{b} #1 \, dx}
\newcommand{\defint}[2]{\int_{#1}^{#2}}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newenvironment{exercise}[3][]
{\begin{tcolorbox}[title={#2\ #1},colback=white]}
{\end{tcolorbox}}
\newcommand{\newexercise}[1]{\noindent {\textbf{#1}}\hrulefill}



\usepackage{amsmath, amsthm, amsfonts, amssymb}


\begin{document}

\noindent Tomas Sbardelotto Dos Santos
\begin{center}
    Midterm 1 Math 168
\end{center}


\newexercise{1. }\\
(Mixing Problem) A 142-liter-tank initially contains V = 100 liters of water and
a certain amount of salt mixed in it. A fresh water (no salt) enters the tank at
the rate of rin= 3 liter/min. The well-mixed solution leaves the tank at the rate
of rout= 2 liter/min. Determine what comes first, the tank gets overflowed with
water or the amount of salt decays to one-half its original value.

\begin{proof
\subsection*{Analyzing Water Volume}

The net rate of volume change in the tank is $\Delta V = r_{\text{in}} - r_{\text{out}} = 1$ L/min, indicating the tank's volume increases by 1 liter every minute. The time to overflow, given the tank's initial volume of 100 liters and capacity of 142 liters, is calculated as:
\[ t_{\text{overflow}} = \frac{42}{1} = 42 \text{ minutes.} \]

\subsection*{Analyzing Salt Concentration}

The differential equation describing the rate of change of the salt amount $S(t)$ in the tank is:
\[ \frac{dS}{dt} = -2 \cdot \frac{S(t)}{100 + t}. \]

\subsection*{Solving the Differential Equation}

Separating variables and integrating both sides gives:
\[ \int \frac{1}{S} dS = -2 \int \frac{1}{100 + t} dt. \]

Solving these integrals, we obtain:
\[ \ln|S| = -2 \ln|100 + t| + C, \]
where $C$ is the integration constant determined using the initial condition $S(0) = S_0$:
\[ C = \ln|10000S_0|. \]

Therefore, the amount of salt at time $t$ is:
\[ S = \frac{10000S_0}{(100 + t)^2}. \]

\subsection*{Determining When $S(t) = \frac{1}{2}S_0$}

Setting $S(t) = \frac{1}{2}S_0$ and solving for $t$ gives:
\[ t = \sqrt{20000} - 100 \approx 41.42 \text{ minutes.} \]

\subsection*{Conclusion}

Since the salt amount decays to half its initial value at approximately 41.42 minutes, which is slightly before the tank overflows at 42 minutes, the salt concentration reaching half of its initial value occurs first.


\end{proof}





\pagebreak
\newexercise{2. }\\

\section*{Solution to the Harvesting Model Problem}

Given the differential equation for the population dynamics with harvesting:
\[
\dot{N} = r\left(1 - \frac{N}{K}\right)N - H\frac{N}{N+A},
\]
where \(N(t)\) is the population size, \(r = 1.25 \text{ year}^{-1}\) is the intrinsic growth rate, \(K = 20 \text{ tons}\) is the carrying capacity, \(H = 6.25 \text{ ton year}^{-1}\) is the harvesting rate, and \(A = 0.5 \text{ tons}\) is the limiting harvesting coefficient.

To find the steady-state solution, we set \(\dot{N}\) to zero and solve for \(N\):
\[
rN\left(1 - \frac{N}{K}\right) - H\frac{N}{N+A} = 0.
\]
Clearing the denominators, we have:
\[
rKN(N + A) - rN^2(N + A) - HKN = 0.
\]
Expanding and simplifying the equation, we get a quadratic equation in \(N\):
\[
(rA - H)N + rKN^2 - rAN^2 = 0.
\]
Solving this quadratic equation yields three equilibrium solutions for \(N\):
\[
N = 0 \text{ tons, } N = 7.5 \text{ tons, and } N = 12 \text{ tons}.
\]

To determine the stability of these equilibrium points, we calculate the first derivative of the right-hand side of the population dynamics equation:
\[
\frac{d}{dN}\left(rN\left(1 - \frac{N}{K}\right) - H\frac{N}{N+A}\right).
\]
Evaluating this derivative at the equilibrium points gives:
\[
\quad \frac{d}{dN}\Bigg|_{N=0} > 0, \frac{d}{dN}\Bigg|_{N=7.5} > 0, \quad \frac{d}{dN}\Bigg|_{N=12} < 0.
\]

The positive derivative at \(N = 7.5\) tons indicates an unstable equilibrium, and the negative derivative at \(N = 12\) tons indicates a stable equilibrium. Given the initial condition \(N(0) = 10\) tons, the population will tend towards the stable equilibrium.

Therefore, the long-term behavior of the population size as \(t \to \infty\) is:
\[
\lim_{{t \to \infty}} N(t) = 12 \text{ tons}.
\]








\pagebreak
\newexercise{3. }\\
\section*{Bifurcation Equations}

We consider a dynamical system governed by the following differential equation:
\[ \frac{dx}{dt} = -x\left(1 - \frac{L_0}{\sqrt{x^2 + 1}}\right) + h, \]
where \( L_0 > 0 \) and \( h \) are parameters, and \( x \) is the state variable.

The steady states \( x_* \) are found by setting the time derivative to zero:
\[ 0 = -x_*\left(1 - \frac{L_0}{\sqrt{x_*^2 + 1}}\right) + h. \]
This leads to the steady-state condition:
\[ 1 - \frac{h}{x_*} = \frac{L_0}{\sqrt{x_*^2 + 1}}. \]

To find the bifurcation points, we analyze the derivatives of both sides of the steady-state condition with respect to \( x_* \). At the bifurcation points, these derivatives are equal, corresponding to the tangency points of the graph of the left and right-hand sides as functions of \( x_* \). This yields the following equations for the derivatives:
\begin{align*}
\frac{d}{dx_*}\left(1 - \frac{h}{x_*}\right) &= \frac{d}{dx_*}\left(\frac{L_0}{\sqrt{x_*^2 + 1}}\right), \\
\frac{h}{x_*^2} &= \frac{L_0 x_*}{(x_*^2 + 1)^{\frac{3}{2}}}.
\end{align*}

Solving this for \( h \) in terms of \( x_* \) and \( L_0 \), we get:
\[ h = -x_*^3. \]

Substituting back into the steady-state condition, we get \( L_0 \) in terms of \( x_* \):
\[ L_0 = (x_*^2 + 1)^{\frac{3}{2}}. \]

Thus, we have derived the parametric equations for the bifurcation curve:
\begin{align*}
L_0 &= (x_*^2 + 1)^{\frac{3}{2}}, \\
h &= -x_*^3.
\end{align*}

Given \( L_0 = 5\sqrt{5} \), we can now find the corresponding values of \( h \) for which there are two steady states.

\begin{align*}
    5\sqrt{5} &= (x_*^2 + 1)^{\frac{3}{2}}\\
    &=(5)^{\frac{3}{2}} \implies x_* = 2, -2
\end{align*}
Thus $h = x_*^3 = 8, -8$.






\pagebreak
\newexercise{4. }

We consider the forced vibration problem for a mass-spring system with an external force described by the following differential equation:
\[
\ddot{x} + \dot{x} + 8x = 10 \cos(3t).
\]
To find the particular solution, we assume a solution of the form:
\[
x_p(t) = A \cos(3t) + B \sin(3t),
\]
where $A$ and $B$ are constants to be determined.

Differentiating $x_p(t)$ with respect to time, we obtain:
\[
\dot{x}_p(t) = -3A \sin(3t) + 3B \cos(3t),
\]
and
\[
\ddot{x}_p(t) = -9A \cos(3t) - 9B \sin(3t).
\]

Substituting $x_p(t)$, $\dot{x}_p(t)$, and $\ddot{x}_p(t)$ into the original differential equation, we get:
\[
(-9A \cos(3t) - 9B \sin(3t)) + (-3A \sin(3t) + 3B \cos(3t)) + 8(A \cos(3t) + B \sin(3t)) = 10 \cos(3t).
\]

Grouping like terms, the equation simplifies to:
\[
(-A + 3B) \cos(3t) + (-3A - B) \sin(3t) = 10 \cos(3t).
\]

Equating the coefficients of $\cos(3t)$ and $\sin(3t)$ from both sides of the equation, we obtain the system of equations:
\[
\begin{cases}
-A + 3B = 10, \\
-3A - B = 0.
\end{cases}
\]

Solving the above system yields $A=-1$ and $B = 3$.

Solving this system of equations, we find the values of $A$ and $B$. The amplitude of the response function $M$ can then be calculated as $M = \sqrt{A^2 + B^2}$.

\[
M = \sqrt{A^2 + B^2} = \sqrt{10}
\]













\pagebreak
\newexercise{5. }
\begin{proof}
    Given the second Newton's law for a mass-spring-damper system, we have:

\[
m\ddot{x} = -kx - \eta\dot{x} + mg
\]

where \( m \) is the mass, \( k \) is the spring constant, \( \eta \) is the damping coefficient, \( g \) is the acceleration due to gravity, and \( x \) is the elongation of the spring. At steady state, \( \dot{x} = 0 \) and \( \ddot{x} = 0 \), so the equation simplifies to:

\[
0 = -kx + mg
\]

Solving for \( x \) yields:

\[
x = \frac{mg}{k}
\]

Substituting in the known values \( m = 1.2\, \text{kg} \), \( g = 10\, \text{N/kg} \), and \( k = 2.4\, \text{N/m} \), we find:

\[
x = \frac{1.2 \times 10}{2.4} = 5.0\, \text{meters}
\]

Thus, the elongation of the spring at equilibrium is 5.0 meters.

\end{proof}


\end{document}